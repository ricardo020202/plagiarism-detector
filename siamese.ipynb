{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bb8de6",
   "metadata": {},
   "source": [
    "<h1>Use siamese GNN to predict the similarity of two source codes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac99c",
   "metadata": {},
   "source": [
    "<h3>Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4390b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java as ts_java\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, GINConv\n",
    "import torch.nn.functional as F, torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "67737eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all seeds to make results reproducible.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d8a2e",
   "metadata": {},
   "source": [
    "<h2>Data preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b41d0",
   "metadata": {},
   "source": [
    "<h3>Define constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dad6d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_directory1 = './datasets/conplag_preprocessed'\n",
    "java_directory2 = './datasets/ir_plag_preprocessed'\n",
    "java_LANGUAGE = Language(ts_java.language())\n",
    "parser = Parser(java_LANGUAGE)\n",
    "csv_paths = ['./labels/conplag-labels.csv', './labels/ir_plag_labels.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6b312cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93039af9",
   "metadata": {},
   "source": [
    "<h3>Get AST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3185acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node, parent_idx=None):\n",
    "        idx = len(nodes)\n",
    "        nodes.append(node.type)\n",
    "        \n",
    "        if parent_idx is not None:\n",
    "            edges.append((parent_idx, idx))\n",
    "        \n",
    "        for child in node.children:\n",
    "            traverse(child, idx)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f384178",
   "metadata": {},
   "source": [
    "<h3>Build data for GNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3b169cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_vocab(java_directories, file_lists):\n",
    "    all_node_types = set()\n",
    "\n",
    "    for java_directory, file_list in zip(java_directories, file_lists):\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(java_directory, file_name)\n",
    "            nodes, _ = parse_java_file(file_path)\n",
    "            all_node_types.update(nodes)\n",
    "\n",
    "    node_type_to_idx = {typ: idx for idx, typ in enumerate(sorted(all_node_types))}\n",
    "    return node_type_to_idx\n",
    "\n",
    "def create_node_features(nodes, node_type_to_idx):\n",
    "    node_features = [node_type_to_idx[typ] for typ in nodes]\n",
    "    return node_features\n",
    "\n",
    "def create_graph_data(nodes, edges, node_features, embedding_layer):\n",
    "    x = embedding_layer(torch.tensor(node_features))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9ee31811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_node_types, embedding_dim):\n",
    "        super(NodeEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_node_types, embedding_dim)\n",
    "\n",
    "    def forward(self, node_indices):\n",
    "        return self.embeddings(node_indices)\n",
    "    \n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7ae20970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pairs(pairs_df, java_directory, node_type_to_idx, embedding_layer):\n",
    "    data_pairs = []\n",
    "    for idx, row in pairs_df.iterrows():\n",
    "        file1, file2, label = row['id1'], row['id2'], row['plagio']\n",
    "\n",
    "        file1_path = os.path.join(java_directory, file1)\n",
    "        file2_path = os.path.join(java_directory, file2)\n",
    "\n",
    "        nodes1, edges1 = parse_java_file(file1_path)\n",
    "        nodes2, edges2 = parse_java_file(file2_path)\n",
    "\n",
    "        node_features1 = create_node_features(nodes1, node_type_to_idx)\n",
    "        node_features2 = create_node_features(nodes2, node_type_to_idx)\n",
    "\n",
    "        data1 = create_graph_data(nodes1, edges1, node_features1, embedding_layer)\n",
    "        data2 = create_graph_data(nodes2, edges2, node_features2, embedding_layer)\n",
    "\n",
    "        data_pairs.append((data1, data2, label))\n",
    "        \n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c7ddf221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "Number of pairs in training dataset: 846\n",
      "Number of pairs in validation set: 133\n",
      "Number of pairs in test set: 267\n",
      "Dataset 1 - First pair:\n",
      "  Graph 1: 919 nodes, 918 edges\n",
      "  Graph 2: 1246 nodes, 1245 edges\n",
      "  Label: 0\n",
      "Dataset 2 - First pair:\n",
      "  Graph 1: 108 nodes, 107 edges\n",
      "  Graph 2: 109 nodes, 108 edges\n",
      "  Label: 1\n"
     ]
    }
   ],
   "source": [
    "pairs_df1 = load_csv(csv_paths[0])\n",
    "pairs_df2 = load_csv(csv_paths[1])\n",
    "\n",
    "file_list1 = list(set(pairs_df1['id1'].tolist() + pairs_df1['id2'].tolist()))\n",
    "file_list2 = list(set(pairs_df2['id1'].tolist() + pairs_df2['id2'].tolist()))\n",
    "\n",
    "java_directories = [java_directory1, java_directory2]\n",
    "file_lists = [file_list1, file_list2]\n",
    "\n",
    "node_type_to_idx = build_global_vocab(java_directories, file_lists)\n",
    "embedding_layer = NodeEmbeddingLayer(len(node_type_to_idx), embedding_dim)\n",
    "\n",
    "data_pairs1 = prepare_data_for_pairs(pairs_df1, java_directory1, node_type_to_idx, embedding_layer)\n",
    "data_pairs2 = prepare_data_for_pairs(pairs_df2, java_directory2, node_type_to_idx, embedding_layer)\n",
    "\n",
    "all_pairs = data_pairs1 + data_pairs2\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "test_size = int(len(all_pairs) * 0.2)\n",
    "val_size = int(len(all_pairs) * 0.1)\n",
    "\n",
    "test_pairs = all_pairs[:test_size]\n",
    "val_pairs = all_pairs[test_size:test_size+val_size]\n",
    "train_pairs = all_pairs[test_size+val_size:]\n",
    "\n",
    "plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 1]\n",
    "non_plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 0]\n",
    "\n",
    "if len(plagiarism_pairs) > len(non_plagiarism_pairs):\n",
    "    plagiarism_pairs = random.sample(plagiarism_pairs, len(non_plagiarism_pairs))\n",
    "else:\n",
    "    non_plagiarism_pairs = random.sample(non_plagiarism_pairs, len(plagiarism_pairs))\n",
    "\n",
    "balanced_train_pairs = plagiarism_pairs + non_plagiarism_pairs\n",
    "random.shuffle(balanced_train_pairs)\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"Number of pairs in training dataset: {len(balanced_train_pairs)}\")\n",
    "print(f\"Number of pairs in validation set: {len(val_pairs)}\")\n",
    "print(f\"Number of pairs in test set: {len(test_pairs)}\")\n",
    "data1, data2, label1 = data_pairs1[0]\n",
    "data3, data4, label2 = data_pairs2[0]\n",
    "print(f\"Dataset 1 - First pair:\")\n",
    "print(f\"  Graph 1: {data1.num_nodes} nodes, {data1.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data2.num_nodes} nodes, {data2.num_edges} edges\")\n",
    "print(f\"  Label: {label1}\")\n",
    "print(f\"Dataset 2 - First pair:\")\n",
    "print(f\"  Graph 1: {data3.num_nodes} nodes, {data3.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data4.num_nodes} nodes, {data4.num_edges} edges\")\n",
    "print(f\"  Label: {label2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b01de",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdc33d",
   "metadata": {},
   "source": [
    "<h3>Build GNN siamese architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_channels, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        nn3 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        nn4 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        nn5 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = GNNEncoder(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        h1 = self.encoder(data1.x, data1.edge_index, data1.batch)\n",
    "        h2 = self.encoder(data2.x, data2.edge_index, data2.batch)\n",
    "        return h1, h2\n",
    "\n",
    "def contrastive_loss(h1, h2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(h1, h2)\n",
    "    loss = (label * torch.pow(distance, 2) + \n",
    "           (1 - label) * torch.pow(F.relu(margin - distance), 2))\n",
    "    return loss.mean()\n",
    "\n",
    "def collate_fn(pairs, device):\n",
    "    data1_list, data2_list, labels = [], [], []\n",
    "    for d1, d2, label in pairs:\n",
    "        data1_list.append(d1)\n",
    "        data2_list.append(d2)\n",
    "        labels.append(label)\n",
    "\n",
    "    batch1 = Batch.from_data_list(data1_list).to(device)\n",
    "    batch2 = Batch.from_data_list(data2_list).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float, device=device).to(device)\n",
    "\n",
    "    return batch1, batch2, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a974c0",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "f8dc2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_pairs, val_pairs, device, scheduler, threshold, epochs=50, batch_size=32):    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_threshold = threshold\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        random.Random(42 + epoch).shuffle(train_pairs)\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(0, len(train_pairs), batch_size):\n",
    "            batch_pairs = train_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            predictions = (distances < best_threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(train_pairs)\n",
    "        train_accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        val_loss, val_accuracy = evaluate(model, val_pairs, device, best_threshold, batch_size)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}% Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            new_threshold, _ = find_best_threshold(model, val_pairs, device, batch_size=batch_size)\n",
    "            print(f\"Updated threshold: {new_threshold:.4f}\")\n",
    "            best_threshold = new_threshold\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'epoch': epoch,\n",
    "                'threshold': best_threshold,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': val_accuracy\n",
    "            }\n",
    "            torch.save(checkpoint, f'./models/siamese_gnn_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved best model at epoch {epoch+1} with threshold {best_threshold:.4f}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return best_threshold, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "def evaluate(model, data_pairs, device, threshold, batch_size=32):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_pairs), batch_size):\n",
    "            batch_pairs = data_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_pairs)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def find_best_threshold(model, dataset, device, batch_size=32, thresholds=np.linspace(0, 2, 100)):\n",
    "    model.eval()\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            all_preds_raw.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds_raw = np.array(all_preds_raw)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = (all_preds_raw < threshold).astype(int)\n",
    "        f1 = f1_score(all_labels, preds)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f03c5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1e399511",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "out_dim = 32\n",
    "threshold = 0.2626\n",
    "epochs = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b45679b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Train Loss: 0.4330, Accuracy: 49.05% Val Loss: 0.5685, Accuracy: 40.60%\n",
      "Saved best model at epoch 1 with threshold 0.2626\n",
      "Epoch 2/200 Train Loss: 0.3710, Accuracy: 46.34% Val Loss: 0.4853, Accuracy: 42.86%\n",
      "Saved best model at epoch 2 with threshold 0.2626\n",
      "Epoch 3/200 Train Loss: 0.3428, Accuracy: 47.28% Val Loss: 0.4919, Accuracy: 41.35%\n",
      "Epoch 4/200 Train Loss: 0.3434, Accuracy: 45.74% Val Loss: 0.4421, Accuracy: 48.12%\n",
      "Saved best model at epoch 4 with threshold 0.2626\n",
      "Epoch 5/200 Train Loss: 0.3214, Accuracy: 48.11% Val Loss: 0.4037, Accuracy: 54.89%\n",
      "Updated threshold: 0.2222\n",
      "Saved best model at epoch 5 with threshold 0.2222\n",
      "Epoch 6/200 Train Loss: 0.3162, Accuracy: 46.93% Val Loss: 0.4221, Accuracy: 55.64%\n",
      "Epoch 7/200 Train Loss: 0.3153, Accuracy: 46.57% Val Loss: 0.3888, Accuracy: 59.40%\n",
      "Saved best model at epoch 7 with threshold 0.2222\n",
      "Epoch 8/200 Train Loss: 0.2871, Accuracy: 50.35% Val Loss: 0.3619, Accuracy: 62.41%\n",
      "Saved best model at epoch 8 with threshold 0.2222\n",
      "Epoch 9/200 Train Loss: 0.2984, Accuracy: 51.77% Val Loss: 0.3664, Accuracy: 60.15%\n",
      "Epoch 10/200 Train Loss: 0.2810, Accuracy: 54.26% Val Loss: 0.3591, Accuracy: 60.90%\n",
      "Updated threshold: 0.3434\n",
      "Saved best model at epoch 10 with threshold 0.3434\n",
      "Epoch 11/200 Train Loss: 0.2750, Accuracy: 58.51% Val Loss: 0.3474, Accuracy: 56.39%\n",
      "Saved best model at epoch 11 with threshold 0.3434\n",
      "Epoch 12/200 Train Loss: 0.2680, Accuracy: 58.63% Val Loss: 0.3511, Accuracy: 54.89%\n",
      "Epoch 13/200 Train Loss: 0.2728, Accuracy: 58.04% Val Loss: 0.3495, Accuracy: 54.89%\n",
      "Epoch 14/200 Train Loss: 0.2715, Accuracy: 60.40% Val Loss: 0.3210, Accuracy: 59.40%\n",
      "Saved best model at epoch 14 with threshold 0.3434\n",
      "Epoch 15/200 Train Loss: 0.2540, Accuracy: 64.54% Val Loss: 0.3337, Accuracy: 59.40%\n",
      "Updated threshold: 0.1616\n",
      "Epoch 16/200 Train Loss: 0.2516, Accuracy: 54.02% Val Loss: 0.3082, Accuracy: 71.43%\n",
      "Saved best model at epoch 16 with threshold 0.1616\n",
      "Epoch 17/200 Train Loss: 0.2500, Accuracy: 53.55% Val Loss: 0.3298, Accuracy: 72.18%\n",
      "Epoch 18/200 Train Loss: 0.2386, Accuracy: 53.66% Val Loss: 0.3218, Accuracy: 68.42%\n",
      "Epoch 19/200 Train Loss: 0.2478, Accuracy: 50.95% Val Loss: 0.3195, Accuracy: 71.43%\n",
      "Epoch 20/200 Train Loss: 0.2282, Accuracy: 55.32% Val Loss: 0.3209, Accuracy: 70.68%\n",
      "Updated threshold: 0.2020\n",
      "Epoch 21/200 Train Loss: 0.2454, Accuracy: 56.62% Val Loss: 0.3229, Accuracy: 70.68%\n",
      "Epoch 22/200 Train Loss: 0.2345, Accuracy: 57.80% Val Loss: 0.3171, Accuracy: 69.92%\n",
      "Epoch 23/200 Train Loss: 0.2403, Accuracy: 57.92% Val Loss: 0.3081, Accuracy: 72.93%\n",
      "Saved best model at epoch 23 with threshold 0.2020\n",
      "Epoch 24/200 Train Loss: 0.2260, Accuracy: 60.17% Val Loss: 0.3084, Accuracy: 72.18%\n",
      "Epoch 25/200 Train Loss: 0.2297, Accuracy: 57.09% Val Loss: 0.2889, Accuracy: 70.68%\n",
      "Updated threshold: 0.2424\n",
      "Saved best model at epoch 25 with threshold 0.2424\n",
      "Epoch 26/200 Train Loss: 0.2259, Accuracy: 61.94% Val Loss: 0.2867, Accuracy: 70.68%\n",
      "Saved best model at epoch 26 with threshold 0.2424\n",
      "Epoch 27/200 Train Loss: 0.2193, Accuracy: 66.08% Val Loss: 0.2924, Accuracy: 68.42%\n",
      "Epoch 28/200 Train Loss: 0.2102, Accuracy: 64.07% Val Loss: 0.2865, Accuracy: 69.17%\n",
      "Saved best model at epoch 28 with threshold 0.2424\n",
      "Epoch 29/200 Train Loss: 0.2304, Accuracy: 63.36% Val Loss: 0.2874, Accuracy: 69.17%\n",
      "Epoch 30/200 Train Loss: 0.2242, Accuracy: 64.07% Val Loss: 0.2934, Accuracy: 68.42%\n",
      "Updated threshold: 0.1818\n",
      "Epoch 31/200 Train Loss: 0.2153, Accuracy: 57.80% Val Loss: 0.2842, Accuracy: 72.18%\n",
      "Saved best model at epoch 31 with threshold 0.1818\n",
      "Epoch 32/200 Train Loss: 0.2157, Accuracy: 59.34% Val Loss: 0.2873, Accuracy: 69.92%\n",
      "Epoch 33/200 Train Loss: 0.2124, Accuracy: 59.22% Val Loss: 0.2847, Accuracy: 71.43%\n",
      "Epoch 34/200 Train Loss: 0.2184, Accuracy: 59.46% Val Loss: 0.2954, Accuracy: 71.43%\n",
      "Epoch 35/200 Train Loss: 0.2149, Accuracy: 61.11% Val Loss: 0.2882, Accuracy: 71.43%\n",
      "Updated threshold: 0.2626\n",
      "Epoch 36/200 Train Loss: 0.2242, Accuracy: 67.14% Val Loss: 0.2824, Accuracy: 70.68%\n",
      "Saved best model at epoch 36 with threshold 0.2626\n",
      "Epoch 37/200 Train Loss: 0.2158, Accuracy: 68.09% Val Loss: 0.2916, Accuracy: 68.42%\n",
      "Epoch 38/200 Train Loss: 0.2223, Accuracy: 69.86% Val Loss: 0.2832, Accuracy: 69.92%\n",
      "Epoch 39/200 Train Loss: 0.2070, Accuracy: 68.79% Val Loss: 0.2762, Accuracy: 70.68%\n",
      "Saved best model at epoch 39 with threshold 0.2626\n",
      "Epoch 40/200 Train Loss: 0.2171, Accuracy: 67.73% Val Loss: 0.2612, Accuracy: 70.68%\n",
      "Updated threshold: 0.3232\n",
      "Saved best model at epoch 40 with threshold 0.3232\n",
      "Epoch 41/200 Train Loss: 0.2210, Accuracy: 69.98% Val Loss: 0.2675, Accuracy: 67.67%\n",
      "Epoch 42/200 Train Loss: 0.2142, Accuracy: 69.74% Val Loss: 0.2490, Accuracy: 71.43%\n",
      "Saved best model at epoch 42 with threshold 0.3232\n",
      "Epoch 43/200 Train Loss: 0.2034, Accuracy: 70.57% Val Loss: 0.2469, Accuracy: 71.43%\n",
      "Saved best model at epoch 43 with threshold 0.3232\n",
      "Epoch 44/200 Train Loss: 0.2130, Accuracy: 70.69% Val Loss: 0.2497, Accuracy: 71.43%\n",
      "Epoch 45/200 Train Loss: 0.1964, Accuracy: 72.22% Val Loss: 0.2382, Accuracy: 72.18%\n",
      "Updated threshold: 0.3434\n",
      "Saved best model at epoch 45 with threshold 0.3434\n",
      "Epoch 46/200 Train Loss: 0.2036, Accuracy: 70.57% Val Loss: 0.2411, Accuracy: 72.18%\n",
      "Epoch 47/200 Train Loss: 0.2096, Accuracy: 70.33% Val Loss: 0.2442, Accuracy: 71.43%\n",
      "Epoch 48/200 Train Loss: 0.1948, Accuracy: 72.93% Val Loss: 0.2346, Accuracy: 73.68%\n",
      "Saved best model at epoch 48 with threshold 0.3434\n",
      "Epoch 49/200 Train Loss: 0.2015, Accuracy: 72.22% Val Loss: 0.2334, Accuracy: 73.68%\n",
      "Saved best model at epoch 49 with threshold 0.3434\n",
      "Epoch 50/200 Train Loss: 0.1992, Accuracy: 71.99% Val Loss: 0.2362, Accuracy: 72.93%\n",
      "Updated threshold: 0.2626\n",
      "Epoch 51/200 Train Loss: 0.1968, Accuracy: 69.50% Val Loss: 0.2297, Accuracy: 75.19%\n",
      "Saved best model at epoch 51 with threshold 0.2626\n",
      "Epoch 52/200 Train Loss: 0.1874, Accuracy: 69.39% Val Loss: 0.2368, Accuracy: 75.19%\n",
      "Epoch 53/200 Train Loss: 0.1840, Accuracy: 69.98% Val Loss: 0.2305, Accuracy: 76.69%\n",
      "Epoch 54/200 Train Loss: 0.1916, Accuracy: 68.09% Val Loss: 0.2332, Accuracy: 78.20%\n",
      "Epoch 55/200 Train Loss: 0.2024, Accuracy: 70.33% Val Loss: 0.2340, Accuracy: 78.20%\n",
      "Updated threshold: 0.2222\n",
      "Epoch 56/200 Train Loss: 0.2003, Accuracy: 69.27% Val Loss: 0.2369, Accuracy: 81.20%\n",
      "Epoch 57/200 Train Loss: 0.1932, Accuracy: 68.32% Val Loss: 0.2281, Accuracy: 81.20%\n",
      "Saved best model at epoch 57 with threshold 0.2222\n",
      "Epoch 58/200 Train Loss: 0.1979, Accuracy: 69.50% Val Loss: 0.2295, Accuracy: 80.45%\n",
      "Epoch 59/200 Train Loss: 0.1970, Accuracy: 68.68% Val Loss: 0.2387, Accuracy: 80.45%\n",
      "Epoch 60/200 Train Loss: 0.1964, Accuracy: 66.67% Val Loss: 0.2321, Accuracy: 77.44%\n",
      "Updated threshold: 0.2626\n",
      "Epoch 61/200 Train Loss: 0.2004, Accuracy: 70.80% Val Loss: 0.2313, Accuracy: 79.70%\n",
      "Epoch 62/200 Train Loss: 0.1900, Accuracy: 71.87% Val Loss: 0.2383, Accuracy: 78.20%\n",
      "Epoch 63/200 Train Loss: 0.1915, Accuracy: 71.28% Val Loss: 0.2412, Accuracy: 76.69%\n",
      "Epoch 64/200 Train Loss: 0.1896, Accuracy: 71.99% Val Loss: 0.2410, Accuracy: 77.44%\n",
      "Epoch 65/200 Train Loss: 0.1958, Accuracy: 70.09% Val Loss: 0.2391, Accuracy: 75.19%\n",
      "Updated threshold: 0.2424\n",
      "Epoch 66/200 Train Loss: 0.1865, Accuracy: 71.04% Val Loss: 0.2367, Accuracy: 78.20%\n",
      "Epoch 67/200 Train Loss: 0.1872, Accuracy: 69.74% Val Loss: 0.2321, Accuracy: 78.20%\n",
      "Early stopping at epoch 67\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "best_threshold, train_losses, val_losses, train_accs, val_accs = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    balanced_train_pairs,\n",
    "    val_pairs,\n",
    "    device,\n",
    "    scheduler,\n",
    "    threshold=threshold,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4ca0a",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "02f098f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_with_metadata(model_path, in_channels, hidden_dim, out_dim, device):\n",
    "    loaded_model = SiameseNetwork(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=hidden_dim,\n",
    "        out_channels=out_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    threshold = checkpoint.get('threshold', 0.5)\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    val_loss = checkpoint.get('val_loss', 0)\n",
    "    val_accuracy = checkpoint.get('val_accuracy', 0)\n",
    "    \n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "    print(f\"Model was saved at epoch {epoch+1} with validation loss {val_loss:.4f} and accuracy {val_accuracy*100:.2f}%\")\n",
    "    print(f\"Using threshold: {threshold:.4f}\")\n",
    "    \n",
    "    return loaded_model, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8f09c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./models/siamese_gnn_model_epoch_57.pth\n",
      "Model was saved at epoch 57 with validation loss 0.2281 and accuracy 81.20%\n",
      "Using threshold: 0.2222\n"
     ]
    }
   ],
   "source": [
    "loaded_model, best_threshold = load_model_with_metadata('./models/siamese_gnn_model_epoch_57.pth', embedding_dim, hidden_dim, out_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2067fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, dataset, device, batch_size=batch_size, threshold=threshold):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [int(round(x)) for x in all_preds]\n",
    "    all_labels = [int(round(x)) for x in all_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[1, 0])\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "\n",
    "    for (i, j), value in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f'{value}', ha='center', va='center', color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f0edce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7664\n"
     ]
    }
   ],
   "source": [
    "_, best_f1 = find_best_threshold(loaded_model, val_pairs, device, batch_size=batch_size, thresholds=np.linspace(0, 2, 100))\n",
    "print(f\"Best F1 score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6428e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos con plagio (1): 117\n",
      "Ejemplos sin plagio (0): 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_289432\\1322368525.py:30: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + labels)\n",
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_289432\\1322368525.py:31: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAIoCAYAAABjzY09AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUABJREFUeJzt3Qd4FFXXwPGzoSS0hCImBELvSBFQiihSJCIiCOqLoiJFLIjSi9Kr2EAQQRFBEKRIea0gooJIR/FDRQRBCdIUhFBMQNjvOZd3N9mQQEJmkyz3//OZJ9mZ2dm7a4Y9e/bcMy632+0WAAAAwDJBmT0AAAAAIDMQCAMAAMBKBMIAAACwEoEwAAAArEQgDAAAACsRCAMAAMBKBMIAAACwEoEwAAAArEQgDAAAACsRCAOAQ3bu3CnNmjWTsLAwcblcsnTpUkeP/9tvv5njzpw509HjBrJbb73VLABwJQiEAVxVfv31V3nsscekdOnSEhISIqGhoXLTTTfJq6++Kv/8849fH7tDhw6ybds2GT16tMyePVtq164tV4tHHnnEBOH6eib3OuqHAN2uy0svvZTm4+/fv1+GDRsmW7dudWjEAHB52VOxDwAEhI8//ljuvfdeCQ4Olocffliuu+46OXPmjKxZs0b69u0rP/74o7z55pt+eWwNDtetWyfPPfecPPXUU355jBIlSpjHyZEjh2SG7Nmzy+nTp+XDDz+U++67z2fbnDlzzAePuLi4Kzq2BsLDhw+XkiVLSo0aNVJ9v88+++yKHg8AFIEwgKvCnj17pF27diZY/OKLL6RIkSLebd26dZNdu3aZQNlf/vzzT/Mzf/78fnsMzbZqsJlZ9AOGZtffe++9iwLhuXPnSosWLWTRokUZMhYNyHPnzi05c+bMkMcDcHWiNALAVeGFF16QkydPyvTp032CYI+yZcvKM888473977//ysiRI6VMmTImwNNM5LPPPivx8fE+99P1d955p8kq33jjjSYQ1bKLWbNmeffRr/Q1AFeaedaAVe/nKSnw/J6Y3kf3S2zFihXSoEEDE0znzZtXKlSoYMZ0uRphDfxvvvlmyZMnj7lvq1atZPv27ck+nn4g0DHpflrL3LFjRxNUptYDDzwgn376qRw7dsy7btOmTaY0QrcldfToUenTp49UrVrVPCctrWjevLl8//333n2++uorueGGG8zvOh5PiYXneWoNsGb3t2zZIrfccosJgD2vS9IaYS1P0f9HSZ9/dHS0FChQwGSeAcCDQBjAVUG/rtcAtX79+qnav0uXLjJkyBCpWbOmjB8/Xho2bChjx441WeWkNHi855575LbbbpOXX37ZBFQaTGqphWrTpo05hrr//vtNffCECRPSNH49lgbcGoiPGDHCPM5dd90l33zzzSXv9/nnn5sg7/DhwybY7dWrl6xdu9ZkbjVwTkozuSdOnDDPVX/XYFNLElJLn6sGqYsXL/bJBlesWNG8lknt3r3bTBrU5/bKK6+YDwpaR62vtycorVSpknnOqmvXrub100WDXo8jR46YAFrLJvS1bdSoUbLj01rwwoULm4D43LlzZt0bb7xhSigmTZokkZGRqX6uACzgBoAAd/z4cbf+c9aqVatU7b9161azf5cuXXzW9+nTx6z/4osvvOtKlChh1q1evdq77vDhw+7g4GB37969vev27Nlj9nvxxRd9jtmhQwdzjKSGDh1q9vcYP368uf3nn3+mOG7PY8yYMcO7rkaNGu5rr73WfeTIEe+677//3h0UFOR++OGHL3q8Tp06+Rzz7rvvdhcqVCjFx0z8PPLkyWN+v+eee9xNmjQxv587d84dERHhHj58eLKvQVxcnNkn6fPQ12/EiBHedZs2bbrouXk0bNjQbJs6dWqy23RJbPny5Wb/UaNGuXfv3u3Omzevu3Xr1pd9jgDsQ0YYQMCLjY01P/Ply5eq/T/55BPzU7OnifXu3dv8TFpLXLlyZVN64KEZRy1b0GynUzy1xf/973/l/PnzqbrPgQMHTJcFzU4XLFjQu75atWome+15nok9/vjjPrf1eWm21fMapoaWQGg5w8GDB01Zhv5MrixCadlJUNCFtxrN0Opjeco+vv3221Q/ph5HyyZSQ1vYaecQzTJrBltLJTQrDABJEQgDCHhad6r0K//U+P33301wpnXDiUVERJiAVLcnVrx48YuOoeURf//9tzjlP//5jyln0JKN8PBwU6KxYMGCSwbFnnFqUJmUlhv89ddfcurUqUs+F30eKi3P5Y477jAfOubPn2+6RWh9b9LX0kPHr2Uj5cqVM8HsNddcYz5I/N///Z8cP3481Y9ZtGjRNE2M0xZu+uFAPyhMnDhRrr322lTfF4A9CIQBXBWBsNZ+/vDDD2m6X9LJainJli1bsuvdbvcVP4anftUjV65csnr1alPz+9BDD5lAUYNjzewm3Tc90vNcPDSg1UzrO++8I0uWLEkxG6zGjBljMu9a7/vuu+/K8uXLzaTAKlWqpDrz7Xl90uK7774zddNKa5IBIDkEwgCuCjoZSy+mob18L0c7PGgQpp0OEjt06JDphuDpAOEEzbgm7rDgkTTrrDRL3aRJEzOp7KeffjIX5tDSgy+//DLF56F27Nhx0baff/7ZZF+1k4Q/aPCrwaZm4ZObYOjx/vvvm4lt2s1D99OyhaZNm170mqT2Q0lqaBZcyyi0pEUn32lHEe1sAQBJEQgDuCr069fPBH1aWqABbVIaJGtHAc9X+yppZwcNQJX2w3WKtmfTEgDN8Cau7dVMatI2Y0l5LiyRtKWbh7aJ0300M5s4sNTMuHZJ8DxPf9DgVtvPvfbaa6ak5FIZ6KTZ5oULF8off/zhs84TsCf3oSGt+vfvL3v37jWvi/4/1fZ12kUipdcRgL24oAaAq4IGnNrGS8sJtD428ZXltJ2YBl86qUxVr17dBEZ6lTkNvLSV18aNG03g1Lp16xRbc10JzYJqYHb33XfL008/bXr2TpkyRcqXL+8zWUwndmlphAbhmunVr/Vff/11KVasmOktnJIXX3zRtBWrV6+edO7c2Vx5TtuEaY9gbafmL5q9HjRoUKoy9frcNEOrre20TEHrirXVXdL/f1qfPXXqVFN/rIFxnTp1pFSpUmkal2bQ9XUbOnSot53bjBkzTK/hwYMHm+wwAHiQEQZw1dC+u5p51Z6/2n1Bryg3YMAA009X+/LqpCmPt956y/TP1a/Me/ToYQKogQMHyrx58xwdU6FChUz2Vy8CoVlrDba1h2/Lli0vGrtOZHv77bfNuCdPnmzqanVcGtSmRMsMli1bZh5H+yLrJLG6deua/sNpDSL9QS98od04tDZYL2iiwb925YiKivLZTy8bra+NZpC1s4X2Y161alWaHkvLNDp16iTXX3+9udR14s4Y+tj6N7B+/XrHnhuAwOfSHmqZPQgAAAAgo5ERBgAAgJUIhAEAAGAlAmEAAABYiUAYAAAAViIQBgAAgJUIhAEAAGAlAmEAAABYiUAYyIJmzpxprrLlNJfLJUuXLk33cfQqXXoRCn9yaqxAVqFXNtQrFzrpq6++MueKE5em9vc55+RYAacQCAOZ+Kaobwq65MyZU8qWLWsuRfvvv//67TEPHDhgLsebXosXL5aRI0c6MibAn+fX888/77NeAz1dnxYlS5aUCRMmpGo/zzmtl4jWSzzrpb39SS9bref1pa4+mNH/PgCBhEAYyES33367efPZuXOnuQztsGHD5MUXX/Tb40VEREhwcPAV3//MmTPmZ8GCBSVfvnwOjgxwXkhIiIwbN07+/vvvDHtM/TCr5/R3330nN9xwg/znP/+RtWvX+u3x9EO0ntdpDe6TO6/T++8DEIgIhIFMpG86+uZTokQJeeKJJ6Rp06bywQcfXLTfr7/+Kq1atZLw8HDJmzeveYP9/PPPffbRN98WLVpIrly5pFSpUjJ37tyLMllJv/rs37+/lC9fXnLnzi2lS5eWwYMHy9mzZ73bNTCvUaOGvPXWW+aYGlgkLY3wfN2ZdNGMnMd///tfkx3T++vjDB8+3CfzrR8EbrnlFrO9cuXKsmLFCsdeY9hLzyc9v8aOHXvJ/RYtWiRVqlQx56OeMy+//LJ3m/6t//7779KzZ0/v3/al6AdEfUw9ryZPnmzOxw8//DDZfZctWyYNGjQwZVCFChWSO++805zriWkQreegnhu1a9f2ZrS3bt2abLnBkSNH5P7775eiRYua87pq1ary3nvv+RxTn9NTTz1lzuFrrrlGoqOjL/r3Qc/95M5rLdtS58+fN6+r/rugz7F69ery/vvv+zzOJ598Yl4H3d6oUSP57bffLvnaAZmBQBjIQvQNw5OdSezkyZNyxx13yMqVK02mSTPJLVu2lL1793r3efjhh2X//v3mjVHf2N988005fPjwZd+09Y3tp59+kldffVWmTZsm48eP99ln165d5nhaDuF5803uq1nP8sUXX5g3bQ1s1ddff23G9swzz5jHeeONN8xjjh492vuG2qZNG5PZ2rBhg0ydOtUE6EB6ZcuWTcaMGSOTJk2Sffv2JbvPli1b5L777pN27drJtm3bTACoHwg9AZ/+3RcrVsyb6dUltbJnzy45cuRI9pxWp06dkl69esnmzZvNuR0UFCR33323OSdUbGysOc81mP32229NOdLlzo24uDipVauWfPzxx/LDDz9I165d5aGHHpKNGzf67PfOO++Yc+6bb74x51xSffr08TmvX3rpJRNYazCuNAieNWuWue+PP/5oPig8+OCDsmrVKrM9JibGnNc6fv13o0uXLjJgwIBUv3ZAhnEDyBQdOnRwt2rVyvx+/vx594oVK9zBwcHuPn36uGfMmOEOCwu75P2rVKninjRpkvl9+/btbj2dN23a5N2+c+dOs278+PHedXp7yZIlKR7zxRdfdNeqVct7e+jQoe4cOXK4Dx8+7LNfw4YN3c8888xF9//rr7/cpUuXdj/55JPedU2aNHGPGTPGZ7/Zs2e7ixQpYn5fvny5O3v27O4//vjDu/3TTz+97FiB1J5fdevWdXfq1Mn8rn9Tid/6HnjgAfdtt93mc9++ffu6K1eu7L1dokQJn/MoJYn3i4+PN3/3+lgfffTRRWNKzp9//mn237Ztm7k9ZcoUd6FChdz//POPd59p06aZfb777jtz+8svvzS3//777xSP26JFC3fv3r19zt/rr7/+ov1SOufWrVvnDgkJcc+fP9/cjouLc+fOndu9du1an/06d+7svv/++83vAwcO9HkNVf/+/S87ViCjZc+4kBtAUh999JEpddByBM0CPfDAAyYjlXSCjWaEdb1meTQ7o2UF//zzjzcjvGPHDpN90vIDD518V6BAgUs+/vz582XixInm61h9DD1uaGiozz5atlG4cOHLPhd9Dm3btjX7a3bZ4/vvvzdZJ08GWJ07d85krk6fPi3bt2+XqKgoiYyM9G6vV6/eZR8PSC2tE27cuLHJcialf39adpTYTTfdZEqK9O9Us8ppoRnbQYMGmb9vPbd1sp6WLCVHS4KGDBlivgn566+/vJlgPa+vu+46c15Xq1bNW5Kkbrzxxks+vo5Zs+ALFiyQP/74w2Sj4+PjTTY3Mc0ap4aORTtd6GunmXPPt0R67t52220+++pjXX/99d7XtU6dOj7bOa+RFREIA5lI6+amTJlivqLUQFCD2eTom5DWzerXkxrgagnFPffck+JXrqmxbt06ad++vanX1RpBnXU+b948n/pIpbPfU0NrnPXrUP0KNvHz0ABbH0O/Jk0q8Rs84C9apqN/4wMHDvSpXfeHvn37msfQIFhr+i9VU6xlA/rBUUuS9PzXQFgD4PSc1zrZVj+IaiCvJRV6/motcNJjpua81tKNu+66ywSwWhqS+JxW+sFca5ETY7IdAg2BMJCJ9M1IA9vL0Yyqvrlq/aDnjSjxxJMKFSqYbK7WD3syPZq1udRseZ2Eo2/Czz33nHedTgq6Eq+88orJQOkxddJPYpql1sxWSs+zUqVKJoDWTHeRIkXMuvXr11/ROICUaGZWJ53puZL070/Pr8T0tk7y8mSD9YOqZlpTQyefpeac1kltel5oEHzzzTebdWvWrPHZR8f67rvvmoyuJ8DctGnTJY+rY9cMt9brKg2uf/nlFzMJNS20UkKPofefPXu2T0Cvx9LxaLa4YcOGyd5fX9ekE385r5EVMVkOCADlypXzTlbTUgMtofB8jaoqVqxoZsjrxBjNyGpArL9r5jiljJQeU9/INAuspRFaIrFkyZI0j027V/Tr189kojQIOHjwoFmOHz9ututXvzqpRrPCOqlGvzLVx9Svj5WOW4OODh06mOemk+sSB+eAEzQ7qt+A6N95Ytq2UCeq6UQ0DRh1Etlrr73mU0ahnSRWr15tSg20hMEJWrakHxp1Uqt+aNVJpjpxLjHPea7nsp43y5cvN98KqUud1/rtkX4o1fs89thjcujQoTSPT0ux9NzWya36wdtzXmtJlk6y1ddHJ8jp66X/fuhkPp2UqLfV448/bko/NEOuAb92sfFMQASyEgJhIABoxlXfOLVDg36dql/zJq4HVhps6lex+jWwZo4fffRR84aVUvmBfuWpb2TaRkkzZfrGqbPl00qzWJot0zc+zeh6Fu0SoXSsWgv92WefmbZvdevWNZ0pNButdKa8BuD6Bqv1jzq7PHE9MeAU/Xo/8QdIpeeRfpuhH860LEE/uOl+iUso9LZ+A1OmTJlU1cunhv7d62Nq1wp9XD0Xk/YQ13p9bb2mH4D1HNUPiDo+ldJ5rR8w9Tnpeadt0rSV25VczU67P2gArP/mJD6vdV6B0g8O+u+Fdo/Q7K92stFSCW2npooXL266zWg7Nm2tpt0ltHYZyGpcOmMuswcBwHnaLkonoWlWp0mTJpk9HAAOmDNnjnTs2NF846Lf+ABIH2qEgauEfrWqGRz9CljrbbVcQb/S9fTzBRB49JsevQiNTkrT0iHtSqHdGwiCAWcQCANXCW1f9uyzz8ru3btNSYR+panZI23oDyAwaV2ulkPoTy1NuPfeeykdAhxEaQQAAACsxGQ5AAAAWIlAGAAAAFYiEAYAAICVCIQBAABgJQJhAIZexlWvJqU/AWQdnJuA/9A1AoARGxsrYWFhplG/XtEKQNbAuQn4DxlhAAAAWIlAGAAAAFbiynJIl/Pnz8v+/fvNlcxcLldmDwfp/Po18U8AWQPn5tVFK1JPnDghkZGREhSU8fnIuLg4OXPmjF+OnTNnTgkJCZFAQo0w0mXfvn0SFRWV2cMAACCgxMTESLFixTI8CM6Vr5DIv6f9cvyIiAjZs2dPQAXDZISRLpoJVjcOWSzZQ/Jk9nAAJLLksbqZPQQASZyIjZWypaK8758ZyWSC/z0twVU6imTL6ezBz52Rgz/OMI9BIAxreMohNAgmEAayFjoMAFlXppYTZsspLocDYbcEJgJhAAAAm2gM7nQg7pKARNcIAAAAWImMMAAAgE1cQRcWJzl9vAwSmKMGAAAA0omMMAAAgE20PtjxGmGXBCIywgAAALASGWEAAACbUCPsRSAMAABgE0ojvAIzfAcAAADSiYwwAACAVfxQGiGBmVsNzFEDAAAA6URGGAAAwCbUCHuREQYAAICVyAgDAADYhPZpXoE5agAAACCdyAgDAADYhBphLwJhAAAAm1Aa4RWYowYAAADSiYwwAACATSiN8CIjDAAAACsRCAMAANhYI+xyeEmD1atXS8uWLSUyMlJcLpcsXbrUZ/vixYulWbNmUqhQIbN969atFx0jLi5OunXrZvbJmzevtG3bVg4dOpSmcRAIAwAAIEOdOnVKqlevLpMnT05xe4MGDWTcuHEpHqNnz57y4YcfysKFC2XVqlWyf/9+adOmTZrGQY0wAACAdTXCTneNSFuNcPPmzc2Skoceesj8/O2335Ldfvz4cZk+fbrMnTtXGjdubNbNmDFDKlWqJOvXr5e6deumahxkhAEAAOCI2NhYnyU+Pl78YcuWLXL27Flp2rSpd13FihWlePHism7dulQfh0AYAADAJkEu/ywiEhUVJWFhYd5l7NixfnkKBw8elJw5c0r+/Pl91oeHh5ttqUVpBAAAgE38eEGNmJgYCQ0N9a4ODg6WrIxAGAAAAI7QIDhxIOwvERERcubMGTl27JhPVli7Rui21KI0AgAAwMYLargcXjJQrVq1JEeOHLJy5Urvuh07dsjevXulXr16qT4OGWEAAABkqJMnT8quXbu8t/fs2WN6BRcsWNBMeDt69KgJarUlmifIVZrt1UXrjzt37iy9evUy99EsdPfu3U0QnNqOEYpAGAAAwCZ+rBFOrc2bN0ujRo28tzWgVR06dJCZM2fKBx98IB07dvRub9eunfk5dOhQGTZsmPl9/PjxEhQUZC6kod0poqOj5fXXX5e0cLndbnea7gEkoq1R9FNZ/THLJXtInsweDoBElne/KbOHACCZ983wQmGmD25G1NIm954d3HCouLKHOHps979xEr9qeKY8r/QgIwwAAGATf9T0ujK2RtgpTJYDAACAlcgIAwAA2CQL1AhnFQTCAAAANqE0wisww3cAAAAgncgIAwAA2ITSCK/AHDUAAACQTmSEAQAAbEKNsBcZYQAAAFiJjDAAAIBV/FAjLIGZWw3MUQMAAADpREYYAADAJtQIe5ERBgAAgJXICAMAAFiXEXa6j3BgZoQJhAEAAGzCBTW8AnPUAAAAQDqREQYAALAJk+W8yAgDAADASmSEAQAAbEKNsFdgjhoAAABIJzLCAAAANqFG2IuMMAAAAKxERhgAAMAm1Ah7EQgDAADYhNIIr8AM3wEAAIB0IiMMAABgEZfLZRZHkREGAAAAAgcZYQAAAIuQEU5ARhgAAABWIiMMAABgE03eOp3AdUlAIiMMAAAAK5ERBgAAsAg1wgkIhAEAACxCIJyA0ggAAABYiYwwAACARcgIJyAjDAAAACuREQYAALAIGeEEZIQBAABgJTLCAAAANuGCGl5khAEAAGAlMsIAAAAWoUY4AYEwAACARTRmdT4QloBEaQQAAACsREYYAADAIi79z/FSBpcEIjLCAAAAsBKBMAAAgIWT5VwOL2mxevVqadmypURGRpr7Ll261Ge72+2WIUOGSJEiRSRXrlzStGlT2blzp88+R48elfbt20toaKjkz59fOnfuLCdPnkzTOAiEAQAAkKFOnTol1atXl8mTJye7/YUXXpCJEyfK1KlTZcOGDZInTx6Jjo6WuLg47z4aBP/444+yYsUK+eijj0xw3bVr1zSNgxphAAAAm2SBC2o0b97cLMnRbPCECRNk0KBB0qpVK7Nu1qxZEh4ebjLH7dq1k+3bt8uyZctk06ZNUrt2bbPPpEmT5I477pCXXnrJZJpTg4wwAAAAHBEbG+uzxMfHp/kYe/bskYMHD5pyCI+wsDCpU6eOrFu3ztzWn1oO4QmCle4fFBRkMsipRSAMAABgE3/UB7supISjoqJM0OpZxo4dm+bhaRCsNAOcmN72bNOf1157rc/27NmzS8GCBb37pAalEQAAABbxx5XlXP87XkxMjJm85hEcHCxZGRlhAAAAOEKD4MTLlQTCERER5uehQ4d81uttzzb9efjwYZ/t//77r+kk4dknNQiEAQAALJIV2qddSqlSpUwwu3LlSu86rTfW2t969eqZ2/rz2LFjsmXLFu8+X3zxhZw/f97UEqcWpREAAADIUNrvd9euXT4T5LZu3WpqfIsXLy49evSQUaNGSbly5UxgPHjwYNMJonXr1mb/SpUqye233y6PPvqoabF29uxZeeqpp0xHidR2jFAEwgAAADbJAu3TNm/eLI0aNfLe7tWrl/nZoUMHmTlzpvTr18/0Gta+wJr5bdCggWmXFhIS4r3PnDlzTPDbpEkT0y2ibdu2pvdwWhAIAwAAIEPdeuutpl9wSrTUYsSIEWZJiWaP586dm65xEAgDAABYxJ9dIwINk+UAAABgJTLCAAAAFiEjnIBAGAAAwCIEwgkojQAAAICVyAgDAABYhIxwAjLCAAAAsBIZYQAAAJtkgQtqZBVkhAEAAGAlMsIAAAAWoUY4ARlhAAAAWImMMAAAgEXICCcgEAYAALAIgXACSiMAAABgJTLCAAAANqF9mhcZYQAAAFiJjDAAAIBFqBFOQEYYAAAAViIjDAAAYBEywgnICAMAAMBKBMKJzJw5U/Lnz+/4cfVT0tKlS9N9nFtvvVV69Ogh/uTUWJE1BblEOtUrLvM61ZLPuteVuR1rysN1ivnsM6BZWVnV8yaf5YW7K2famAEbvDl1itxwfTW5tmCoWRo2qCfLl31qth09elR6PtNdqlWpIAXy5ZJypYtLrx5Py/HjxzN72AhQLv3P5fASoG0jrCuNeOSRR+Sdd94xv+fIkUOKFy8uDz/8sDz77LN+e8wDBw5IgQIF0n2cxYsXmzEDV+qB2sWkVfUIGbt8p/x25LRUCM8rA5qVk1Px52TR1gPe/Tbs+Vue/2yn9/aZc+czacSAHYoWKyYjxzwvZcuWE7fbLe/OfkfubdNK1m/6ztw+cGC/jB33klSqVFn27v1dund73Kx7b/77mT10BCBKIywOhNXtt98uM2bMkPj4ePnkk0+kW7duJsAsUqSIXx4vIiIiXfc/c+aM5MyZUwoWLOjYmGCnKpH55Jtfj8r6PX+b2wdj46VJhWukYkRen/008D16+mwmjRKwT4s7W/rcHj5ytEx7Y4ps3LBeHunUWeYtWOTdVrpMGRk2YrR06vCg/Pvvv5I9u5Vv5YAjrCyNCA4ONsFpiRIl5IknnpCmTZvKBx98cNF+v/76q7Rq1UrCw8Mlb968csMNN8jnn39+Uba3RYsWkitXLilVqpTMnTtXSpYsKRMmTEix3KB///5Svnx5yZ07t5QuXVoGDx4sZ88mBB3Dhg2TGjVqyFtvvWWOGRISclFpxFdffZXsVxOa8fb473//KzVr1jT318cZPny4+UfTY+fOnXLLLbeY7ZUrV5YVK1Y49hoja/px/wmpGRUmxfJf+Jsqc01uqRoZKht+O+azX41iYbL0sRtkdoea0qtxaQkN4Y0WyCjnzp2TBfPnyalTp6RO3XrJ7hN7/LiEhoYSBCN9F9RwObwEIM4gERPEHjly5KL1J0+elDvuuENGjx5tgudZs2ZJy5YtZceOHaakQmlZxV9//WUCU80q9+rVSw4fPnzJx8uXL5+pR46MjJRt27bJo48+atb169fPu8+uXbtk0aJFphwiW7ZsFx2jfv36Jgj32L59uxmrBrbq66+/NmObOHGi3HzzzSao79q1q9k2dOhQOX/+vLRp08YE+Rs2bDC1ZqmpP9Ysui4esbGxl70Pso45m/ZJ7uBsMvuRmnL+vFuCglzy1je/y+c//+ndZ+Nvx2T1rqNy8HicROYPkUdvKmFqhJ+c939y3p2pwweuaj9s2ya33lxP4uLiTPJl/vtLpFLli+vz9T1n7JiR0qnLhX/TAVw5qwNhrbtauXKlLF++XLp3737R9urVq5vFY+TIkbJkyRKTPX7qqafk559/NhniTZs2Se3atc0+msUtV67cJR930KBB3t81e9ynTx+ZN2+eTyCs5RAaeBcuXDjZY2iphKfkQoP4Ll26SKdOncyiNPs7YMAA6dChg7mtGWEdvz6GBsI6bh2/PncNyNWYMWOkefPmlxz72LFjzbERmBqVv0Zuq1hYRn7yi6kRLnttHnmqYSn569QZWf7ThWD4i1/+8u6/+8hp+fWvUzKvU22TJf42hsk5gL+Ur1BBNmzeahITSxa/L4926iCfrVzlEwxr8uHuu1qYWuFBQ4Zl6ngRuKgRtjwQ/uijj8ynbS1H0MzoAw88YMoRFi5ceFFGWNd//PHHJvuqZQX//POP7N2712zXzLB+LaXlBx5ly5a97MS4+fPnm0ytZmn1MfS4+hVXYlq2kVIQnJg+h7Zt25r9X331Ve/677//Xr755huTzU78dZtmGk6fPm0yyFFRUd4gWNWrl/xXcIkNHDjQZL0T/6Osx0FgeOKWkiYr7Al2NdANzxcs7W8o5g2EkzpwPF6OnT4rRfOHEAgDfqQJjjJly5rfa9aqJVs2b5LJk16V16a8YdadOHFC7mpxu/kGUbPFTJ4G0s/KQLhRo0YyZcoU84+OBoIp1VhpplbrZl966SUT4GoJxT333GOytVdq3bp10r59e5NVjY6OlrCwMJMNfvnll332y5MnT6qOpzXOMTExsnHjRp/noQG2PoaWPyTlqTm+EloiogsCU3D2IHEnKW8473ZL0CU+yRfOm1NCc2WXI6eYPAdkJE3UeErRNOnQ8o5o8+/v+0s+SNe/4wAZYcsDYQ0yNbC9HM2o6uSzu+++2xtc/vbbb97tFSpUMNnc7777TmrVquWt7f377wsz8pOzdu1ak7197rnnvOt+//33K3oer7zyiixYsMAcs1ChQj7bNEutGeuUnmelSpVMAK2Zbk+3jPXr11/ROBA41u4+Kg/eWEwOnYg3pRHlCueR+2oWlU9+PGS258oRJB3qFpfVO/8yXSMiw0Lk8ZtLyh/H4mTT7yn/XQNIn8HPDZTo25tLVFRxk/mdP2+urF71lXz4yXITBN/ZvJn8c/q0zHjnXXPbMz9DvzlMbh4JgNSxMhBOLa311clqOkFOP+lodwf9hO5RsWJF03FCJ6Fphlm/purdu7fJHKf0yUiPqaUVmgXWLhRadqF1x2mlNb5a7zt58mS55ppr5ODBg2a9PrZmmYcMGSJ33nmnmdSnWeygoCBTLvHDDz/IqFGjzLi1c4XWEL/44ovmH9XEwTmuTq9+uUc61y8uPRuXlgK5c8hfJ8/IB9sOyjvrY8x2bResnSRur1xJ8gZnN9s37z0m09fulbPnmCkH+Mufhw9L544Py8EDB8y/4ddVrWaC4CZNbzMB8aaNG8x+VSr6Jjd+3rlHSpQsmUmjRqDSEMXpBK4rMBPCBMKXy7jq5DPt0KDBprY9S9olQSe0de7c2XRr0MlrOpnsxx9/TPFrq7vuukt69uxpJtvpV17aek0DbK1FTos1a9aYmt/HH3/cLB4a2GpHCi270FroESNGyLhx40yQroG7TqpTGhhrAK5jv/HGG82kPa1b1h7LuHr9c/acvLZqj1mSo/2D+y75KcPHBdhu6rTpKW67peGt8s9ZPojC6UDY6dIICUgut7ZOgGP27dtnJo9pxrZJkyZytdMPBpq9qD9muWQPSV1dM4CMsbz7TZk9BADJvG+GFwoz3UGSTpTPqPfs0t3fl6BgZ9+zz8efkt2T7smU55UeZITT6YsvvjC1w1WrVjX1tlquoNlVTz9fAACALMUPpRESoBlhAuF00vZlzz77rOzevdu0tNEyijlz5tDWBgAAIIsjEE4nrcXVBQAAIBDQPi1BUKLfAQAAAGuQEQYAALAI7dMSkBEGAACAlcgIAwAAWCQoyGUWJ7kdPl5GISMMAAAAK5ERBgAAsAg1wgkIhAEAACxC+7QElEYAAADASmSEAQAALEJpRAIywgAAALASgTAAAICFNcIuh5e0OHHihPTo0UNKlCghuXLlkvr168umTZu8291utwwZMkSKFClitjdt2lR27tzp+GtBIAwAAIAM1aVLF1mxYoXMnj1btm3bJs2aNTPB7h9//GG2v/DCCzJx4kSZOnWqbNiwQfLkySPR0dESFxfn6DgIhAEAACyS2Rnhf/75RxYtWmSC3VtuuUXKli0rw4YNMz+nTJlissETJkyQQYMGSatWraRatWoya9Ys2b9/vyxdutTR14JAGAAAAI6IjY31WeLj4y/a599//5Vz585JSEiIz3otgVizZo3s2bNHDh48aDLEHmFhYVKnTh1Zt26dOIlAGAAAwMKuES6HFxUVFWWCVs8yduzYix4/X758Uq9ePRk5cqTJ8mpQ/O6775og98CBAyYIVuHh4T7309uebU6hfRoAAIBFXOKHC2rIhePFxMRIaGiod31wcHCy+2ttcKdOnaRo0aKSLVs2qVmzptx///2yZcsWyUhkhAEAAOAIDYITLykFwmXKlJFVq1bJyZMnTfC8ceNGOXv2rJQuXVoiIiLMPocOHfK5j972bHMKgTAAAIBF/FkakVbaDUJbpP3999+yfPlyMzmuVKlSJuBduXKldz+tN9buEVpS4SRKIwAAAJChNOjV7hAVKlSQXbt2Sd++faVixYrSsWNHU7ahPYZHjRol5cqVM4Hx4MGDJTIyUlq3bu3oOAiEAQAALHIlF8C4nLQe7/jx4zJw4EDZt2+fFCxYUNq2bSujR4+WHDlymO39+vWTU6dOSdeuXeXYsWPSoEEDWbZs2UWdJtKLQBgAAAAZ6r777jPLpQLrESNGmMWfCIQBAAAskp6a3pQ4fbyMwmQ5AAAAWImMMAAAgEWyQo1wVkEgDAAAYBFKIxJQGgEAAAArkREGAACwCKURCcgIAwAAwEpkhAEAAGzihxphCcyEMBlhAAAA2ImMMAAAgEWoEU5ARhgAAABWIiMMAABgEfoIJyAQBgAAsAilEQkojQAAAICVyAgDAABYhNKIBGSEAQAAYCUywgAAABahRjgBGWEAAABYiYwwAACARcgIJyAjDAAAACuREQYAALAIXSMSEAgDAABYhNKIBJRGAAAAwEpkhAEAACxCaUQCMsIAAACwEhlhAAAAi1AjnICMMAAAAKxERhgAAMAimrt1vEZYAhMZYQAAAFiJjDAAAIBFglwuszjJ6eNlFAJhAAAAi9A+LQGlEQAAALASGWEAAACL0D4tARlhAAAAWImMMAAAgEWCXBcWJzl9vIxCRhgAAABWIiMMAABgE9M1gitqKDLCAAAAsBIZYQAAAIvQRzgBgTAAAIBFXP/7z0lOHy+jUBoBAAAAK5ERBgAAsAjt0xKQEQYAAICVyAgDAABYhEssJyAjDAAAgAxz7tw5GTx4sJQqVUpy5colZcqUkZEjR4rb7fbuo78PGTJEihQpYvZp2rSp7Ny50/GxEAgDAABY2D7N5fCSWuPGjZMpU6bIa6+9Jtu3bze3X3jhBZk0aZJ3H709ceJEmTp1qmzYsEHy5Mkj0dHREhcX5+hrQWkEAAAAMszatWulVatW0qJFC3O7ZMmS8t5778nGjRu92eAJEybIoEGDzH5q1qxZEh4eLkuXLpV27do5NhYywgAAABYJcrn8sqjY2FifJT4+XpKqX7++rFy5Un755Rdz+/vvv5c1a9ZI8+bNze09e/bIwYMHTTmER1hYmNSpU0fWrVsnTiIjDAAAYBF/XlkuKirKZ/3QoUNl2LBhPusGDBhgguSKFStKtmzZTM3w6NGjpX379ma7BsFKM8CJ6W3PNqcQCAMAAMARMTExEhoa6r0dHBx80T4LFiyQOXPmyNy5c6VKlSqydetW6dGjh0RGRkqHDh0kIxEIAwAAWMSf7dNCQ0N9AuHk9O3b12SFPbW+VatWld9//13Gjh1rAuGIiAiz/tChQ6ZrhIferlGjhqPjpkYYAAAAGeb06dMSFOQbgmqJxPnz583v2lZNg2GtI/bQUgrtHlGvXj1Hx0JGGAAAwCL+rBFOjZYtW5qa4OLFi5vSiO+++05eeeUV6dSpkze7rKUSo0aNknLlypnAWPsOa+lE69atxUkEwgAAAMgw2i9YA9snn3xSDh8+bALcxx57zFxAw6Nfv35y6tQp6dq1qxw7dkwaNGggy5Ytk5CQEEfHQiAMAABgkcTtzpySluPly5fP9AnWJSWaFR4xYoRZ/IkaYQAAAFiJjDAAAIBFNHfrcImwOH28jEJGGAAAAFYiIwwAAGARf/YRDjQEwgAAABYJcl1YnOT08TIKpREAAACwEhlhAAAAi1AakYCMMAAAAKxERhgAAMAyAZrAdRwZYQAAAFiJjDAAAIBFqBFOQEYYAAAAViIjDAAAYBH6CCcgEAYAALAIpREJKI0AAACAlcgIAwAAWERzt07nb10SmMgIAwAAwEpXFAh//fXX8uCDD0q9evXkjz/+MOtmz54ta9ascXp8AAAAcFCQy+WXxYpAeNGiRRIdHS25cuWS7777TuLj483648ePy5gxY/wxRgAAACDzA+FRo0bJ1KlTZdq0aZIjRw7v+ptuukm+/fZbp8cHAAAAB2ny1h+LFYHwjh075JZbbrlofVhYmBw7dsypcQEAAABZKxCOiIiQXbt2XbRe64NLly7t1LgAAADgxz7CLocXKwLhRx99VJ555hnZsGGDedL79++XOXPmSJ8+feSJJ57wzygBAADgCEoj0tFHeMCAAXL+/Hlp0qSJnD592pRJBAcHm0C4e/fuaT0cAAAAEBiBsGaBn3vuOenbt68pkTh58qRUrlxZ8ubN658RAgAAwDH+aHcWFKAp4Su+slzOnDlNAAwAAABYEQg3atTokgXRX3zxRXrHBAAAAD/xR02vy2VJIFyjRg2f22fPnpWtW7fKDz/8IB06dHBybAAAAEDWCYTHjx+f7Pphw4aZemEAAABkXf5od+aypX1aSh588EF5++23nTocAAAAkDUnyyW1bt06CQkJcepwCDDvdbpBQkNDM3sYABIpcMNTmT0EAEm4z53JElnQID8c04pAuE2bNj633W63HDhwQDZv3iyDBw92cmwAAABwGKUR6QiEw8LCfG4HBQVJhQoVZMSIEdKsWbO0Hg4AAADI+oHwuXPnpGPHjlK1alUpUKCA/0YFAAAAv9DkbRDt09Je0pEtWzaT9T127Fha7gYAAABkOWmubb7uuutk9+7d/hkNAAAA/Eqzwf5YrAiER40aJX369JGPPvrITJKLjY31WQAAAICrqkZYJ8P17t1b7rjjDnP7rrvu8pkhqN0j9LbWEQMAACBromvEFQTCw4cPl8cff1y+/PLL1N4FAAAACPxAWDO+qmHDhv4cDwAAAPzIHzW9QS4L2qcFatobAAAAF2g453RI57IhEC5fvvxlg+GjR4+md0wAAABA1gqEtU446ZXlAAAAEDiCXC6zOMnp42XJQLhdu3Zy7bXX+m80AAAAQFYLhKkPBgAACHxBV3Ihictw+ngZJSitXSMAAACA9ChZsqS3n3HipVu3bmZ7XFyc+b1QoUKSN29eadu2rRw6dEgyLRA+f/48ZREAAABXSdcIl8NLWmzatMlcodizrFixwqy/9957zc+ePXvKhx9+KAsXLpRVq1bJ/v37pU2bNplbIwwAAACkV+HChX1uP//881KmTBlzvYrjx4/L9OnTZe7cudK4cWOzfcaMGVKpUiVZv3691K1bV2wv6QAAAMAVCJILXSOCnFzkQko4NjbWZ4mPj7/seM6cOSPvvvuudOrUyZRHbNmyRc6ePStNmzb17lOxYkUpXry4rFu3zuHXAgAAANbwZ2lEVFSUabXrWcaOHXvZ8SxdulSOHTsmjzzyiLl98OBByZkzp+TPn99nv/DwcLPNSZRGAAAAwBExMTESGhrqvR0cHHzZ+2gZRPPmzSUyMlIyGoEwAACARYJcFxYneY6nQXDiQPhyfv/9d/n8889l8eLF3nURERGmXEKzxImzwto1Qrc5Om5HjwYAAACkkk6C065kLVq08K6rVauW5MiRQ1auXOldt2PHDtm7d6/Uq1dPnERGGAAAwCJaz+v0JZFdV3A4bc2rgXCHDh0ke/aEkFRrizt37iy9evWSggULmgxz9+7dTRDsZMcIRSAMAACADKclEZrl1W4RSY0fP16CgoLMhTS080R0dLS8/vrrjo+BQBgAAMAiV3IBjMu5kuM1a9YsxSsXh4SEyOTJk83iT9QIAwAAwEpkhAEAACziz64RgYZAGAAAwCKu//3nJKePl1EojQAAAICVyAgDAABYhNKIBGSEAQAAYCUywgAAABYhI5yAjDAAAACsREYYAADAIi6XyyxOcvp4GYWMMAAAAKxERhgAAMAi1AgnIBAGAACwiFYxOF3J4ArQQJjSCAAAAFiJjDAAAIBFglwuszjJ6eNlFDLCAAAAsBIZYQAAAIswWS4BGWEAAABYiYwwAACATfzQNULICAMAAACBg4wwAACARYLEZRYnOX28jEJGGAAAAFYiIwwAAGARriyXgEAYAADAIrRPS0BpBAAAAKxERhgAAMAiXGI5ARlhAAAAWImMMAAAgEWYLJeAjDAAAACsREYYAADAtgtqOF0jLIGZEiYjDAAAACuREQYAALAINcIJCIQBAAAsEuSHkoAgCUyBOm4AAAAgXcgIAwAAWMTlcpnFSU4fL6OQEQYAAICVyAgDAABYRHO3TudvXRKYyAgDAADASmSEAQAALKIX03D8ghquwMwJkxEGAACAlcgIAwAAWCYw87fOIxAGAACwCFeWS0BpBAAAAKxERhgAAMAiXFAjARlhAAAAWImMMAAAgEWC/JAJDZLAFKjjBgAAQID6448/5MEHH5RChQpJrly5pGrVqrJ582bvdrfbLUOGDJEiRYqY7U2bNpWdO3c6Pg4CYQAAAAtrhF0OL6n1999/y0033SQ5cuSQTz/9VH766Sd5+eWXpUCBAt59XnjhBZk4caJMnTpVNmzYIHny5JHo6GiJi4tz9LWgNAIAAAAZZty4cRIVFSUzZszwritVqpRPNnjChAkyaNAgadWqlVk3a9YsCQ8Pl6VLl0q7du0cGwsZYQAAAIu4/LSo2NhYnyU+Pl6S+uCDD6R27dpy7733yrXXXivXX3+9TJs2zbt9z549cvDgQVMO4REWFiZ16tSRdevWiZMIhAEAACziz9KIqKgoE7R6lrFjx170+Lt375YpU6ZIuXLlZPny5fLEE0/I008/Le+8847ZrkGw0gxwYnrbs80plEYAAADAETExMRIaGuq9HRwcfNE+58+fNxnhMWPGmNuaEf7hhx9MPXCHDh0kI5ERBgAAsLB9WpDDi9IgOPGSXCCsnSAqV67ss65SpUqyd+9e83tERIT5eejQIZ999LZnm5OvBQAAAJAhtGPEjh07fNb98ssvUqJECe/EOQ14V65c6d2u9cbaPaJevXqOjoXSCAAAAItk9iWWe/bsKfXr1zelEffdd59s3LhR3nzzTbN4jtWjRw8ZNWqUqSPWwHjw4MESGRkprVu3dnTcBMIAAADIMDfccIMsWbJEBg4cKCNGjDCBrrZLa9++vXeffv36yalTp6Rr165y7NgxadCggSxbtkxCQkIcHQuBMAAAgEUStztzSlqPd+edd5olxeO5XCZI1sWfqBEGAACAlcgIAwAAWETLeR0uERanj5dRCIQBAAAsEiQuszjJ6eNlFEojAAAAYCUywgAAABahNCIBGWEAAABYiYwwAACARVz/+89JTh8vo5ARBgAAgJXICAMAAFiEGuEEZIQBAABgJTLCAAAAFtF6Xqf7/roCtEaYQBgAAMAilEYkoDQCAAAAViIjDAAAYBEywgnICAMAAMBKZIQBAAAswgU1EpARBgAAgJXICAMAAFgkyHVhcZLTx8soZIQBAABgJTLCAAAAFqFGOAGBMAAAgEVon5aA0ggAAABYiYwwAACARTR563xpRGAiIwwAAAArkREGAACwCO3TEpARBgAAgJXICAMAAFiE9mkJyAgDAADASgTCSTzyyCPSunVrR4/51VdficvlkmPHjqX7WHqcpUuXir84OVZkPdPfnCo33Xi9FA8vYJZmt94kK5Z/6t1+Z3RjKZA7u8/Ss/uTmTpm4Gp0U80y8v6Ex2T3Z6Pln+9ek5a3VvPZ3qpxdfnw9W6y78txZnu18kUvOsbyac+YbYmXic+1y8BngUDvI+xyeAlEQZkddGrQ9fzzz/us10BP16dFyZIlZcKECanaT4+tS548eaRmzZqycOFC8af69evLgQMHJCwsLN3H0uM0b97ckXHBPpFFi8rQEaPly282yhdrNsjNDRtJ+/vayPaffvTu06FjF/l59z7vMny07/kJIP3y5AqWbb/8IT3Gzk92e+5cOWXt1l9l0MRLJz6mL/pGSjYd6F2em+C/RAmutvZpzi+BKNNrhENCQmTcuHHy2GOPSYECBTLkMUeMGCGPPvqoxMbGyssvvyz/+c9/pGjRoiZg9YecOXNKREREuo5x5swZR44DuzVv0dLn9uDho+Ttt96QzRs3SKXKVcy6XLlzSzh/Z4BfffbNT2ZJyXsfbzI/ixcpeMnj/BN3Rg4dOeH4+ABbZHppRNOmTU1wN3bs2Evut2jRIqlSpYoEBwebrK4GsB633nqr/P7779KzZ09vtvdS8uXLZx6zfPnyMnnyZMmVK5d8+OGHye67bNkyadCggeTPn18KFSokd955p/z6668++6xdu1Zq1KhhgvratWt7M9pbt25NttzgyJEjcv/995vgO3fu3FK1alV57733fI6pz+mpp56SHj16yDXXXCPR0dEXlUYMGzbM+3wTLzNnzjTbz58/b17XUqVKmedYvXp1ef/9930e55NPPjGvg25v1KiR/Pbbb5d87XD1OHfunCxaOF9OnzolN9Sp612/cP5cKRMVLvVqV5fhQ56V06dPZ+o4AaTsP3fUlpgvnpfNC5+VEd3vklwhOTJ7SAgAQeKSIJfDS4DmhDM9I5wtWzYZM2aMPPDAA/L0009LsWLFLtpny5Ytct9995nAT7O3Gng++eSTJjDV8orFixebIK9r164m05sW2bNnlxw5cpiMa3JOnTolvXr1kmrVqsnJkydlyJAhcvfdd5sgNygoyGSVW7ZsKXfccYfMnTvXBOQavF5KXFyc1KpVS/r37y+hoaHy8ccfy0MPPSRlypSRG2+80bvfO++8I0888YR88803yR6nT58+8vjjj3tvz5kzx4xPg3GlQfC7774rU6dOlXLlysnq1avlwQcflMKFC0vDhg0lJiZG2rRpI926dTOv3ebNm6V3796XHHt8fLxZPPT5I7D8+MM2iW7UwPwd5smbV2bPe18qVqpstt1z3/0SVby4RBSJNPsNHzRQdv3yi9kHQNYy/9PNsvfAUTnw53GpWi5SRj3TSsqXuFba9Xkrs4cGBIxMD4SVBpaaUR06dKhMnz79ou2vvPKKNGnSRAYPHmxuawbzp59+khdffNEEwgULFjQBtSfTm1oa/Gpm+fjx49K4ceNk92nbtq3P7bffftsEkvr41113nQl+NQs7bdo0kxGuXLmy/PHHH5cMyDUTrEGsR/fu3WX58uWyYMECn0BYg9cXXnghxePkzZvXLGr9+vUyaNAgEzzruDRY1Q8Yn3/+udSrV8/sU7p0aVmzZo288cYbJhCeMmWKCb492fUKFSrItm3bTKlKSjS4Hj58eIrbkfWVK19BVq/fIrHHj8t/ly6SJ7t2ko+Wf2GC4Uc6J/zdVrmuqjmfWt3RTPbs/lVKlS6TqeMG4OvtxQlJkh937ZcDf8XKsjefllLFrpE9+/7K1LEha/NHTa9LAlOml0Z4aPClQdz27dsv2qbrbrrpJp91envnzp3m69200kysBpBalqCPq5P1WrRokey++hhaxqBBpGZvtSxD7d271/zcsWOHyRZrEOyROJhNjo555MiRpiRCg3gdiwbCnmN6aNY4NfR+2ulCg2vNnKtdu3aZr7Rvu+02b8Csy6xZs7ylHfq61qlTx+dYnqA5JQMHDjQfHDyLZpURWLTWvHSZslKjZi0ZOmKMXFe1mkydPCnZfWvdcOHvY/evuzJ4lADSatO2C6VtZaIKZ/ZQgICRJTLC6pZbbjF1sBpoaZbXn/r27WseQwPD8PDwS9YUa9lDiRIlTMY3MjLS1N1qxjWlUorU0Ez2q6++arpcaDCs3Su0nCLpMXX95Wjpxl133WUCWJ0E6KFlHErLLjQDnZjWWV8pvW967o+sR/+mz5xJKHdJbNv/XahzD48oksGjApBW1StcKC08+NfxzB4KsjpSwlkvEFaamdUSCf2KPrFKlSpdVCert7VEQksiPFmu1GaHdfJZ2bJlL7ufTmrTjK8GwTfffLNZp6UFielYtQ5XSxE8AeKmTRdm+6ZEx96qVStTr+sJRH755RdTVpEWbrfbHEPvP3v2bJ+AXo+l49FssZZBJEdf1w8++MBnnZZY4Oqlk9+aNrtdoqKKy4kTJ+T9Be/JmtWrZNEHn5jyh/fnvye3RTeXgoUKyQ/btslz/XtL/QY3m6wxAOfkyZXTJ3Nbsmgh0yv479jTEnPwbykQmluiIgpIkWsvtN0sXzLc/Dx0JNZ0idDyh/80ry3L1/woR46dkqrli8oLvdvI11t2yg8792fa8wICTZYKhDU72r59e5k4caLPep3AdcMNN5hyAp0st27dOnnttdfk9ddf9+6jJQs6Gaxdu3YmANRgN720nZtOyHvzzTelSJEiJqgcMGCAzz46ye+5554zk810m+7z0ksvmW0pZZq19le7N+ikP30MrYE+dOhQmgNhnTyoNcCfffaZyQB7ssDar1jrpbVUQjtpaKCsnS+0lEGDcC3x6NChg5lop/XBmiHv0qWLmZTo6TiBq9Nfh/+UJ7p0lEMHD0hoWJipA9YguFGT22Tfvhj56suVMmXyRNNJomixKGnZ+m7p0/+5zB42cNWpWbmEfPbWM97bL/S5MB9l9gfrpevQd6VFw6oybcRD3u2zx3UyP0dN/URGv/GJnD37rzSuU0GeeqCRCar3Hfpblq7cKs+/tTwTng0CDZdYzqKBsNKv9+fP920wrhe90Ilk2hFBg2ENSnW/xCUUelt7EevkL83OarY0vbQrxLx580w3Cy2H0OyvBuna2sxDg0ptvabdHTSbrcG8jlMD5MR1w4nppLbdu3ebUhCtU9YgWmt8NVBNi1WrVpngN2n/4xkzZpjXRl8rndinE9z08bQFnL6Wzz77rNmvePHipi2dBsuTJk0ytc06wa5Tpwv/4OLqM2nqtBS3FSsWJR9/9mWGjgewlWZuc13/VIrb3/1wg1lSsu/QMWnW5VU/jQ5XPX9cCc4lAcnldiJihA9tY9axY0cT2Gp/3quZtk/TDPTvB4+aDwUAso4i9RMyjgCyBve5MxK/bZqJETL6fdPznr1y617Jm8/Zxz55Ilaa1CieKc/rqsoIByLtxKBdJXRS2vfff2+6Umj3hqs9CAYAAIGHuXIJCIQdcPDgQVMOoT+1bOPee++V0aNHZ/awAAAAcAkEwg7o16+fWQAAALI8UsJZ74IaAAAAQEYiEAYAALCwfZrL4f/S2gJW28wmXipWrOjdHhcXJ926dTNtbPUCaG3btjWtZp1GIAwAAIAMV6VKFTlw4IB3SXzRMm3tqu1pFy5caNrF7t+/X9q0aeP4GKgRBgAAsIjLD32EXVdwvOzZs0tERMRF67UF2/Tp02Xu3LnSuHFj7zUS9Iq4egXcunXrilPICAMAAMCxXsWJF73IWUp27twpkZGRpgWtXllYr86r9Eq3Z8+elaZNm3r31bIJvRCYXl3YSQTCAAAAFjaNcDm8qKioKHPRDs+iV7dNTp06dWTmzJmybNkymTJliuzZs0duvvlmOXHihGlHmzNnTnNF3MTCw8PNNidRGgEAAGATP7ZPi4mJ8bmyXHBwcLK7N2/e3Pt7tWrVTGBcokQJWbBgQYZekIyMMAAAAByhQXDiJaVAOCnN/pYvX1527dpl6obPnDkjx44d89lHu0YkV1OcHgTCAAAAFskK7dOSOnnypPz666/mCr21atWSHDlyyMqVK73bd+zYYWqI69WrJ06iNAIAAAAZqk+fPtKyZUtTDqGt0YYOHSrZsmWT+++/39QWd+7cWXr16iUFCxY0meXu3bubINjJjhGKQBgAAMAiWaF92r59+0zQe+TIESlcuLA0aNDAtEbT39X48eMlKCjIXEhDO09ER0fL66+/7uygCYQBAACQ0ebNm3fJ7SEhITJ58mSz+BOBMAAAgEX82DQi4DBZDgAAAFYiIwwAAGATUsJeBMIAAAAWcaLdWVJOHy+jUBoBAAAAK5ERBgAAsEhWaJ+WVZARBgAAgJXICAMAAFiEuXIJyAgDAADASmSEAQAAbEJK2IuMMAAAAKxERhgAAMAi9BFOQCAMAABgEdqnJaA0AgAAAFYiIwwAAGAR5solICMMAAAAK5ERBgAAsAkpYS8ywgAAALASGWEAAACL0D4tARlhAAAAWImMMAAAgEXoI5yAQBgAAMAizJVLQGkEAAAArERGGAAAwCakhL3ICAMAAMBKZIQBAAAsQvu0BGSEAQAAYCUywgAAADbxQ/s0CcyEMBlhAAAA2ImMMAAAgEVoGpGAQBgAAMAmRMJelEYAAADASmSEAQAALEL7tARkhAEAAGAlMsIAAAAWcfmhfZorMBPCZIQBAABgJzLCAAAAFqFpRAIywgAAALASGWEAAACbkBL2IhAGAACwCO3TElAaAQAAACuREQYAALCtMsLp9mkSmMgIAwAAwEpkhAEAACzCXLkEZIQBAABgJQJhAAAACy+x7HJ4uVLPP/+8uFwu6dGjh3ddXFycdOvWTQoVKiR58+aVtm3byqFDh8RpBMIAAADIFJs2bZI33nhDqlWr5rO+Z8+e8uGHH8rChQtl1apVsn//fmnTpo3jj08gDAAAYGWVsMvhJW1Onjwp7du3l2nTpkmBAgW8648fPy7Tp0+XV155RRo3biy1atWSGTNmyNq1a2X9+vWOvhIEwgAAABbxZ2lEbGyszxIfH5/iOLT0oUWLFtK0aVOf9Vu2bJGzZ8/6rK9YsaIUL15c1q1b5+hrQSAMAAAAR0RFRUlYWJh3GTt2bLL7zZs3T7799ttktx88eFBy5swp+fPn91kfHh5utjmJ9mkAAAAW8Wf7tJiYGAkNDfWuDw4Ovmhf3eeZZ56RFStWSEhIiGQmMsIAAABwhAbBiZfkAmEtfTh8+LDUrFlTsmfPbhadEDdx4kTzu2Z+z5w5I8eOHfO5n3aNiIiIECeREQYAALBIetudJSctx2vSpIls27bNZ13Hjh1NHXD//v1NeUWOHDlk5cqVpm2a2rFjh+zdu1fq1asnTiIQBgAAQIbJly+fXHfddT7r8uTJY3oGe9Z37txZevXqJQULFjSZ5e7du5sguG7duo6OhUAYAADAIq7//eckp483fvx4CQoKMhlh7TwRHR0tr7/+ujiNQBgAAACZ6quvvvK5rZPoJk+ebBZ/IhAGAACwiT/bRgQYAmEAAACLEAcnoH0aAAAArERGGAAAwCKZ3T4tKyEjDAAAACuREQYAALBIILRPyyhkhAEAAGAlMsIAAAA2oW2EFxlhAAAAWImMMAAAgEVICCcgIwwAAAArkREGAACwCH2EExAIAwAAWMX59mkSoMURlEYAAADASmSEAQAALEJpRAIywgAAALASgTAAAACsRCAMAAAAK1EjDAAAYBFqhBOQEQYAAICVyAgDAABY10XY2RSu832JMwaBMAAAgEUojUhAaQQAAACsREYYAADAIpq85QLLF5ARBgAAgJXICAMAANiElLAXGWEAAABYiYwwAACARWifloCMMAAAAKxERhgAAMAi9BFOQCAMAABgEebKJaA0AgAAAFYiIwwAAGATUsJeZIQBAABgJTLCAAAAFqF9WgIywgAAALASGWEAAACL0D4tAYEw0sXtdpufJ07EZvZQACThPncms4cAIIXz0vP+mRliY2MD4pgZgUAY6XLixAnz87pyJTN7KAAABNT7Z1hYWIY+Zs6cOSUiIkLKlYryy/EjIiLMYwQSlzszP5Ig4J0/f172798v+fLlE1egfi8C76f5qKgoiYmJkdDQ0MweDoD/4dy8umjYpUFwZGSkBAVl/FStuLg4OXPGP98W5cyZU0JCQiSQkBFGuuhJXKxYscweBhykb7S82QJZD+fm1SOjM8GJaaAaaMGqP9E1AgAAAFYiEAYAAICVCIQBGMHBwTJ06FDzE0DWwbkJ+A+T5QAAAGAlMsIAAACwEoEwAAAArEQgDAAAACsRCANAgHvkkUekdevW3tu33nqr9OjRI8PH8dVXX5kL6xw7dizDHxsArgSBMAD4MUDVwFAXveJS2bJlZcSIEfLvv//69XEXL14sI0eOTNW+BK8AbMaV5QDAj26//XaZMWOGxMfHyyeffCLdunWTHDlyyMCBA33200uearDshIIFCzpyHAC42pERBgA/0t6vERERUqJECXniiSekadOm8sEHH3jLGUaPHi2RkZFSoUIFs39MTIzcd999kj9/fhPQtmrVSn777Tfv8c6dOye9evUy2wsVKiT9+vWTpF0wk5ZGaBDev39/iYqKMuPRzPT06dPNcRs1amT2KVCggMkM67jU+fPnZezYsVKqVCnJlSuXVK9eXd5//32fx9HAvnz58ma7HifxOAEgEBAIA0AG0qBRs79q5cqVsmPHDlmxYoV89NFHcvbsWYmOjpZ8+fLJ119/Ld98843kzZvXZJU993n55Zdl5syZ8vbbb8uaNWvk6NGjsmTJkks+5sMPPyzvvfeeTJw4UbZv3y5vvPGGOa4GxosWLTL76DgOHDggr776qrmtQfCsWbNk6tSp8uOPP0rPnj3lwQcflFWrVnkD9jZt2kjLli1l69at0qVLFxkwYICfXz0AcBalEQCQATRrq4Hv8uXLpXv37vLnn39Knjx55K233vKWRLz77rsmE6vrNDurtKxCs79ay9usWTOZMGGCKavQIFRpoKrHTMkvv/wiCxYsMMG2ZqNV6dKlLyqjuPbaa83jeDLIY8aMkc8//1zq1avnvY8G3hpEN2zYUKZMmSJlypQxgbnSjPa2bdtk3LhxfnoFAcB5BMIA4Eea6dXsq2Z7Nch94IEHZNiwYaZWuGrVqj51wd9//73s2rXLZIQTi4uLk19//VWOHz9usrZ16tTxbsuePbvUrl37ovIID83WZsuWzQSvqaVjOH36tNx2220+6zUrff3115vfNbOceBzKEzQDQKAgEAYAP9LaWc2easCrtcAauHpoRjixkydPSq1atWTOnDkXHadw4cJXXIqRVjoO9fHHH0vRokV9tmmNMQBcLQiEAcCPNNjVyWmpUbNmTZk/f74pUwgNDU12nyJFisiGDRvklltuMbe1FduWLVvMfZOjWWfNRGttr6c0IjFPRlon4XlUrlzZBLx79+5NMZNcqVIlM+kvsfXr16fqeQJAVsFkOQDIItq3by/XXHON6RShk+X27NljaoOffvpp2bdvn9nnmWeekeeff16WLl0qP//8szz55JOX7AFcsmRJ6dChg3Tq1Mncx3NMrRtW2s1C65G1hEPrljUbrKUZffr0MRPk3nnnHVOW8e2338qkSZPMbfX444/Lzp07pW/fvmai3dy5c80kPgAIJATCAJBF5M6dW1avXi3Fixc3k+E069q5c2dTI+zJEPfu3VseeughE9xqTa4GrXffffclj6ulGffcc48JmitWrCiPPvqonDp1ymzT0ofhw4ebjg/h4eHy1FNPmfV6QY7Bgweb7hE6Du1coaUS2k5N6Ri144QG19paTSft6QQ7AAgkLndKMywAAACAqxgZYQAAAFiJQBgAAABWIhAGAACAlQiEAQAAYCUCYQAAAFiJQBgAAABWIhAGAACAlQiEAQAAYCUCYQAAAFiJQBgAAABWIhAGAACAlQiEAQAAIDb6f/XipTHwCGzdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_plagio = sum(1 for _, _, label in test_pairs if label == 1)\n",
    "num_no_plagio = sum(1 for _, _, label in test_pairs if label == 0)\n",
    "\n",
    "print(f\"Ejemplos con plagio (1): {num_plagio}\")\n",
    "print(f\"Ejemplos sin plagio (0): {num_no_plagio}\")\n",
    "\n",
    "cm = compute_confusion_matrix(loaded_model, test_pairs, device, batch_size=batch_size, threshold=best_threshold)\n",
    "plot_confusion_matrix(cm, labels=['Plagiarized', 'Not Plagiarized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f54be710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2363, Test Accuracy: 74.91%\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = evaluate(loaded_model, test_pairs, device, best_threshold, batch_size=batch_size)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
