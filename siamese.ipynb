{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bb8de6",
   "metadata": {},
   "source": [
    "<h1>Use siamese GNN to predict the similarity of two source codes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac99c",
   "metadata": {},
   "source": [
    "<h3>Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4390b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java as ts_java\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, GINConv\n",
    "import torch.nn.functional as F, torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67737eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all seeds to make results reproducible.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d8a2e",
   "metadata": {},
   "source": [
    "<h2>Data preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b41d0",
   "metadata": {},
   "source": [
    "<h3>Define constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dad6d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_directory1 = './datasets/conplag_preprocessed'\n",
    "java_directory2 = './datasets/ir_plag_preprocessed'\n",
    "java_LANGUAGE = Language(ts_java.language())\n",
    "parser = Parser(java_LANGUAGE)\n",
    "csv_paths = ['./labels/conplag-labels.csv', './labels/ir_plag_labels.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6b312cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93039af9",
   "metadata": {},
   "source": [
    "<h3>Get AST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3185acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node, parent_idx=None):\n",
    "        idx = len(nodes)\n",
    "        nodes.append(node.type)\n",
    "        \n",
    "        if parent_idx is not None:\n",
    "            edges.append((parent_idx, idx))\n",
    "        \n",
    "        for child in node.children:\n",
    "            traverse(child, idx)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f384178",
   "metadata": {},
   "source": [
    "<h3>Build data for GNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3b169cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_vocab(java_directories, file_lists):\n",
    "    all_node_types = set()\n",
    "\n",
    "    for java_directory, file_list in zip(java_directories, file_lists):\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(java_directory, file_name)\n",
    "            nodes, _ = parse_java_file(file_path)\n",
    "            all_node_types.update(nodes)\n",
    "\n",
    "    node_type_to_idx = {typ: idx for idx, typ in enumerate(sorted(all_node_types))}\n",
    "    return node_type_to_idx\n",
    "\n",
    "def create_node_features(nodes, node_type_to_idx):\n",
    "    node_features = [node_type_to_idx[typ] for typ in nodes]\n",
    "    return node_features\n",
    "\n",
    "def create_graph_data(nodes, edges, node_features, embedding_layer):\n",
    "    x = embedding_layer(torch.tensor(node_features))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9ee31811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_node_types, embedding_dim):\n",
    "        super(NodeEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_node_types, embedding_dim)\n",
    "\n",
    "    def forward(self, node_indices):\n",
    "        return self.embeddings(node_indices)\n",
    "    \n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7ae20970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pairs(pairs_df, java_directory, node_type_to_idx, embedding_layer):\n",
    "    data_pairs = []\n",
    "    for idx, row in pairs_df.iterrows():\n",
    "        file1, file2, label = row['id1'], row['id2'], row['plagio']\n",
    "\n",
    "        file1_path = os.path.join(java_directory, file1)\n",
    "        file2_path = os.path.join(java_directory, file2)\n",
    "\n",
    "        nodes1, edges1 = parse_java_file(file1_path)\n",
    "        nodes2, edges2 = parse_java_file(file2_path)\n",
    "\n",
    "        node_features1 = create_node_features(nodes1, node_type_to_idx)\n",
    "        node_features2 = create_node_features(nodes2, node_type_to_idx)\n",
    "\n",
    "        data1 = create_graph_data(nodes1, edges1, node_features1, embedding_layer)\n",
    "        data2 = create_graph_data(nodes2, edges2, node_features2, embedding_layer)\n",
    "\n",
    "        data_pairs.append((data1, data2, label))\n",
    "        \n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c7ddf221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "Number of pairs in training dataset: 846\n",
      "Number of pairs in validation set: 133\n",
      "Number of pairs in test set: 267\n",
      "Dataset 1 - First pair:\n",
      "  Graph 1: 919 nodes, 918 edges\n",
      "  Graph 2: 1246 nodes, 1245 edges\n",
      "  Label: 0\n",
      "Dataset 2 - First pair:\n",
      "  Graph 1: 108 nodes, 107 edges\n",
      "  Graph 2: 109 nodes, 108 edges\n",
      "  Label: 1\n"
     ]
    }
   ],
   "source": [
    "pairs_df1 = load_csv(csv_paths[0])\n",
    "pairs_df2 = load_csv(csv_paths[1])\n",
    "\n",
    "file_list1 = list(set(pairs_df1['id1'].tolist() + pairs_df1['id2'].tolist()))\n",
    "file_list2 = list(set(pairs_df2['id1'].tolist() + pairs_df2['id2'].tolist()))\n",
    "\n",
    "java_directories = [java_directory1, java_directory2]\n",
    "file_lists = [file_list1, file_list2]\n",
    "\n",
    "node_type_to_idx = build_global_vocab(java_directories, file_lists)\n",
    "embedding_layer = NodeEmbeddingLayer(len(node_type_to_idx), embedding_dim)\n",
    "\n",
    "data_pairs1 = prepare_data_for_pairs(pairs_df1, java_directory1, node_type_to_idx, embedding_layer)\n",
    "data_pairs2 = prepare_data_for_pairs(pairs_df2, java_directory2, node_type_to_idx, embedding_layer)\n",
    "\n",
    "all_pairs = data_pairs1 + data_pairs2\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "test_size = int(len(all_pairs) * 0.2)\n",
    "val_size = int(len(all_pairs) * 0.1)\n",
    "\n",
    "test_pairs = all_pairs[:test_size]\n",
    "val_pairs = all_pairs[test_size:test_size+val_size]\n",
    "train_pairs = all_pairs[test_size+val_size:]\n",
    "\n",
    "plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 1]\n",
    "non_plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 0]\n",
    "\n",
    "if len(plagiarism_pairs) > len(non_plagiarism_pairs):\n",
    "    plagiarism_pairs = random.sample(plagiarism_pairs, len(non_plagiarism_pairs))\n",
    "else:\n",
    "    non_plagiarism_pairs = random.sample(non_plagiarism_pairs, len(plagiarism_pairs))\n",
    "\n",
    "balanced_train_pairs = plagiarism_pairs + non_plagiarism_pairs\n",
    "random.shuffle(balanced_train_pairs)\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"Number of pairs in training dataset: {len(balanced_train_pairs)}\")\n",
    "print(f\"Number of pairs in validation set: {len(val_pairs)}\")\n",
    "print(f\"Number of pairs in test set: {len(test_pairs)}\")\n",
    "data1, data2, label1 = data_pairs1[0]\n",
    "data3, data4, label2 = data_pairs2[0]\n",
    "print(f\"Dataset 1 - First pair:\")\n",
    "print(f\"  Graph 1: {data1.num_nodes} nodes, {data1.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data2.num_nodes} nodes, {data2.num_edges} edges\")\n",
    "print(f\"  Label: {label1}\")\n",
    "print(f\"Dataset 2 - First pair:\")\n",
    "print(f\"  Graph 1: {data3.num_nodes} nodes, {data3.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data4.num_nodes} nodes, {data4.num_edges} edges\")\n",
    "print(f\"  Label: {label2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b01de",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdc33d",
   "metadata": {},
   "source": [
    "<h3>Build GNN siamese architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9c79f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNNEncoder(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super(GNNEncoder, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = global_mean_pool(x, batch)\n",
    "#         return x\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_channels, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        nn3 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        nn4 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        nn5 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = GNNEncoder(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        h1 = self.encoder(data1.x, data1.edge_index, data1.batch)\n",
    "        h2 = self.encoder(data2.x, data2.edge_index, data2.batch)\n",
    "        return h1, h2\n",
    "\n",
    "def contrastive_loss(h1, h2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(h1, h2)\n",
    "    loss = (label * torch.pow(distance, 2) + \n",
    "           (1 - label) * torch.pow(F.relu(margin - distance), 2))\n",
    "    return loss.mean()\n",
    "\n",
    "def collate_fn(pairs, device):\n",
    "    data1_list, data2_list, labels = [], [], []\n",
    "    for d1, d2, label in pairs:\n",
    "        data1_list.append(d1)\n",
    "        data2_list.append(d2)\n",
    "        labels.append(label)\n",
    "\n",
    "    batch1 = Batch.from_data_list(data1_list).to(device)\n",
    "    batch2 = Batch.from_data_list(data2_list).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float, device=device).to(device)\n",
    "\n",
    "    return batch1, batch2, labels\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Cuántas épocas esperar después de la última mejora.\n",
    "            min_delta (float): Mínima mejora en la métrica para ser considerada como mejora.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(f\"Early stopping triggered after {self.patience} epochs without improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a974c0",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f8dc2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_pairs, val_pairs, device, scheduler, threshold, epochs=50, batch_size=32):\n",
    "    set_seed(42)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_threshold = threshold\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        random.Random(42 + epoch).shuffle(train_pairs)\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(0, len(train_pairs), batch_size):\n",
    "            batch_pairs = train_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            predictions = (distances < best_threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(train_pairs)\n",
    "        train_accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        val_loss, val_accuracy = evaluate(model, val_pairs, device, best_threshold, batch_size)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}% Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            new_threshold, _ = find_best_threshold(model, val_pairs, device, batch_size=batch_size)\n",
    "            print(f\"Updated threshold: {new_threshold:.4f}\")\n",
    "            best_threshold = new_threshold\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                'epoch': epoch,\n",
    "                'threshold': best_threshold,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': val_accuracy\n",
    "            }\n",
    "            torch.save(checkpoint, f'./models/siamese_gnn_model_epoch_{epoch+1}.pth')\n",
    "            print(f\"Saved best model at epoch {epoch+1} with threshold {best_threshold:.4f}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return best_threshold, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "def evaluate(model, data_pairs, device, threshold, batch_size=32):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_pairs), batch_size):\n",
    "            batch_pairs = data_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_pairs)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def find_best_threshold(model, dataset, device, batch_size=32, thresholds=np.linspace(0, 2, 100)):\n",
    "    model.eval()\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            all_preds_raw.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds_raw = np.array(all_preds_raw)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = (all_preds_raw < threshold).astype(int)\n",
    "        f1 = f1_score(all_labels, preds)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f03c5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e399511",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "out_dim = 32\n",
    "threshold = 0.2626\n",
    "epochs = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b45679b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Train Loss: 0.4248, Accuracy: 48.11% Val Loss: 0.5625, Accuracy: 40.60%\n",
      "Saved best model at epoch 1 with threshold 0.2626\n",
      "Epoch 2/200 Train Loss: 0.3637, Accuracy: 43.85% Val Loss: 0.4872, Accuracy: 42.86%\n",
      "Saved best model at epoch 2 with threshold 0.2626\n",
      "Epoch 3/200 Train Loss: 0.3375, Accuracy: 46.22% Val Loss: 0.4631, Accuracy: 46.62%\n",
      "Saved best model at epoch 3 with threshold 0.2626\n",
      "Epoch 4/200 Train Loss: 0.3246, Accuracy: 47.99% Val Loss: 0.4087, Accuracy: 52.63%\n",
      "Saved best model at epoch 4 with threshold 0.2626\n",
      "Epoch 5/200 Train Loss: 0.3199, Accuracy: 47.52% Val Loss: 0.4161, Accuracy: 53.38%\n",
      "Updated threshold: 0.2020\n",
      "Epoch 6/200 Train Loss: 0.3166, Accuracy: 47.64% Val Loss: 0.4186, Accuracy: 56.39%\n",
      "Epoch 7/200 Train Loss: 0.3008, Accuracy: 47.99% Val Loss: 0.3898, Accuracy: 61.65%\n",
      "Saved best model at epoch 7 with threshold 0.2020\n",
      "Epoch 8/200 Train Loss: 0.2963, Accuracy: 47.16% Val Loss: 0.3820, Accuracy: 61.65%\n",
      "Saved best model at epoch 8 with threshold 0.2020\n",
      "Epoch 9/200 Train Loss: 0.2868, Accuracy: 51.54% Val Loss: 0.3614, Accuracy: 62.41%\n",
      "Saved best model at epoch 9 with threshold 0.2020\n",
      "Epoch 10/200 Train Loss: 0.2695, Accuracy: 53.43% Val Loss: 0.3451, Accuracy: 66.17%\n",
      "Updated threshold: 0.2626\n",
      "Saved best model at epoch 10 with threshold 0.2626\n",
      "Epoch 11/200 Train Loss: 0.2617, Accuracy: 56.86% Val Loss: 0.3428, Accuracy: 65.41%\n",
      "Saved best model at epoch 11 with threshold 0.2626\n",
      "Epoch 12/200 Train Loss: 0.2642, Accuracy: 54.96% Val Loss: 0.3656, Accuracy: 63.91%\n",
      "Epoch 13/200 Train Loss: 0.2618, Accuracy: 56.74% Val Loss: 0.3478, Accuracy: 62.41%\n",
      "Epoch 14/200 Train Loss: 0.2670, Accuracy: 60.17% Val Loss: 0.3302, Accuracy: 62.41%\n",
      "Saved best model at epoch 14 with threshold 0.2626\n",
      "Epoch 15/200 Train Loss: 0.2563, Accuracy: 61.82% Val Loss: 0.3296, Accuracy: 61.65%\n",
      "Updated threshold: 0.1414\n",
      "Saved best model at epoch 15 with threshold 0.1414\n",
      "Epoch 16/200 Train Loss: 0.2480, Accuracy: 53.55% Val Loss: 0.2963, Accuracy: 72.93%\n",
      "Saved best model at epoch 16 with threshold 0.1414\n",
      "Epoch 17/200 Train Loss: 0.2396, Accuracy: 50.95% Val Loss: 0.2941, Accuracy: 72.93%\n",
      "Saved best model at epoch 17 with threshold 0.1414\n",
      "Epoch 18/200 Train Loss: 0.2226, Accuracy: 51.54% Val Loss: 0.3033, Accuracy: 69.17%\n",
      "Epoch 19/200 Train Loss: 0.2354, Accuracy: 52.01% Val Loss: 0.3067, Accuracy: 66.17%\n",
      "Epoch 20/200 Train Loss: 0.2300, Accuracy: 53.43% Val Loss: 0.2950, Accuracy: 70.68%\n",
      "Updated threshold: 0.2222\n",
      "Epoch 21/200 Train Loss: 0.2249, Accuracy: 58.39% Val Loss: 0.2670, Accuracy: 69.17%\n",
      "Saved best model at epoch 21 with threshold 0.2222\n",
      "Epoch 22/200 Train Loss: 0.2307, Accuracy: 60.87% Val Loss: 0.2765, Accuracy: 72.18%\n",
      "Epoch 23/200 Train Loss: 0.2215, Accuracy: 60.52% Val Loss: 0.2694, Accuracy: 70.68%\n",
      "Epoch 24/200 Train Loss: 0.2248, Accuracy: 62.17% Val Loss: 0.2757, Accuracy: 72.93%\n",
      "Epoch 25/200 Train Loss: 0.2209, Accuracy: 64.54% Val Loss: 0.2605, Accuracy: 72.18%\n",
      "Updated threshold: 0.3232\n",
      "Saved best model at epoch 25 with threshold 0.3232\n",
      "Epoch 26/200 Train Loss: 0.2047, Accuracy: 70.69% Val Loss: 0.2590, Accuracy: 71.43%\n",
      "Saved best model at epoch 26 with threshold 0.3232\n",
      "Epoch 27/200 Train Loss: 0.1984, Accuracy: 72.81% Val Loss: 0.2363, Accuracy: 68.42%\n",
      "Saved best model at epoch 27 with threshold 0.3232\n",
      "Epoch 28/200 Train Loss: 0.2036, Accuracy: 72.34% Val Loss: 0.2286, Accuracy: 70.68%\n",
      "Saved best model at epoch 28 with threshold 0.3232\n",
      "Epoch 29/200 Train Loss: 0.2029, Accuracy: 71.51% Val Loss: 0.2269, Accuracy: 73.68%\n",
      "Saved best model at epoch 29 with threshold 0.3232\n",
      "Epoch 30/200 Train Loss: 0.2058, Accuracy: 72.58% Val Loss: 0.2293, Accuracy: 77.44%\n",
      "Updated threshold: 0.3232\n",
      "Epoch 31/200 Train Loss: 0.2011, Accuracy: 72.10% Val Loss: 0.2091, Accuracy: 75.94%\n",
      "Saved best model at epoch 31 with threshold 0.3232\n",
      "Epoch 32/200 Train Loss: 0.2023, Accuracy: 71.51% Val Loss: 0.2159, Accuracy: 71.43%\n",
      "Epoch 33/200 Train Loss: 0.2080, Accuracy: 71.04% Val Loss: 0.2115, Accuracy: 73.68%\n",
      "Epoch 34/200 Train Loss: 0.1981, Accuracy: 72.22% Val Loss: 0.2334, Accuracy: 73.68%\n",
      "Epoch 35/200 Train Loss: 0.1892, Accuracy: 75.65% Val Loss: 0.2161, Accuracy: 77.44%\n",
      "Updated threshold: 0.2020\n",
      "Epoch 36/200 Train Loss: 0.2018, Accuracy: 62.17% Val Loss: 0.2073, Accuracy: 82.71%\n",
      "Saved best model at epoch 36 with threshold 0.2020\n",
      "Epoch 37/200 Train Loss: 0.1987, Accuracy: 65.84% Val Loss: 0.2264, Accuracy: 81.20%\n",
      "Epoch 38/200 Train Loss: 0.1940, Accuracy: 65.01% Val Loss: 0.2037, Accuracy: 82.71%\n",
      "Saved best model at epoch 38 with threshold 0.2020\n",
      "Epoch 39/200 Train Loss: 0.1938, Accuracy: 60.05% Val Loss: 0.2117, Accuracy: 81.95%\n",
      "Epoch 40/200 Train Loss: 0.1885, Accuracy: 65.72% Val Loss: 0.2176, Accuracy: 82.71%\n",
      "Updated threshold: 0.2222\n",
      "Epoch 41/200 Train Loss: 0.1962, Accuracy: 67.97% Val Loss: 0.2112, Accuracy: 82.71%\n",
      "Epoch 42/200 Train Loss: 0.1915, Accuracy: 65.37% Val Loss: 0.2025, Accuracy: 82.71%\n",
      "Saved best model at epoch 42 with threshold 0.2222\n",
      "Epoch 43/200 Train Loss: 0.1977, Accuracy: 68.09% Val Loss: 0.2175, Accuracy: 80.45%\n",
      "Epoch 44/200 Train Loss: 0.1890, Accuracy: 68.56% Val Loss: 0.2004, Accuracy: 81.95%\n",
      "Saved best model at epoch 44 with threshold 0.2222\n",
      "Epoch 45/200 Train Loss: 0.1924, Accuracy: 65.72% Val Loss: 0.2026, Accuracy: 83.46%\n",
      "Updated threshold: 0.2424\n",
      "Epoch 46/200 Train Loss: 0.1917, Accuracy: 69.39% Val Loss: 0.2162, Accuracy: 81.20%\n",
      "Epoch 47/200 Train Loss: 0.1863, Accuracy: 69.86% Val Loss: 0.2109, Accuracy: 82.71%\n",
      "Epoch 48/200 Train Loss: 0.1802, Accuracy: 71.51% Val Loss: 0.2092, Accuracy: 81.20%\n",
      "Epoch 49/200 Train Loss: 0.1797, Accuracy: 69.39% Val Loss: 0.2089, Accuracy: 81.95%\n",
      "Epoch 50/200 Train Loss: 0.1802, Accuracy: 69.15% Val Loss: 0.2061, Accuracy: 81.95%\n",
      "Updated threshold: 0.2222\n",
      "Epoch 51/200 Train Loss: 0.1823, Accuracy: 67.38% Val Loss: 0.2074, Accuracy: 82.71%\n",
      "Epoch 52/200 Train Loss: 0.1781, Accuracy: 69.03% Val Loss: 0.2132, Accuracy: 82.71%\n",
      "Epoch 53/200 Train Loss: 0.1841, Accuracy: 69.98% Val Loss: 0.2123, Accuracy: 81.95%\n",
      "Epoch 54/200 Train Loss: 0.1871, Accuracy: 68.32% Val Loss: 0.2135, Accuracy: 83.46%\n",
      "Early stopping at epoch 54\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "best_threshold, train_losses, val_losses, train_accs, val_accs = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    balanced_train_pairs,\n",
    "    val_pairs,\n",
    "    device,\n",
    "    scheduler,\n",
    "    threshold=threshold,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4ca0a",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "02f098f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_with_metadata(model_path, in_channels, hidden_dim, out_dim, device):\n",
    "    loaded_model = SiameseNetwork(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=hidden_dim,\n",
    "        out_channels=out_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    threshold = checkpoint.get('threshold', 0.5)\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    val_loss = checkpoint.get('val_loss', 0)\n",
    "    val_accuracy = checkpoint.get('val_accuracy', 0)\n",
    "    \n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "    print(f\"Model was saved at epoch {epoch+1} with validation loss {val_loss:.4f} and accuracy {val_accuracy*100:.2f}%\")\n",
    "    print(f\"Using threshold: {threshold:.4f}\")\n",
    "    \n",
    "    return loaded_model, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8f09c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./models/siamese_gnn_model_epoch_44.pth\n",
      "Model was saved at epoch 44 with validation loss 0.2004 and accuracy 81.95%\n",
      "Using threshold: 0.2222\n"
     ]
    }
   ],
   "source": [
    "loaded_model, threshold = load_model_with_metadata('./models/siamese_gnn_model_epoch_44.pth', embedding_dim, hidden_dim, out_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2067fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, dataset, device, batch_size=batch_size, threshold=threshold):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [int(round(x)) for x in all_preds]\n",
    "    all_labels = [int(round(x)) for x in all_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[1, 0])\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "\n",
    "    for (i, j), value in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f'{value}', ha='center', va='center', color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f0edce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.2424, Best F1 score: 0.7647\n"
     ]
    }
   ],
   "source": [
    "best_threshold, best_f1 = find_best_threshold(loaded_model, val_pairs, device, batch_size=batch_size, thresholds=np.linspace(0, 2, 100))\n",
    "print(f\"Best threshold: {best_threshold:.4f}, Best F1 score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6428e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos con plagio (1): 117\n",
      "Ejemplos sin plagio (0): 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_289432\\1322368525.py:30: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + labels)\n",
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_289432\\1322368525.py:31: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_plagio = sum(1 for _, _, label in test_pairs if label == 1)\n",
    "num_no_plagio = sum(1 for _, _, label in test_pairs if label == 0)\n",
    "\n",
    "print(f\"Ejemplos con plagio (1): {num_plagio}\")\n",
    "print(f\"Ejemplos sin plagio (0): {num_no_plagio}\")\n",
    "\n",
    "cm = compute_confusion_matrix(loaded_model, test_pairs, device, batch_size=batch_size, threshold=best_threshold)\n",
    "plot_confusion_matrix(cm, labels=['Plagiarized', 'Not Plagiarized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f54be710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2242, Test Accuracy: 72.66%\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = evaluate(loaded_model, test_pairs, device, best_threshold, batch_size=batch_size)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
