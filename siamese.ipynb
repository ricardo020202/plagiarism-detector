{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bb8de6",
   "metadata": {},
   "source": [
    "<h1>Use siamese GNN to predict the similarity of two source codes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac99c",
   "metadata": {},
   "source": [
    "<h3>Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4390b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java as ts_java\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, GINConv\n",
    "import torch.nn.functional as F, torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d8a2e",
   "metadata": {},
   "source": [
    "<h2>Data preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b41d0",
   "metadata": {},
   "source": [
    "<h3>Define constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dad6d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_directory1 = './datasets/conplag_preprocessed'\n",
    "java_directory2 = './datasets/ir_plag_preprocessed'\n",
    "java_LANGUAGE = Language(ts_java.language())\n",
    "parser = Parser(java_LANGUAGE)\n",
    "csv_paths = ['./labels/conplag-labels.csv', './labels/ir_plag_labels.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6b312cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93039af9",
   "metadata": {},
   "source": [
    "<h3>Get AST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3185acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node, parent_idx=None):\n",
    "        idx = len(nodes)\n",
    "        nodes.append(node.type)\n",
    "        \n",
    "        if parent_idx is not None:\n",
    "            edges.append((parent_idx, idx))\n",
    "        \n",
    "        for child in node.children:\n",
    "            traverse(child, idx)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f384178",
   "metadata": {},
   "source": [
    "<h3>Build data for GNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3b169cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_vocab(java_directories, file_lists):\n",
    "    all_node_types = set()\n",
    "\n",
    "    for java_directory, file_list in zip(java_directories, file_lists):\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(java_directory, file_name)\n",
    "            nodes, _ = parse_java_file(file_path)\n",
    "            all_node_types.update(nodes)\n",
    "\n",
    "    node_type_to_idx = {typ: idx for idx, typ in enumerate(sorted(all_node_types))}\n",
    "    return node_type_to_idx\n",
    "\n",
    "def create_node_features(nodes, node_type_to_idx):\n",
    "    node_features = [node_type_to_idx[typ] for typ in nodes]\n",
    "    return node_features\n",
    "\n",
    "def create_graph_data(nodes, edges, node_features, embedding_layer):\n",
    "    x = embedding_layer(torch.tensor(node_features))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9ee31811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_node_types, embedding_dim):\n",
    "        super(NodeEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_node_types, embedding_dim)\n",
    "\n",
    "    def forward(self, node_indices):\n",
    "        return self.embeddings(node_indices)\n",
    "    \n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7ae20970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pairs(pairs_df, java_directory, node_type_to_idx, embedding_layer):\n",
    "    data_pairs = []\n",
    "    for idx, row in pairs_df.iterrows():\n",
    "        file1, file2, label = row['id1'], row['id2'], row['plagio']\n",
    "\n",
    "        file1_path = os.path.join(java_directory, file1)\n",
    "        file2_path = os.path.join(java_directory, file2)\n",
    "\n",
    "        nodes1, edges1 = parse_java_file(file1_path)\n",
    "        nodes2, edges2 = parse_java_file(file2_path)\n",
    "\n",
    "        node_features1 = create_node_features(nodes1, node_type_to_idx)\n",
    "        node_features2 = create_node_features(nodes2, node_type_to_idx)\n",
    "\n",
    "        data1 = create_graph_data(nodes1, edges1, node_features1, embedding_layer)\n",
    "        data2 = create_graph_data(nodes2, edges2, node_features2, embedding_layer)\n",
    "\n",
    "        data_pairs.append((data1, data2, label))\n",
    "        \n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c7ddf221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "Number of pairs in training dataset: 830\n",
      "Number of pairs in validation set: 133\n",
      "Number of pairs in test set: 267\n",
      "Dataset 1 - First pair:\n",
      "  Graph 1: 919 nodes, 918 edges\n",
      "  Graph 2: 1246 nodes, 1245 edges\n",
      "  Label: 0\n",
      "Dataset 2 - First pair:\n",
      "  Graph 1: 108 nodes, 107 edges\n",
      "  Graph 2: 109 nodes, 108 edges\n",
      "  Label: 1\n"
     ]
    }
   ],
   "source": [
    "pairs_df1 = load_csv(csv_paths[0])\n",
    "pairs_df2 = load_csv(csv_paths[1])\n",
    "\n",
    "file_list1 = list(set(pairs_df1['id1'].tolist() + pairs_df1['id2'].tolist()))\n",
    "file_list2 = list(set(pairs_df2['id1'].tolist() + pairs_df2['id2'].tolist()))\n",
    "\n",
    "java_directories = [java_directory1, java_directory2]\n",
    "file_lists = [file_list1, file_list2]\n",
    "\n",
    "node_type_to_idx = build_global_vocab(java_directories, file_lists)\n",
    "embedding_layer = NodeEmbeddingLayer(len(node_type_to_idx), embedding_dim)\n",
    "\n",
    "data_pairs1 = prepare_data_for_pairs(pairs_df1, java_directory1, node_type_to_idx, embedding_layer)\n",
    "data_pairs2 = prepare_data_for_pairs(pairs_df2, java_directory2, node_type_to_idx, embedding_layer)\n",
    "\n",
    "all_pairs = data_pairs1 + data_pairs2\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "test_size = int(len(all_pairs) * 0.2)\n",
    "val_size = int(len(all_pairs) * 0.1)\n",
    "\n",
    "test_pairs = all_pairs[:test_size]\n",
    "val_pairs = all_pairs[test_size:test_size+val_size]\n",
    "train_pairs = all_pairs[test_size+val_size:]\n",
    "\n",
    "plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 1]\n",
    "non_plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 0]\n",
    "\n",
    "if len(plagiarism_pairs) > len(non_plagiarism_pairs):\n",
    "    plagiarism_pairs = random.sample(plagiarism_pairs, len(non_plagiarism_pairs))\n",
    "else:\n",
    "    non_plagiarism_pairs = random.sample(non_plagiarism_pairs, len(plagiarism_pairs))\n",
    "\n",
    "balanced_train_pairs = plagiarism_pairs + non_plagiarism_pairs\n",
    "random.shuffle(balanced_train_pairs)\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"Number of pairs in training dataset: {len(balanced_train_pairs)}\")\n",
    "print(f\"Number of pairs in validation set: {len(val_pairs)}\")\n",
    "print(f\"Number of pairs in test set: {len(test_pairs)}\")\n",
    "data1, data2, label1 = data_pairs1[0]\n",
    "data3, data4, label2 = data_pairs2[0]\n",
    "print(f\"Dataset 1 - First pair:\")\n",
    "print(f\"  Graph 1: {data1.num_nodes} nodes, {data1.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data2.num_nodes} nodes, {data2.num_edges} edges\")\n",
    "print(f\"  Label: {label1}\")\n",
    "print(f\"Dataset 2 - First pair:\")\n",
    "print(f\"  Graph 1: {data3.num_nodes} nodes, {data3.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data4.num_nodes} nodes, {data4.num_edges} edges\")\n",
    "print(f\"  Label: {label2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b01de",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdc33d",
   "metadata": {},
   "source": [
    "<h3>Build GNN siamese architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9c79f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNNEncoder(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super(GNNEncoder, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = global_mean_pool(x, batch)\n",
    "#         return x\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_channels, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        nn3 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        nn4 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        nn5 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = GNNEncoder(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        h1 = self.encoder(data1.x, data1.edge_index, data1.batch)\n",
    "        h2 = self.encoder(data2.x, data2.edge_index, data2.batch)\n",
    "        return h1, h2\n",
    "\n",
    "def contrastive_loss(h1, h2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(h1, h2)\n",
    "    loss = (label * torch.pow(distance, 2) + \n",
    "           (1 - label) * torch.pow(F.relu(margin - distance), 2))\n",
    "    return loss.mean()\n",
    "\n",
    "def collate_fn(pairs, device):\n",
    "    data1_list, data2_list, labels = [], [], []\n",
    "    for d1, d2, label in pairs:\n",
    "        data1_list.append(d1)\n",
    "        data2_list.append(d2)\n",
    "        labels.append(label)\n",
    "\n",
    "    batch1 = Batch.from_data_list(data1_list).to(device)\n",
    "    batch2 = Batch.from_data_list(data2_list).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float, device=device).to(device)\n",
    "\n",
    "    return batch1, batch2, labels\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Cuántas épocas esperar después de la última mejora.\n",
    "            min_delta (float): Mínima mejora en la métrica para ser considerada como mejora.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(f\"Early stopping triggered after {self.patience} epochs without improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a974c0",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f8dc2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, optimizer, data_pairs, dataset2, device, scheduler, epochs=10, batch_size=32, threshold=1.0):\n",
    "#     early_stopping = EarlyStopping(patience=30, min_delta=0.001)\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for i in range(0, len(data_pairs), batch_size):\n",
    "#             batch_pairs = data_pairs[i:i+batch_size]\n",
    "\n",
    "#             batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             h1, h2 = model(batch1, batch2)\n",
    "#             loss = contrastive_loss(h1, h2, labels)\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             distances = F.pairwise_distance(h1, h2)\n",
    "#             predictions = (distances < threshold).float()\n",
    "#             correct += (predictions == labels).sum().item()\n",
    "#             total += labels.size(0)\n",
    "\n",
    "#         train_loss = total_loss\n",
    "#         train_accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for i in range(0, len(dataset2), batch_size):\n",
    "#                 batch_pairs = dataset2[i:i+batch_size]\n",
    "\n",
    "#                 batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "#                 h1, h2 = model(batch1, batch2)\n",
    "#                 loss = contrastive_loss(h1, h2, labels)\n",
    "\n",
    "#                 val_loss += loss.item()\n",
    "\n",
    "#                 distances = F.pairwise_distance(h1, h2)\n",
    "#                 predictions = (distances < threshold).float()\n",
    "#                 val_correct += (predictions == labels).sum().item()\n",
    "#                 val_total += labels.size(0)\n",
    "\n",
    "#         scheduler.step(val_loss)\n",
    "\n",
    "#         val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}\")\n",
    "#         print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}%  Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             torch.save(model.state_dict(), './models/best_model.pt')\n",
    "#             break\n",
    "\n",
    "def train(model, optimizer, train_pairs, val_pairs, device, scheduler, threshold, epochs=50, batch_size=32):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_pairs)\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i in range(0, len(train_pairs), batch_size):\n",
    "            batch_pairs = train_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(train_pairs)\n",
    "        train_accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        val_loss, val_accuracy = evaluate(model, val_pairs, device, batch_size)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}% Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            \n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "def evaluate(model, data_pairs, device, threshold, batch_size=32):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_pairs), batch_size):\n",
    "            batch_pairs = data_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_pairs)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f03c5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1e399511",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "out_dim = 32\n",
    "threshold = 0.2626\n",
    "epochs = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45679b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Train Loss: 0.4351, Accuracy: 48.80% Val Loss: 0.5422, Accuracy: 43.61%\n",
      "Epoch 2/200 Train Loss: 0.3682, Accuracy: 40.48% Val Loss: 0.4497, Accuracy: 43.61%\n",
      "Epoch 3/200 Train Loss: 0.3365, Accuracy: 46.87% Val Loss: 0.4112, Accuracy: 43.61%\n",
      "Epoch 4/200 Train Loss: 0.3161, Accuracy: 49.76% Val Loss: 0.3768, Accuracy: 43.61%\n",
      "Epoch 5/200 Train Loss: 0.3195, Accuracy: 49.76% Val Loss: 0.3947, Accuracy: 43.61%\n",
      "Epoch 6/200 Train Loss: 0.3090, Accuracy: 50.12% Val Loss: 0.3611, Accuracy: 43.61%\n",
      "Epoch 7/200 Train Loss: 0.3149, Accuracy: 51.93% Val Loss: 0.3618, Accuracy: 43.61%\n",
      "Epoch 8/200 Train Loss: 0.2959, Accuracy: 55.90% Val Loss: 0.3571, Accuracy: 43.61%\n",
      "Epoch 9/200 Train Loss: 0.2884, Accuracy: 55.54% Val Loss: 0.3141, Accuracy: 43.61%\n",
      "Epoch 10/200 Train Loss: 0.2858, Accuracy: 55.90% Val Loss: 0.3350, Accuracy: 43.61%\n",
      "Epoch 11/200 Train Loss: 0.2771, Accuracy: 56.87% Val Loss: 0.2992, Accuracy: 43.61%\n",
      "Epoch 12/200 Train Loss: 0.2570, Accuracy: 59.88% Val Loss: 0.2789, Accuracy: 43.61%\n",
      "Epoch 13/200 Train Loss: 0.2411, Accuracy: 63.25% Val Loss: 0.2892, Accuracy: 43.61%\n",
      "Epoch 14/200 Train Loss: 0.2474, Accuracy: 64.46% Val Loss: 0.2658, Accuracy: 43.61%\n",
      "Epoch 15/200 Train Loss: 0.2393, Accuracy: 60.84% Val Loss: 0.2996, Accuracy: 43.61%\n",
      "Epoch 16/200 Train Loss: 0.2443, Accuracy: 63.01% Val Loss: 0.2787, Accuracy: 43.61%\n",
      "Epoch 17/200 Train Loss: 0.2332, Accuracy: 61.33% Val Loss: 0.2772, Accuracy: 43.61%\n",
      "Epoch 18/200 Train Loss: 0.2326, Accuracy: 63.49% Val Loss: 0.2719, Accuracy: 43.61%\n",
      "Epoch 19/200 Train Loss: 0.2263, Accuracy: 63.86% Val Loss: 0.2656, Accuracy: 43.61%\n",
      "Epoch 20/200 Train Loss: 0.2276, Accuracy: 63.98% Val Loss: 0.2478, Accuracy: 43.61%\n",
      "Epoch 21/200 Train Loss: 0.2202, Accuracy: 62.77% Val Loss: 0.2501, Accuracy: 43.61%\n",
      "Epoch 22/200 Train Loss: 0.2203, Accuracy: 64.70% Val Loss: 0.2515, Accuracy: 43.61%\n",
      "Epoch 23/200 Train Loss: 0.2193, Accuracy: 64.58% Val Loss: 0.2461, Accuracy: 43.61%\n",
      "Epoch 24/200 Train Loss: 0.2093, Accuracy: 64.82% Val Loss: 0.2356, Accuracy: 43.61%\n",
      "Epoch 25/200 Train Loss: 0.2187, Accuracy: 66.14% Val Loss: 0.2578, Accuracy: 43.61%\n",
      "Epoch 26/200 Train Loss: 0.2081, Accuracy: 70.72% Val Loss: 0.2436, Accuracy: 43.61%\n",
      "Epoch 27/200 Train Loss: 0.2176, Accuracy: 67.59% Val Loss: 0.2233, Accuracy: 43.61%\n",
      "Epoch 28/200 Train Loss: 0.2148, Accuracy: 69.40% Val Loss: 0.2442, Accuracy: 43.61%\n",
      "Epoch 29/200 Train Loss: 0.2010, Accuracy: 71.33% Val Loss: 0.2362, Accuracy: 43.61%\n",
      "Epoch 30/200 Train Loss: 0.1946, Accuracy: 69.04% Val Loss: 0.2254, Accuracy: 43.61%\n",
      "Epoch 31/200 Train Loss: 0.2094, Accuracy: 68.80% Val Loss: 0.2429, Accuracy: 43.61%\n",
      "Epoch 32/200 Train Loss: 0.2028, Accuracy: 72.05% Val Loss: 0.2334, Accuracy: 43.61%\n",
      "Epoch 33/200 Train Loss: 0.2015, Accuracy: 69.16% Val Loss: 0.2236, Accuracy: 43.61%\n",
      "Epoch 34/200 Train Loss: 0.2009, Accuracy: 68.55% Val Loss: 0.2253, Accuracy: 43.61%\n",
      "Epoch 35/200 Train Loss: 0.1988, Accuracy: 70.36% Val Loss: 0.2253, Accuracy: 43.61%\n",
      "Epoch 36/200 Train Loss: 0.1946, Accuracy: 69.16% Val Loss: 0.2258, Accuracy: 43.61%\n",
      "Epoch 37/200 Train Loss: 0.2050, Accuracy: 71.81% Val Loss: 0.2320, Accuracy: 43.61%\n",
      "Early stopping at epoch 37\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "train(model, optimizer, balanced_train_pairs, val_pairs, device, scheduler, threshold, epochs=epochs, batch_size=batch_size)\n",
    "torch.save(model.state_dict(), './models/siamese_gnn_model26.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4ca0a",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8f09c119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (encoder): GNNEncoder(\n",
       "    (conv1): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv2): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv3): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv4): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv5): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"models/siamese_gnn_model25.pth\"))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2067fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, dataset, device, batch_size=batch_size, threshold=threshold):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [int(round(x)) for x in all_preds]\n",
    "    all_labels = [int(round(x)) for x in all_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[1, 0])\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "\n",
    "    for (i, j), value in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f'{value}', ha='center', va='center', color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def find_best_threshold(model, dataset, device, batch_size=32, thresholds=np.linspace(0, 2, 100)):\n",
    "    model.eval()\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            all_preds_raw.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds_raw = np.array(all_preds_raw)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = (all_preds_raw < threshold).astype(int)\n",
    "        f1 = f1_score(all_labels, preds)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f0edce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.2222, Best F1 score: 0.7304\n"
     ]
    }
   ],
   "source": [
    "best_threshold, best_f1 = find_best_threshold(loaded_model, val_pairs, device, batch_size=batch_size, thresholds=np.linspace(0, 2, 100))\n",
    "print(f\"Best threshold: {best_threshold:.4f}, Best F1 score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6428e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos con plagio (1): 121\n",
      "Ejemplos sin plagio (0): 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_255972\\1376787608.py:30: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + labels)\n",
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_255972\\1376787608.py:31: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_plagio = sum(1 for _, _, label in test_pairs if label == 1)\n",
    "num_no_plagio = sum(1 for _, _, label in test_pairs if label == 0)\n",
    "\n",
    "print(f\"Ejemplos con plagio (1): {num_plagio}\")\n",
    "print(f\"Ejemplos sin plagio (0): {num_no_plagio}\")\n",
    "\n",
    "cm = compute_confusion_matrix(loaded_model, test_pairs, device, batch_size=batch_size, threshold=best_threshold)\n",
    "plot_confusion_matrix(cm, labels=['Plagiarized', 'Not Plagiarized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f54be710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2860, Test Accuracy: 47.57%\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = evaluate(loaded_model, test_pairs, device, best_threshold, batch_size=batch_size)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
