{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bb8de6",
   "metadata": {},
   "source": [
    "<h1>Use siamese GNN to predict the similarity of two source codes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac99c",
   "metadata": {},
   "source": [
    "<h3>Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4390b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java as ts_java\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F, torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d8a2e",
   "metadata": {},
   "source": [
    "<h2>Data preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b41d0",
   "metadata": {},
   "source": [
    "<h3>Define constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dad6d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_directory1 = './datasets/fire14-source-code-training-dataset/java'\n",
    "java_directory2 = './datasets/ir_plag_preprocessed'\n",
    "java_LANGUAGE = Language(ts_java.language())\n",
    "parser = Parser(java_LANGUAGE)\n",
    "csv_paths = ['./labels/fire14-labels.csv', './labels/ir_plag_labels.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b312cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93039af9",
   "metadata": {},
   "source": [
    "<h3>Get AST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3185acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node, parent_idx=None):\n",
    "        idx = len(nodes)\n",
    "        nodes.append(node.type)\n",
    "        \n",
    "        if parent_idx is not None:\n",
    "            edges.append((parent_idx, idx))\n",
    "        \n",
    "        for child in node.children:\n",
    "            traverse(child, idx)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f384178",
   "metadata": {},
   "source": [
    "<h3>Build data for GNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b169cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_vocab(java_directories, file_lists):\n",
    "    all_node_types = set()\n",
    "\n",
    "    for java_directory, file_list in zip(java_directories, file_lists):\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(java_directory, file_name)\n",
    "            nodes, _ = parse_java_file(file_path)\n",
    "            all_node_types.update(nodes)\n",
    "\n",
    "    node_type_to_idx = {typ: idx for idx, typ in enumerate(sorted(all_node_types))}\n",
    "    return node_type_to_idx\n",
    "\n",
    "def create_node_features(nodes, node_type_to_idx):\n",
    "    node_features = [node_type_to_idx[typ] for typ in nodes]\n",
    "    return node_features\n",
    "\n",
    "def create_graph_data(nodes, edges, node_features, embedding_layer):\n",
    "    x = embedding_layer(torch.tensor(node_features))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ee31811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_node_types, embedding_dim):\n",
    "        super(NodeEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_node_types, embedding_dim)\n",
    "\n",
    "    def forward(self, node_indices):\n",
    "        return self.embeddings(node_indices)\n",
    "    \n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7ae20970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pairs(pairs_df, java_directory, node_type_to_idx, embedding_layer):\n",
    "    data_pairs = []\n",
    "    for idx, row in pairs_df.iterrows():\n",
    "        file1, file2, label = row['id1'], row['id2'], row['plagio']\n",
    "\n",
    "        file1_path = os.path.join(java_directory, file1)\n",
    "        file2_path = os.path.join(java_directory, file2)\n",
    "\n",
    "        nodes1, edges1 = parse_java_file(file1_path)\n",
    "        nodes2, edges2 = parse_java_file(file2_path)\n",
    "\n",
    "        node_features1 = create_node_features(nodes1, node_type_to_idx)\n",
    "        node_features2 = create_node_features(nodes2, node_type_to_idx)\n",
    "\n",
    "        data1 = create_graph_data(nodes1, edges1, node_features1, embedding_layer)\n",
    "        data2 = create_graph_data(nodes2, edges2, node_features2, embedding_layer)\n",
    "\n",
    "        data_pairs.append((data1, data2, label))\n",
    "        \n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7ddf221",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df1 = load_csv(csv_paths[0])\n",
    "pairs_df2 = load_csv(csv_paths[1])\n",
    "\n",
    "file_list1 = list(set(pairs_df1['id1'].tolist() + pairs_df1['id2'].tolist()))\n",
    "file_list2 = list(set(pairs_df2['id1'].tolist() + pairs_df2['id2'].tolist()))\n",
    "\n",
    "java_directories = [java_directory1, java_directory2]\n",
    "file_lists = [file_list1, file_list2]\n",
    "\n",
    "node_type_to_idx = build_global_vocab(java_directories, file_lists)\n",
    "embedding_layer = NodeEmbeddingLayer(len(node_type_to_idx), embedding_dim)\n",
    "\n",
    "data_pairs1 = prepare_data_for_pairs(pairs_df1, java_directory1, node_type_to_idx, embedding_layer)\n",
    "data_pairs2 = prepare_data_for_pairs(pairs_df2, java_directory2, node_type_to_idx, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "efd9de80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "Number of pairs in dataset 1: 504\n",
      "Number of pairs in dataset 2: 460\n",
      "Dataset 1 - First pair:\n",
      "  Graph 1: 2068 nodes, 2067 edges\n",
      "  Graph 2: 619 nodes, 618 edges\n",
      "  Label: 0\n",
      "Dataset 2 - First pair:\n",
      "  Graph 1: 108 nodes, 107 edges\n",
      "  Graph 2: 109 nodes, 108 edges\n",
      "  Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preparation complete.\")\n",
    "print(f\"Number of pairs in dataset 1: {len(data_pairs1)}\")\n",
    "print(f\"Number of pairs in dataset 2: {len(data_pairs2)}\")\n",
    "\n",
    "data1, data2, label1 = data_pairs1[0]\n",
    "data3, data4, label2 = data_pairs2[0]\n",
    "\n",
    "print(f\"Dataset 1 - First pair:\")\n",
    "print(f\"  Graph 1: {data1.num_nodes} nodes, {data1.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data2.num_nodes} nodes, {data2.num_edges} edges\")\n",
    "print(f\"  Label: {label1}\")\n",
    "\n",
    "print(f\"Dataset 2 - First pair:\")\n",
    "print(f\"  Graph 1: {data3.num_nodes} nodes, {data3.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data4.num_nodes} nodes, {data4.num_edges} edges\")\n",
    "print(f\"  Label: {label2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b01de",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdc33d",
   "metadata": {},
   "source": [
    "<h3>Build GNN siamese architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c79f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = GNNEncoder(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        h1 = self.encoder(data1.x, data1.edge_index, data1.batch)\n",
    "        h2 = self.encoder(data2.x, data2.edge_index, data2.batch)\n",
    "        return h1, h2\n",
    "\n",
    "def contrastive_loss(h1, h2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(h1, h2)\n",
    "    loss = (label * torch.pow(distance, 2) + \n",
    "           (1 - label) * torch.pow(F.relu(margin - distance), 2))\n",
    "    return loss.mean()\n",
    "\n",
    "def collate_fn(pairs, device):\n",
    "    data1_list, data2_list, labels = [], [], []\n",
    "    for d1, d2, label in pairs:\n",
    "        data1_list.append(d1)\n",
    "        data2_list.append(d2)\n",
    "        labels.append(label)\n",
    "\n",
    "    batch1 = Batch.from_data_list(data1_list).to(device)\n",
    "    batch2 = Batch.from_data_list(data2_list).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float, device=device).to(device)\n",
    "\n",
    "    return batch1, batch2, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a974c0",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f8dc2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_pairs, dataset2, device, epochs=10, batch_size=32, threshold=1.0):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(0, len(data_pairs), batch_size):\n",
    "            batch_pairs = data_pairs[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss\n",
    "        train_accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(dataset2), batch_size):\n",
    "                batch_pairs = dataset2[i:i+batch_size]\n",
    "\n",
    "                batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "                h1, h2 = model(batch1, batch2)\n",
    "                loss = contrastive_loss(h1, h2, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                distances = F.pairwise_distance(h1, h2)\n",
    "                predictions = (distances < threshold).float()\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"  Train     Loss: {train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}%  Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f03c5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b45679b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Train     Loss: 4.0244, Accuracy: 77.83%  Validation Loss: 8.8993, Accuracy: 36.71%\n",
      "Epoch 2\n",
      "  Train     Loss: 3.8663, Accuracy: 78.04%  Validation Loss: 8.7891, Accuracy: 36.51%\n",
      "Epoch 3\n",
      "  Train     Loss: 3.7540, Accuracy: 78.26%  Validation Loss: 8.6432, Accuracy: 38.69%\n",
      "Epoch 4\n",
      "  Train     Loss: 3.6544, Accuracy: 78.26%  Validation Loss: 8.4798, Accuracy: 39.48%\n",
      "Epoch 5\n",
      "  Train     Loss: 3.5619, Accuracy: 78.26%  Validation Loss: 8.2902, Accuracy: 40.87%\n",
      "Epoch 6\n",
      "  Train     Loss: 3.4769, Accuracy: 78.26%  Validation Loss: 8.0761, Accuracy: 41.87%\n",
      "Epoch 7\n",
      "  Train     Loss: 3.3962, Accuracy: 78.48%  Validation Loss: 7.8552, Accuracy: 42.46%\n",
      "Epoch 8\n",
      "  Train     Loss: 3.3183, Accuracy: 78.26%  Validation Loss: 7.6272, Accuracy: 43.85%\n",
      "Epoch 9\n",
      "  Train     Loss: 3.2420, Accuracy: 79.13%  Validation Loss: 7.3909, Accuracy: 46.03%\n",
      "Epoch 10\n",
      "  Train     Loss: 3.1691, Accuracy: 79.35%  Validation Loss: 7.1509, Accuracy: 48.02%\n",
      "Epoch 11\n",
      "  Train     Loss: 3.1003, Accuracy: 79.78%  Validation Loss: 6.9105, Accuracy: 49.40%\n",
      "Epoch 12\n",
      "  Train     Loss: 3.0359, Accuracy: 79.78%  Validation Loss: 6.6757, Accuracy: 50.79%\n",
      "Epoch 13\n",
      "  Train     Loss: 2.9757, Accuracy: 80.87%  Validation Loss: 6.4452, Accuracy: 53.57%\n",
      "Epoch 14\n",
      "  Train     Loss: 2.9204, Accuracy: 80.43%  Validation Loss: 6.2252, Accuracy: 56.35%\n",
      "Epoch 15\n",
      "  Train     Loss: 2.8698, Accuracy: 80.43%  Validation Loss: 6.0181, Accuracy: 57.74%\n",
      "Epoch 16\n",
      "  Train     Loss: 2.8238, Accuracy: 81.30%  Validation Loss: 5.8291, Accuracy: 59.92%\n",
      "Epoch 17\n",
      "  Train     Loss: 2.7819, Accuracy: 81.52%  Validation Loss: 5.6592, Accuracy: 61.11%\n",
      "Epoch 18\n",
      "  Train     Loss: 2.7435, Accuracy: 82.17%  Validation Loss: 5.5052, Accuracy: 62.10%\n",
      "Epoch 19\n",
      "  Train     Loss: 2.7098, Accuracy: 82.39%  Validation Loss: 5.3679, Accuracy: 62.70%\n",
      "Epoch 20\n",
      "  Train     Loss: 2.6796, Accuracy: 82.83%  Validation Loss: 5.2482, Accuracy: 64.29%\n",
      "Epoch 21\n",
      "  Train     Loss: 2.6524, Accuracy: 83.70%  Validation Loss: 5.1422, Accuracy: 63.89%\n",
      "Epoch 22\n",
      "  Train     Loss: 2.6278, Accuracy: 83.70%  Validation Loss: 5.0457, Accuracy: 64.68%\n",
      "Epoch 23\n",
      "  Train     Loss: 2.6055, Accuracy: 83.70%  Validation Loss: 4.9623, Accuracy: 65.87%\n",
      "Epoch 24\n",
      "  Train     Loss: 2.5853, Accuracy: 83.26%  Validation Loss: 4.8895, Accuracy: 66.67%\n",
      "Epoch 25\n",
      "  Train     Loss: 2.5666, Accuracy: 83.48%  Validation Loss: 4.8224, Accuracy: 67.06%\n",
      "Epoch 26\n",
      "  Train     Loss: 2.5496, Accuracy: 83.70%  Validation Loss: 4.7632, Accuracy: 67.26%\n",
      "Epoch 27\n",
      "  Train     Loss: 2.5344, Accuracy: 84.13%  Validation Loss: 4.7108, Accuracy: 67.86%\n",
      "Epoch 28\n",
      "  Train     Loss: 2.5202, Accuracy: 84.35%  Validation Loss: 4.6665, Accuracy: 68.06%\n",
      "Epoch 29\n",
      "  Train     Loss: 2.5070, Accuracy: 84.78%  Validation Loss: 4.6263, Accuracy: 68.45%\n",
      "Epoch 30\n",
      "  Train     Loss: 2.4950, Accuracy: 85.00%  Validation Loss: 4.5917, Accuracy: 69.25%\n",
      "Epoch 31\n",
      "  Train     Loss: 2.4837, Accuracy: 85.43%  Validation Loss: 4.5602, Accuracy: 69.44%\n",
      "Epoch 32\n",
      "  Train     Loss: 2.4732, Accuracy: 85.43%  Validation Loss: 4.5330, Accuracy: 69.84%\n",
      "Epoch 33\n",
      "  Train     Loss: 2.4632, Accuracy: 85.43%  Validation Loss: 4.5090, Accuracy: 69.84%\n",
      "Epoch 34\n",
      "  Train     Loss: 2.4539, Accuracy: 85.22%  Validation Loss: 4.4875, Accuracy: 70.44%\n",
      "Epoch 35\n",
      "  Train     Loss: 2.4451, Accuracy: 85.22%  Validation Loss: 4.4680, Accuracy: 70.44%\n",
      "Epoch 36\n",
      "  Train     Loss: 2.4368, Accuracy: 85.43%  Validation Loss: 4.4503, Accuracy: 70.63%\n",
      "Epoch 37\n",
      "  Train     Loss: 2.4287, Accuracy: 85.43%  Validation Loss: 4.4348, Accuracy: 70.83%\n",
      "Epoch 38\n",
      "  Train     Loss: 2.4211, Accuracy: 85.43%  Validation Loss: 4.4206, Accuracy: 71.03%\n",
      "Epoch 39\n",
      "  Train     Loss: 2.4139, Accuracy: 85.43%  Validation Loss: 4.4066, Accuracy: 71.03%\n",
      "Epoch 40\n",
      "  Train     Loss: 2.4071, Accuracy: 85.43%  Validation Loss: 4.3944, Accuracy: 70.83%\n",
      "Epoch 41\n",
      "  Train     Loss: 2.4005, Accuracy: 85.43%  Validation Loss: 4.3836, Accuracy: 71.23%\n",
      "Epoch 42\n",
      "  Train     Loss: 2.3940, Accuracy: 85.22%  Validation Loss: 4.3733, Accuracy: 71.43%\n",
      "Epoch 43\n",
      "  Train     Loss: 2.3878, Accuracy: 85.22%  Validation Loss: 4.3630, Accuracy: 71.83%\n",
      "Epoch 44\n",
      "  Train     Loss: 2.3820, Accuracy: 85.87%  Validation Loss: 4.3557, Accuracy: 71.83%\n",
      "Epoch 45\n",
      "  Train     Loss: 2.3760, Accuracy: 86.30%  Validation Loss: 4.3471, Accuracy: 72.22%\n",
      "Epoch 46\n",
      "  Train     Loss: 2.3704, Accuracy: 86.74%  Validation Loss: 4.3383, Accuracy: 72.62%\n",
      "Epoch 47\n",
      "  Train     Loss: 2.3651, Accuracy: 86.74%  Validation Loss: 4.3315, Accuracy: 72.22%\n",
      "Epoch 48\n",
      "  Train     Loss: 2.3596, Accuracy: 86.74%  Validation Loss: 4.3251, Accuracy: 72.42%\n",
      "Epoch 49\n",
      "  Train     Loss: 2.3544, Accuracy: 86.96%  Validation Loss: 4.3189, Accuracy: 72.42%\n",
      "Epoch 50\n",
      "  Train     Loss: 2.3493, Accuracy: 86.96%  Validation Loss: 4.3137, Accuracy: 72.42%\n",
      "Epoch 51\n",
      "  Train     Loss: 2.3442, Accuracy: 86.96%  Validation Loss: 4.3076, Accuracy: 72.22%\n",
      "Epoch 52\n",
      "  Train     Loss: 2.3394, Accuracy: 87.17%  Validation Loss: 4.3019, Accuracy: 72.22%\n",
      "Epoch 53\n",
      "  Train     Loss: 2.3347, Accuracy: 87.17%  Validation Loss: 4.2969, Accuracy: 72.42%\n",
      "Epoch 54\n",
      "  Train     Loss: 2.3301, Accuracy: 87.17%  Validation Loss: 4.2920, Accuracy: 72.62%\n",
      "Epoch 55\n",
      "  Train     Loss: 2.3255, Accuracy: 87.17%  Validation Loss: 4.2874, Accuracy: 72.62%\n",
      "Epoch 56\n",
      "  Train     Loss: 2.3210, Accuracy: 87.17%  Validation Loss: 4.2823, Accuracy: 72.82%\n",
      "Epoch 57\n",
      "  Train     Loss: 2.3168, Accuracy: 87.39%  Validation Loss: 4.2772, Accuracy: 72.62%\n",
      "Epoch 58\n",
      "  Train     Loss: 2.3124, Accuracy: 87.39%  Validation Loss: 4.2731, Accuracy: 72.62%\n",
      "Epoch 59\n",
      "  Train     Loss: 2.3082, Accuracy: 87.39%  Validation Loss: 4.2696, Accuracy: 72.62%\n",
      "Epoch 60\n",
      "  Train     Loss: 2.3041, Accuracy: 87.39%  Validation Loss: 4.2658, Accuracy: 72.62%\n",
      "Epoch 61\n",
      "  Train     Loss: 2.2998, Accuracy: 87.39%  Validation Loss: 4.2619, Accuracy: 72.42%\n",
      "Epoch 62\n",
      "  Train     Loss: 2.2957, Accuracy: 87.39%  Validation Loss: 4.2567, Accuracy: 72.62%\n",
      "Epoch 63\n",
      "  Train     Loss: 2.2918, Accuracy: 87.39%  Validation Loss: 4.2518, Accuracy: 73.21%\n",
      "Epoch 64\n",
      "  Train     Loss: 2.2878, Accuracy: 87.39%  Validation Loss: 4.2467, Accuracy: 73.02%\n",
      "Epoch 65\n",
      "  Train     Loss: 2.2839, Accuracy: 87.61%  Validation Loss: 4.2424, Accuracy: 73.41%\n",
      "Epoch 66\n",
      "  Train     Loss: 2.2800, Accuracy: 87.83%  Validation Loss: 4.2371, Accuracy: 73.41%\n",
      "Epoch 67\n",
      "  Train     Loss: 2.2759, Accuracy: 87.83%  Validation Loss: 4.2313, Accuracy: 73.41%\n",
      "Epoch 68\n",
      "  Train     Loss: 2.2723, Accuracy: 87.83%  Validation Loss: 4.2267, Accuracy: 73.61%\n",
      "Epoch 69\n",
      "  Train     Loss: 2.2685, Accuracy: 87.83%  Validation Loss: 4.2217, Accuracy: 74.01%\n",
      "Epoch 70\n",
      "  Train     Loss: 2.2648, Accuracy: 87.83%  Validation Loss: 4.2168, Accuracy: 74.01%\n",
      "Epoch 71\n",
      "  Train     Loss: 2.2611, Accuracy: 88.26%  Validation Loss: 4.2127, Accuracy: 73.81%\n",
      "Epoch 72\n",
      "  Train     Loss: 2.2575, Accuracy: 88.48%  Validation Loss: 4.2079, Accuracy: 73.81%\n",
      "Epoch 73\n",
      "  Train     Loss: 2.2539, Accuracy: 88.48%  Validation Loss: 4.2031, Accuracy: 73.81%\n",
      "Epoch 74\n",
      "  Train     Loss: 2.2503, Accuracy: 88.48%  Validation Loss: 4.1985, Accuracy: 73.61%\n",
      "Epoch 75\n",
      "  Train     Loss: 2.2467, Accuracy: 88.70%  Validation Loss: 4.1935, Accuracy: 74.21%\n",
      "Epoch 76\n",
      "  Train     Loss: 2.2432, Accuracy: 88.70%  Validation Loss: 4.1892, Accuracy: 74.21%\n",
      "Epoch 77\n",
      "  Train     Loss: 2.2397, Accuracy: 88.70%  Validation Loss: 4.1849, Accuracy: 74.40%\n",
      "Epoch 78\n",
      "  Train     Loss: 2.2364, Accuracy: 88.70%  Validation Loss: 4.1801, Accuracy: 74.40%\n",
      "Epoch 79\n",
      "  Train     Loss: 2.2329, Accuracy: 88.70%  Validation Loss: 4.1755, Accuracy: 74.40%\n",
      "Epoch 80\n",
      "  Train     Loss: 2.2295, Accuracy: 88.70%  Validation Loss: 4.1708, Accuracy: 74.40%\n",
      "Epoch 81\n",
      "  Train     Loss: 2.2261, Accuracy: 88.70%  Validation Loss: 4.1664, Accuracy: 74.40%\n",
      "Epoch 82\n",
      "  Train     Loss: 2.2228, Accuracy: 88.70%  Validation Loss: 4.1618, Accuracy: 74.80%\n",
      "Epoch 83\n",
      "  Train     Loss: 2.2195, Accuracy: 88.70%  Validation Loss: 4.1576, Accuracy: 75.00%\n",
      "Epoch 84\n",
      "  Train     Loss: 2.2162, Accuracy: 88.70%  Validation Loss: 4.1528, Accuracy: 75.00%\n",
      "Epoch 85\n",
      "  Train     Loss: 2.2130, Accuracy: 88.70%  Validation Loss: 4.1483, Accuracy: 75.00%\n",
      "Epoch 86\n",
      "  Train     Loss: 2.2097, Accuracy: 88.70%  Validation Loss: 4.1446, Accuracy: 75.00%\n",
      "Epoch 87\n",
      "  Train     Loss: 2.2064, Accuracy: 88.70%  Validation Loss: 4.1396, Accuracy: 75.20%\n",
      "Epoch 88\n",
      "  Train     Loss: 2.2032, Accuracy: 88.70%  Validation Loss: 4.1352, Accuracy: 75.20%\n",
      "Epoch 89\n",
      "  Train     Loss: 2.2000, Accuracy: 88.70%  Validation Loss: 4.1310, Accuracy: 75.40%\n",
      "Epoch 90\n",
      "  Train     Loss: 2.1967, Accuracy: 88.91%  Validation Loss: 4.1263, Accuracy: 75.40%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "out_dim = 32\n",
    "\n",
    "model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, optimizer, data_pairs2, data_pairs1, device, epochs=90, batch_size=20, threshold=0.5)\n",
    "\n",
    "torch.save(model.state_dict(), './models/siamese_gnn_model5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4ca0a",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f09c119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (encoder): GNNEncoder(\n",
       "    (conv1): GCNConv(16, 32)\n",
       "    (conv2): GCNConv(32, 32)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(\"models/siamese_gnn_model5.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2067fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, dataset, device, batch_size=20, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [int(round(x)) for x in all_preds]\n",
    "    all_labels = [int(round(x)) for x in all_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6428e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 67  38]\n",
      " [ 13 342]]\n"
     ]
    }
   ],
   "source": [
    "cm = compute_confusion_matrix(model, data_pairs2, device)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
