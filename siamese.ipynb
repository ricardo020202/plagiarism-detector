{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bb8de6",
   "metadata": {},
   "source": [
    "<h1>Use siamese GNN to predict the similarity of two source codes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac99c",
   "metadata": {},
   "source": [
    "<h3>Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4390b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_java as ts_java\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, GINConv\n",
    "import torch.nn.functional as F, torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d8a2e",
   "metadata": {},
   "source": [
    "<h2>Data preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b41d0",
   "metadata": {},
   "source": [
    "<h3>Define constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dad6d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_directory1 = './datasets/conplag_preprocessed'\n",
    "java_directory2 = './datasets/ir_plag_preprocessed'\n",
    "java_LANGUAGE = Language(ts_java.language())\n",
    "parser = Parser(java_LANGUAGE)\n",
    "csv_paths = ['./labels/conplag-labels.csv', './labels/ir_plag_labels.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6b312cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93039af9",
   "metadata": {},
   "source": [
    "<h3>Get AST</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3185acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def traverse(node, parent_idx=None):\n",
    "        idx = len(nodes)\n",
    "        nodes.append(node.type)\n",
    "        \n",
    "        if parent_idx is not None:\n",
    "            edges.append((parent_idx, idx))\n",
    "        \n",
    "        for child in node.children:\n",
    "            traverse(child, idx)\n",
    "\n",
    "    traverse(root_node)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f384178",
   "metadata": {},
   "source": [
    "<h3>Build data for GNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3b169cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_vocab(java_directories, file_lists):\n",
    "    all_node_types = set()\n",
    "\n",
    "    for java_directory, file_list in zip(java_directories, file_lists):\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(java_directory, file_name)\n",
    "            nodes, _ = parse_java_file(file_path)\n",
    "            all_node_types.update(nodes)\n",
    "\n",
    "    node_type_to_idx = {typ: idx for idx, typ in enumerate(sorted(all_node_types))}\n",
    "    return node_type_to_idx\n",
    "\n",
    "def create_node_features(nodes, node_type_to_idx):\n",
    "    node_features = [node_type_to_idx[typ] for typ in nodes]\n",
    "    return node_features\n",
    "\n",
    "def create_graph_data(nodes, edges, node_features, embedding_layer):\n",
    "    x = embedding_layer(torch.tensor(node_features))\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9ee31811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_node_types, embedding_dim):\n",
    "        super(NodeEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_node_types, embedding_dim)\n",
    "\n",
    "    def forward(self, node_indices):\n",
    "        return self.embeddings(node_indices)\n",
    "    \n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7ae20970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_pairs(pairs_df, java_directory, node_type_to_idx, embedding_layer):\n",
    "    data_pairs = []\n",
    "    for idx, row in pairs_df.iterrows():\n",
    "        file1, file2, label = row['id1'], row['id2'], row['plagio']\n",
    "\n",
    "        file1_path = os.path.join(java_directory, file1)\n",
    "        file2_path = os.path.join(java_directory, file2)\n",
    "\n",
    "        nodes1, edges1 = parse_java_file(file1_path)\n",
    "        nodes2, edges2 = parse_java_file(file2_path)\n",
    "\n",
    "        node_features1 = create_node_features(nodes1, node_type_to_idx)\n",
    "        node_features2 = create_node_features(nodes2, node_type_to_idx)\n",
    "\n",
    "        data1 = create_graph_data(nodes1, edges1, node_features1, embedding_layer)\n",
    "        data2 = create_graph_data(nodes2, edges2, node_features2, embedding_layer)\n",
    "\n",
    "        data_pairs.append((data1, data2, label))\n",
    "        \n",
    "    return data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7ddf221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "Number of pairs in training dataset: 836\n",
      "Number of pairs in validation set: 133\n",
      "Number of pairs in test set: 267\n",
      "Dataset 1 - First pair:\n",
      "  Graph 1: 919 nodes, 918 edges\n",
      "  Graph 2: 1246 nodes, 1245 edges\n",
      "  Label: 0\n",
      "Dataset 2 - First pair:\n",
      "  Graph 1: 108 nodes, 107 edges\n",
      "  Graph 2: 109 nodes, 108 edges\n",
      "  Label: 1\n"
     ]
    }
   ],
   "source": [
    "pairs_df1 = load_csv(csv_paths[0])\n",
    "pairs_df2 = load_csv(csv_paths[1])\n",
    "\n",
    "file_list1 = list(set(pairs_df1['id1'].tolist() + pairs_df1['id2'].tolist()))\n",
    "file_list2 = list(set(pairs_df2['id1'].tolist() + pairs_df2['id2'].tolist()))\n",
    "\n",
    "java_directories = [java_directory1, java_directory2]\n",
    "file_lists = [file_list1, file_list2]\n",
    "\n",
    "node_type_to_idx = build_global_vocab(java_directories, file_lists)\n",
    "embedding_layer = NodeEmbeddingLayer(len(node_type_to_idx), embedding_dim)\n",
    "\n",
    "data_pairs1 = prepare_data_for_pairs(pairs_df1, java_directory1, node_type_to_idx, embedding_layer)\n",
    "data_pairs2 = prepare_data_for_pairs(pairs_df2, java_directory2, node_type_to_idx, embedding_layer)\n",
    "\n",
    "all_pairs = data_pairs1 + data_pairs2\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "test_size = int(len(all_pairs) * 0.2)\n",
    "val_size = int(len(all_pairs) * 0.1)\n",
    "\n",
    "test_pairs = all_pairs[:test_size]\n",
    "val_pairs = all_pairs[test_size:test_size+val_size]\n",
    "train_pairs = all_pairs[test_size+val_size:]\n",
    "\n",
    "plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 1]\n",
    "non_plagiarism_pairs = [pair for pair in train_pairs if pair[2] == 0]\n",
    "\n",
    "if len(plagiarism_pairs) > len(non_plagiarism_pairs):\n",
    "    plagiarism_pairs = random.sample(plagiarism_pairs, len(non_plagiarism_pairs))\n",
    "else:\n",
    "    non_plagiarism_pairs = random.sample(non_plagiarism_pairs, len(plagiarism_pairs))\n",
    "\n",
    "balanced_train_pairs = plagiarism_pairs + non_plagiarism_pairs\n",
    "random.shuffle(balanced_train_pairs)\n",
    "\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"Number of pairs in training dataset: {len(balanced_train_pairs)}\")\n",
    "print(f\"Number of pairs in validation set: {len(val_pairs)}\")\n",
    "print(f\"Number of pairs in test set: {len(test_pairs)}\")\n",
    "data1, data2, label1 = data_pairs1[0]\n",
    "data3, data4, label2 = data_pairs2[0]\n",
    "print(f\"Dataset 1 - First pair:\")\n",
    "print(f\"  Graph 1: {data1.num_nodes} nodes, {data1.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data2.num_nodes} nodes, {data2.num_edges} edges\")\n",
    "print(f\"  Label: {label1}\")\n",
    "print(f\"Dataset 2 - First pair:\")\n",
    "print(f\"  Graph 1: {data3.num_nodes} nodes, {data3.num_edges} edges\")\n",
    "print(f\"  Graph 2: {data4.num_nodes} nodes, {data4.num_edges} edges\")\n",
    "print(f\"  Label: {label2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b01de",
   "metadata": {},
   "source": [
    "<h2>Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdc33d",
   "metadata": {},
   "source": [
    "<h3>Build GNN siamese architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9c79f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNNEncoder(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super(GNNEncoder, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = global_mean_pool(x, batch)\n",
    "#         return x\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(in_channels, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        nn3 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, out_dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        nn4 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        nn5 = nn.Sequential(nn.Linear(out_dim, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = GNNEncoder(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        h1 = self.encoder(data1.x, data1.edge_index, data1.batch)\n",
    "        h2 = self.encoder(data2.x, data2.edge_index, data2.batch)\n",
    "        return h1, h2\n",
    "\n",
    "def contrastive_loss(h1, h2, label, margin=1.0):\n",
    "    distance = F.pairwise_distance(h1, h2)\n",
    "    loss = (label * torch.pow(distance, 2) + \n",
    "           (1 - label) * torch.pow(F.relu(margin - distance), 2))\n",
    "    return loss.mean()\n",
    "\n",
    "def collate_fn(pairs, device):\n",
    "    data1_list, data2_list, labels = [], [], []\n",
    "    for d1, d2, label in pairs:\n",
    "        data1_list.append(d1)\n",
    "        data2_list.append(d2)\n",
    "        labels.append(label)\n",
    "\n",
    "    batch1 = Batch.from_data_list(data1_list).to(device)\n",
    "    batch2 = Batch.from_data_list(data2_list).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.float, device=device).to(device)\n",
    "\n",
    "    return batch1, batch2, labels\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Cuántas épocas esperar después de la última mejora.\n",
    "            min_delta (float): Mínima mejora en la métrica para ser considerada como mejora.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(f\"Early stopping triggered after {self.patience} epochs without improvement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a974c0",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f8dc2f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_pairs, dataset2, device, scheduler, epochs=10, batch_size=32, threshold=1.0):\n",
    "    early_stopping = EarlyStopping(patience=30, min_delta=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(0, len(data_pairs), batch_size):\n",
    "            batch_pairs = data_pairs[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss\n",
    "        train_accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(dataset2), batch_size):\n",
    "                batch_pairs = dataset2[i:i+batch_size]\n",
    "\n",
    "                batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "                h1, h2 = model(batch1, batch2)\n",
    "                loss = contrastive_loss(h1, h2, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                distances = F.pairwise_distance(h1, h2)\n",
    "                predictions = (distances < threshold).float()\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy*100:.2f}%  Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            torch.save(model.state_dict(), './models/best_model.pt')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f03c5028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1e399511",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "out_dim = 32\n",
    "threshold = 0.45\n",
    "epochs = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b45679b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 3.2479, Accuracy: 50.00%  Validation Loss: 1.6794, Accuracy: 44.19%\n",
      "Epoch 2\n",
      "Train Loss: 2.3924, Accuracy: 53.11%  Validation Loss: 1.0108, Accuracy: 53.93%\n",
      "Epoch 3\n",
      "Train Loss: 2.0298, Accuracy: 55.26%  Validation Loss: 0.7616, Accuracy: 60.30%\n",
      "Epoch 4\n",
      "Train Loss: 1.9417, Accuracy: 59.33%  Validation Loss: 0.8251, Accuracy: 58.80%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[168]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-3\u001b[39m, weight_decay=\u001b[32m1e-5\u001b[39m)\n\u001b[32m      8\u001b[39m scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalanced_train_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m torch.save(model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m./models/siamese_gnn_model21.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, data_pairs, dataset2, device, scheduler, epochs, batch_size, threshold)\u001b[39m\n\u001b[32m     37\u001b[39m batch_pairs = dataset2[i:i+batch_size]\n\u001b[32m     39\u001b[39m batch1, batch2, labels = collate_fn(batch_pairs, device)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m h1, h2 = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m loss = contrastive_loss(h1, h2, labels)\n\u001b[32m     44\u001b[39m val_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[164]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mSiameseNetwork.forward\u001b[39m\u001b[34m(self, data1, data2)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data1, data2):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     h1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     h2 = \u001b[38;5;28mself\u001b[39m.encoder(data2.x, data2.edge_index, data2.batch)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m h1, h2\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[164]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mGNNEncoder.forward\u001b[39m\u001b[34m(self, x, edge_index, batch)\u001b[39m\n\u001b[32m     54\u001b[39m x = F.relu(x)\n\u001b[32m     55\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m x = F.relu(x)\n\u001b[32m     59\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py:84\u001b[39m, in \u001b[36mGINConv.forward\u001b[39m\u001b[34m(self, x, edge_index, size)\u001b[39m\n\u001b[32m     81\u001b[39m     x = (x, x)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m x_r = x[\u001b[32m1\u001b[39m]\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gin_conv_GINConv_propagate_ag_jaeix.py:173\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(self, edge_index, x, size)\u001b[39m\n\u001b[32m    167\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.update(\n\u001b[32m    168\u001b[39m         out,\n\u001b[32m    169\u001b[39m     )\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gin_conv_GINConv_propagate_ag_jaeix.py:83\u001b[39m, in \u001b[36mcollect\u001b[39m\u001b[34m(self, edge_index, x, size)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_x_0, Tensor):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_size(size, \u001b[32m0\u001b[39m, _x_0)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     x_j = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_index_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_x_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_j\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m     x_j = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:267\u001b[39m, in \u001b[36mMessagePassing._index_select\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m src.index_select(\u001b[38;5;28mself\u001b[39m.node_dim, index)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_index_select_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricor\\OneDrive\\Desktop\\plagiarism-detector\\.venv\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:271\u001b[39m, in \u001b[36mMessagePassing._index_select_safe\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_index_select_safe\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: Tensor, index: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m index.numel() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index.min() < \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "train(model, optimizer, balanced_train_pairs, test_pairs, device, scheduler, epochs=epochs, batch_size=batch_size, threshold=threshold)\n",
    "torch.save(model.state_dict(), './models/siamese_gnn_model21.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4ca0a",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8f09c119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (encoder): GNNEncoder(\n",
       "    (conv1): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv2): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv3): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv4): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (conv5): GINConv(nn=Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    ))\n",
       "    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = SiameseNetwork(\n",
    "    in_channels=embedding_dim,\n",
    "    hidden_channels=hidden_dim,\n",
    "    out_channels=out_dim\n",
    ").to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"models/siamese_gnn_model20.pth\"))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2067fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, dataset, device, batch_size=batch_size, threshold=threshold):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = [int(round(x)) for x in all_preds]\n",
    "    all_labels = [int(round(x)) for x in all_labels]\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[1, 0])\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "\n",
    "    for (i, j), value in np.ndenumerate(cm):\n",
    "        ax.text(j, i, f'{value}', ha='center', va='center', color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def find_best_threshold(model, dataset, device, batch_size=32, thresholds=np.linspace(0, 2, 100)):\n",
    "    model.eval()\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            batch_pairs = dataset[i:i+batch_size]\n",
    "\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "\n",
    "            h1, h2 = model(batch1, batch2)\n",
    "\n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            all_preds_raw.extend(distances.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds_raw = np.array(all_preds_raw)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    best_threshold = None\n",
    "    best_f1 = -1\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = (all_preds_raw < threshold).astype(int)\n",
    "        f1 = f1_score(all_labels, preds)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "def evaluate(model, data_pairs, device, threshold, batch_size=32):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_pairs), batch_size):\n",
    "            batch_pairs = data_pairs[i:i+batch_size]\n",
    "            batch1, batch2, labels = collate_fn(batch_pairs, device)\n",
    "            \n",
    "            h1, h2 = model(batch1, batch2)\n",
    "            loss = contrastive_loss(h1, h2, labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_pairs)\n",
    "            \n",
    "            distances = F.pairwise_distance(h1, h2)\n",
    "            predictions = (distances < threshold).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_pairs)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6428e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos con plagio (1): 118\n",
      "Ejemplos sin plagio (0): 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_237864\\151096293.py:30: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + labels)\n",
      "C:\\Users\\ricor\\AppData\\Local\\Temp\\ipykernel_237864\\151096293.py:31: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAroAAAIoCAYAAACGU0hRAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAATJNJREFUeJzt3QucTPX7wPHnjF2767bY2KXW/S4iii1RUZJEdFEqhXRVuVPu1+6khCSlKCS6k1RKrrn9Jcndulcu67a72Pm/nm/N/HbWLjvM7M7M+bx7ndfunHPme74zOTvPPOf5fo/ldDqdAgAAAIQYR253AAAAAPAHAl0AAACEJAJdAAAAhCQCXQAAAIQkAl0AAACEJAJdAAAAhCQCXQAAAIQkAl0AAACEJAJdAAAAhCQCXQDwkU2bNsnNN98s0dHRYlmWzJkzx6ftb9++3bT73nvv+bTdYHb99debBQAyQ6ALIKRs2bJFHn30USlXrpxERkZKoUKF5Nprr5XXX39dTp486ddjt2/fXtatWyfDhw+XDz74QOrWrSuh4qGHHjJBtr6fmb2PGuTrdl1eeeUVr9vfs2ePDBo0SNasWeOjHgOASFhudwAAfOWrr76Su+66SyIiIuTBBx+Uyy+/XFJTU2XRokXSs2dPWb9+vbz99tt+ObYGf0uWLJHnn39ennrqKb8co3Tp0uY44eHhkhvCwsLkxIkT8sUXX8jdd9/tsW3q1Knmi0VycvIFta2B7uDBg6VMmTJSq1atbD/v22+/vaDjAbAHAl0AIWHbtm3Stm1bEwx+//33UqJECfe2J598UjZv3mwCYX/566+/zM/ChQv77RiaLdVgMrfoFwjNjn/00UdnBbrTpk2T5s2by6xZs3KkLxpw58uXT/LmzZsjxwMQnChdABASXnrpJTl27JhMmjTJI8h1qVChgjzzzDPux6dPn5ahQ4dK+fLlTQCnmcTnnntOUlJSPJ6n62+77TaTFb766qtNoKllEVOmTHHvo5fcNcBWmjnWgFSf57rk7/o9PX2O7pfe/PnzpUGDBiZYLlCggFSuXNn06Xw1uhrYX3fddZI/f37z3JYtW8qGDRsyPZ4G/Non3U9riR9++GETNGbXfffdJ998840cPnzYvW7FihWmdEG3ZXTw4EHp0aOH1KhRw7wmLX1o1qyZrF271r3Pjz/+KFdddZX5XfvjKoFwvU6twdXs/MqVK6Vhw4YmwHW9LxlrdLV8RP8fZXz9TZs2lSJFipjMMQD7INAFEBL0croGoNdcc0229u/UqZMMGDBArrzyShk1apQ0atRIRo4cabLCGWlweOedd8pNN90kr776qgmYNFjUUgjVunVr04a69957TX3u6NGjveq/tqUBtQbaQ4YMMce5/fbb5Zdffjnn87777jsTxB04cMAEs926dZPFixebzKsGxhlpJvbo0aPmtervGkxqyUB26WvVIPTTTz/1yOZWqVLFvJcZbd261QzK09f22muvmS8CWses77cr6Kxatap5zapz587m/dNFg1qXf/75xwTIWtag7+0NN9yQaf+0FrtYsWIm4D1z5oxZN2HCBFPi8MYbb0jJkiWz/VoBhAAnAAS5I0eOOPXPWcuWLbO1/5o1a8z+nTp18ljfo0cPs/777793rytdurRZ99NPP7nXHThwwBkREeHs3r27e922bdvMfi+//LJHm+3btzdtZDRw4ECzv8uoUaPM47/++ivLfruOMXnyZPe6WrVqOYsXL+78559/3OvWrl3rdDgczgcffPCs43Xo0MGjzTvuuMMZExOT5THTv478+fOb3++8805n48aNze9nzpxxxsXFOQcPHpzpe5CcnGz2yfg69P0bMmSIe92KFSvOem0ujRo1MtvGjx+f6TZd0ps3b57Zf9iwYc6tW7c6CxQo4GzVqtV5XyOA0ENGF0DQS0pKMj8LFiyYrf2//vpr81Ozn+l1797d/MxYy1utWjVTGuCiGUMtK9Bspa+4ans/++wzSUtLy9Zz9u7da2Yp0Oxy0aJF3etr1qxpss+u15neY4895vFYX5dmS13vYXZoiYKWG+zbt8+UTejPzMoWlJaFOBz/ftRohlWP5SrLWLVqVbaPqe1oWUN26BRvOvOGZok1A62lDJrVBWA/BLoAgp7WfSq9JJ8dO3bsMMGX1u2mFxcXZwJO3Z5eqVKlzmpDyxcOHTokvnLPPfeYcgMtqYiNjTUlFDNmzDhn0OvqpwaNGWk5wN9//y3Hjx8/52vR16G8eS233nqr+VIxffp0M9uC1tdmfC9dtP9a1lGxYkUTrF5yySXmi8L//d//yZEjR7J9zEsvvdSrgWc6xZkG//pFYMyYMVK8ePFsPxdA6CDQBRASga7WXv72229ePS/jYLCs5MmTJ9P1Tqfzgo/hqh91iYqKkp9++snU3D7wwAMmENTgVzOzGfe9GBfzWlw0YNVM6fvvvy+zZ8/OMpurRowYYTLnWm/74Ycfyrx588ygu+rVq2c7c+16f7yxevVqU7estCYYgD0R6AIICTrYSW8WoXPZno/OkKBBls4UkN7+/fvNbAKuGRR8QTOm6WcocMmYNVaaZW7cuLEZtPX777+bG09oacAPP/yQ5etQGzduPGvbH3/8YbKnOhODP2hwq8GkZtEzG8Dn8sknn5iBYzobhu6nZQVNmjQ56z3J7peO7NAstpY5aMmJDm7TGTl0ZggA9kOgCyAk9OrVywR1eulfA9aMNAjWEfmuS+8q48wIGmAqnQ/WV3T6Mr1Erxna9LW1mgnNOA1XRq4bJ2Sc8sxFp1HTfTSzmj5w1My2zjLgep3+oMGrTs/25ptvmpKPc2WQM2aLZ86cKbt37/ZY5wrIM/tS4K3evXvLzp07zfui/091ejedhSGr9xFA6OKGEQBCggaUOs2VXu7X+tT0d0bT6bY0uNJBW+qKK64wgY/eJU0DK53qavny5SYwatWqVZZTV10IzWJq4HXHHXfI008/beasHTdunFSqVMljMJYOnNLSBQ2yNVOrl93feustueyyy8zcull5+eWXzbRbCQkJ0rFjR3PnNJ1GS+fI1enG/EWzz/369ctWpl1fm2ZYdeo3LSPQul6dCi7j/z+tjx4/fryp/9XAt169elK2bFmv+qUZcH3fBg4c6J7ubPLkyWau3f79+5vsLgD7IKMLIGTovLOaOdU5b3X2Ar0jWp8+fcx8sjovrQ5KcnnnnXfM/LF6SfvZZ581AVLfvn3l448/9mmfYmJiTPZWb3KgWWcNpnUO2xYtWpzVdx0o9u6775p+jx071tS1ar80aM2KlgHMnTvXHEfnBdZBWPXr1zfz73obJPqD3thBZ7PQ2ly9YYcG9zqrRXx8vMd+eltjfW80A6wzQ+h8xAsXLvTqWFpG0aFDB6ldu7a5FXP6mSX02PpvYOnSpT57bQACn6VzjOV2JwAAAABfI6MLAACAkESgCwAAgJBEoAsAAICQRKALAACAkESgCwAAgJBEoAsAAICQRKALAACAkESgCwSg9957z9wlytcsy5I5c+ZcdDt6lym9yYI/+aqvQKDQO/Ppnfd86ccffzTnii9unezvc86XfQWyi0AXyMUPPf2jr0vevHmlQoUK5lapp0+f9tsx9+7da24Xe7E+/fRTGTp0qE/6BPjz/HrhhRc81msgp+u9UaZMGRk9enS29nOd03oLY70Fsd562p/0tsp6Xp/r7nk5/fcBCCQEukAuuuWWW8yHy6ZNm8xtUgcNGiQvv/yy344XFxcnERERF/z81NRU87No0aJSsGBBH/YM8L3IyEh58cUX5dChQzl2TP2yquf06tWr5aqrrpJ77rlHFi9e7Lfj6ZdkPa+9Dd4zO68v9u8DEIgIdIFcpB8q+uFSunRpefzxx6VJkyby+eefn7Xfli1bpGXLlhIbGysFChQwH6Dfffedxz764dq8eXOJioqSsmXLyrRp087KRGW8NNm7d2+pVKmS5MuXT8qVKyf9+/eXU6dOubdr4F2rVi155513TJsaOGQsXXBdjsy4aEbN5bPPPjPZLX2+Hmfw4MEemWsN9Bs2bGi2V6tWTebPn++z9xj2peeTnl8jR448536zZs2S6tWrm/NRz5lXX33VvU3/re/YsUO6du3q/rd9LvoFUI+p59XYsWPN+fjFF19kuu/cuXOlQYMGpkwpJiZGbrvtNnOup6dBsp6Dem7UrVvXnZFes2ZNpuUA//zzj9x7771y6aWXmvO6Ro0a8tFHH3m0qa/pqaeeMufwJZdcIk2bNj3r74Oe+5md11pWpdLS0sz7qn8X9DVeccUV8sknn3gc5+uvvzbvg26/4YYbZPv27ed87wB/INAFAoh+ILiyK+kdO3ZMbr31VlmwYIHJFGkmuEWLFrJz5073Pg8++KDs2bPHfPDpB/fbb78tBw4cOO+Hsn5w/f777/L666/LxIkTZdSoUR77bN682bSn5QquD9fMLp26lu+//958KGvgqn7++WfTt2eeecYcZ8KECeaYw4cPd39gtm7d2mSmli1bJuPHjzcBOHCx8uTJIyNGjJA33nhDdu3alek+K1eulLvvvlvatm0r69atMwGefuFzBXT67/6yyy5zZ2p1ya6wsDAJDw/P9JxWx48fl27dusmvv/5qzm2HwyF33HGHOSdUUlKSOc81WF21apUpFzrfuZGcnCx16tSRr776Sn777Tfp3LmzPPDAA7J8+XKP/d5//31zzv3yyy/mnMuoR48eHuf1K6+8YgJnDbaVBrlTpkwxz12/fr35InD//ffLwoULzfbExERzXmv/9e9Gp06dpE+fPtl+7wCfcQLIFe3bt3e2bNnS/J6WluacP3++MyIiwtmjRw/n5MmTndHR0ed8fvXq1Z1vvPGG+X3Dhg1OPZ1XrFjh3r5p0yazbtSoUe51+nj27NlZtvnyyy8769Sp4348cOBAZ3h4uPPAgQMe+zVq1Mj5zDPPnPX8v//+21muXDnnE0884V7XuHFj54gRIzz2++CDD5wlSpQwv8+bN88ZFhbm3L17t3v7N998c96+Atk9v+rXr+/s0KGD+V3/TaX/6LvvvvucN910k8dze/bs6axWrZr7cenSpT3Oo6yk3y8lJcX8u9djffnll2f1KTN//fWX2X/dunXm8bhx45wxMTHOkydPuveZOHGi2Wf16tXm8Q8//GAeHzp0KMt2mzdv7uzevbvH+Vu7du2z9svqnFuyZIkzMjLSOX36dPM4OTnZmS9fPufixYs99uvYsaPz3nvvNb/37dvX4z1UvXv3Pm9fAV8L813IDMBbX375pSlF0HIBzeLcd999JqOUcQCLZnR1vWZpNLuil/1Pnjzpzuhu3LjRZI+0PMBFB7cVKVLknMefPn26jBkzxlwu1WNou4UKFfLYR8sqihUrdt7Xoq+hTZs2Zn/NDrusXbvWZI1cGVx15swZk3k6ceKEbNiwQeLj46VkyZLu7QkJCec9HpBdWqd74403mixlRvrvT8uC0rv22mtNyY/+O9WssDc049qvXz/z71vPbR0MpyVFmdGSnQEDBpgrGX///bc7k6vn9eWXX27O65o1a7pLhtTVV199zuNrnzWLPWPGDNm9e7fJJqekpJhsbHqa9c0O7YvOFKHvnWa+XVd59Ny96aabPPbVY9WuXdv9vtarV89jO+c1cgOBLpCLtG5t3Lhx5hKiBnoarGZGP2S0blUvH2oAqyUOd955Z5aXRLNjyZIl0q5dO1MvqzV6Omr7448/9qhPVDp6PDu0xlgvV+ol0vSvQwNoPYZexswo/Qc44C9aRqP/xvv27etRO+4PPXv2NMfQIFdr6s9V06uX9fWLoZYM6fmvga4GuBdzXutgVv2iqYG6ljzo+au1uBnbzM55raUVt99+uwlQtXQj/Tmt9Iu31gKnx2A2BBoCXSAX6YeNBq7noxlR/fDU+j3XB036gR2VK1c22Vit33VlajTrcq7R5jrIRT9kn3/+efc6HXRzIV577TWTQdI2dVBNeppl1sxUVq+zatWqJkDWTHWJEiXMuqVLl15QP4CsaGZVB3XpuZLx35+eX+npYx1E5crm6hdRzZRmhw7uys45rYPG9LzQIPe6664z6xYtWuSxj/b1ww8/NBlZVwC5YsWKc7arfdcMtdbLKg2e//zzTzPI0xtayaBt6PM/+OADj4Bd29L+aLa3UaNGmT5f39eMA2s5r5EbGIwGBIGKFSu6B4NpKYCWOLguc6oqVaqYEeY68EQzqhrw6u+a+c0qo6Rt6geVZnG1dEFLGGbPnu1133T2h169eplMkn7I79u3zyxHjhwx2/XSrA5a0ayuDlrRS5p6TL28q7TfGlS0b9/evDYdvJY++AZ8QbObegVD/52np9P66UAwHeilAaEO0nrzzTc9yhx0JoaffvrJlAJoiYEvaFmRfinUQaP6pVQHcerAtPRc57mey3rezJs3z1zVUec6r/Xqj37p1Oc8+uijsn//fq/7p6VSem7r4FH9Yu06r7VkSgex6vujA9D0/dK/HzpYTgf96WP12GOPmdIMzXBrQK+zwLgG+AE5iUAXCAKaMdUPRp3hQC936mXY9PW4SoNJvVSql2k18/vII4+YD6SsygP0kqR+UOk0Q5rp0g9GHW3uLc1CabZLP9g0I+tadJYFpX3VWuRvv/3WTItWv359M7ODZpOVjjTXAFs/QLX+UEdnp6/nBXxFL7+n/4Ko9DzSqxH65UvLBvSLme6XvsRBH+sVlPLly2erXj079N+9HlNnfdDj6rmYcQ5trZfXqcn0C66eo/oFUPunsjqv9QukviY973QaMZ3q7ELuxqazJ2iAq39z0p/XWtev9IuB/r3Q2Rc0e6szwWgpg043pkqVKmVma9HpynTqMZ2dQWuHgZxm6Yi0HD8qAL/T6ZR0kJdmZRo3bpzb3QHgA1OnTpWHH37YXDHRKzYAzo0aXSBE6KVPzcDoJVqtd9VyAr3k6prPFkDw0Ss1epMVHfSlpT06q4POfkCQC2QPgS4QInR6r+eee062bt1qShb0kqNmf3TCegDBSetitVxBf2rpwF133UVpD+AFShcAAAAQkhiMBgAAgJBEoAsAAICQRKALAACAkESgCwAAgJBEoAvA0NuM6t2Q9CeAwMG5CVw4Zl0AYCQlJUl0dLSZiF7vyAQgMHBuAheOjC4AAABCEoEuAAAAQhJ3RsNFSUtLkz179pg7cVmWldvdwUVeHk3/E0Bg4NwMLVoxevToUSlZsqQ4HDmfb0xOTpbU1FS/tJ03b16JjIyUQEKNLi7Krl27JD4+Pre7AQBAUElMTJTLLrssx4PcqIIxIqdP+KX9uLg42bZtW0AFu2R0cVE0k6vy3vSCWOGB8w8bgMjODx7O7S4AyOBoUpJUKBvv/vzMSamayT19QiKqPyySJ69vGz+TKvvWTzbHINBFyHCVK2iQa4VH5XZ3AKTDCH0gcOVquV+evGL5ONAN1PIAAl0AAAA7sUyk7fs2AxCzLgAAACAkkdEFAACwE8vx7+LrNgNQYPYKAAAAuEhkdAEAAOzEsvxQoxuYRbpkdAEAABCSyOgCAADYiWWfGl0CXQAAADuxKF0AAAAA/OLo0aPy7LPPSunSpSUqKkquueYaWbFihXu70+mUAQMGSIkSJcz2Jk2ayKZNm7w+DoEuAACArTj+V77gq8XLkLJTp04yf/58+eCDD2TdunVy8803m2B29+7dZvtLL70kY8aMkfHjx8uyZcskf/780rRpU0lOTvb2lQIAAAA54+TJkzJr1iwTzDZs2FAqVKgggwYNMj/HjRtnsrmjR4+Wfv36ScuWLaVmzZoyZcoU2bNnj8yZM8erYxHoAgAA2LFG1/LxIiJJSUkeS0pKylmHP336tJw5c0YiIyM91muJwqJFi2Tbtm2yb98+k+F1iY6Olnr16smSJUu8eqkEugAAAPCJ+Ph4E5S6lpEjR561T8GCBSUhIUGGDh1qsrQa9H744YcmiN27d68JclVsbKzH8/Sxa1t2MesCAACAnVj+m14sMTFRChUq5F4dERGR6e5am9uhQwe59NJLJU+ePHLllVfKvffeKytXrvRpt8joAgAAwCc0yE2/ZBXoli9fXhYuXCjHjh0zwfHy5cvl1KlTUq5cOYmLizP77N+/3+M5+ti1LbsIdAEAAOzE8l+Nrrd0NgWdQuzQoUMyb948M/isbNmyJqBdsGCBez+t99XZF7TkwRuULgAAANiJlft3RtOgVmdXqFy5smzevFl69uwpVapUkYcfflgsyzJz7A4bNkwqVqxoAt/+/ftLyZIlpVWrVl4dh0AXAAAAOerIkSPSt29f2bVrlxQtWlTatGkjw4cPl/DwcLO9V69ecvz4cencubMcPnxYGjRoIHPnzj1rpobzsZwaTgMXSC8l6KjKiFtHixUeldvdAZDOoU8653YXAGTyuRkbE20CvfSDtnL0M7t+L7HCMq+dvVDO0ymSsvSlXHld50KNLgAAAEISpQsAAAB2YuV+jW5OCcxeAQAAABeJjC4AAICdWJYfMroXNr2Yv5HRBQAAQEgiowsAAGAnDuvfxddtBiACXQAAADuxGIwGAAAABDUyugAAALYbjGb5vs0AREYXAAAAIYmMLgAAgJ1Y1OgCAAAAQY2MLgAAgJ1Y1OgCAAAAQY2MLgAAgJ1Y9qnRJdAFAACwE4vSBQAAACCokdEFAACwE8s+pQuB2SsAAADgIpHRBQAAsBOLGl0AAAAgqJHRBQAAsBWHH2pqAzN3Gpi9AgAAAC4SGV0AAAA7sajRBQAAAIIaGV0AAADbZXQdvm8zABHoAgAA2InFDSMAAACAoEZGFwAAwE4sBqMBAAAAQY2MLgAAgJ1Y1OgCAAAAQY2MLgAAgJ1Y1OgCAAAAQY2MLgAAgJ1Y9qnRJdAFAACwE4vSBQAAACCokdEFAACwEcuyzOLjRiUQkdEFAABASCKjCwAAYCMWGV0AAAAguJHRBQAAsBPrv8XXbQYgMroAAAAISWR0AQAAbMSyUY0ugS4AAICNWDYKdCldAAAAQEgiowsAAGAjFhldAAAAILiR0QUAALARi4wuAAAAENzI6AIAANiJxQ0jAAAAgKBGRhcAAMBGLBvV6BLoAgAA2Ihl/Rvs+rZRCUiULgAAACAkkdEFAACwEUv/83mpQWCmdMnoAgAAICSR0QUAALARy0aD0cjoAgAAICSR0QUAALATixtGAAAAAEGNjC4AAICdWL6v0XUGaI0ugS4AAICNWH4IdH0/XZlvULoAAACAkERGFwAAwEYsMroAAABAcCOjCwAAYCcW04sBAAAAQY1AFwAAwIY1upaPl+w6c+aM9O/fX8qWLStRUVFSvnx5GTp0qDidTvc++vuAAQOkRIkSZp8mTZrIpk2bvH6tBLoAAADIMS+++KKMGzdO3nzzTdmwYYN5/NJLL8kbb7zh3kcfjxkzRsaPHy/Lli2T/PnzS9OmTSU5OdmrY1GjCwAAYCNWLs+6sHjxYmnZsqU0b97cPC5Tpox89NFHsnz5cnc2d/To0dKvXz+zn5oyZYrExsbKnDlzpG3bttk+FhldAAAAG7H8WLqQlJTksaSkpJx1/GuuuUYWLFggf/75p3m8du1aWbRokTRr1sw83rZtm+zbt8+UK7hER0dLvXr1ZMmSJV69VjK6AAAA8In4+HiPxwMHDpRBgwZ5rOvTp48JgqtUqSJ58uQxNbvDhw+Xdu3ame0a5CrN4Kanj13bsotAFwAAwEYsP5YuJCYmSqFChdzrIyIiztp3xowZMnXqVJk2bZpUr15d1qxZI88++6yULFlS2rdv79N+EegCAADAJzTITR/oZqZnz54mq+uqta1Ro4bs2LFDRo4caQLduLg4s37//v1m1gUXfVyrVi2v+kONLgAAgB1vGGH5eMmmEydOiMPhGYJqCUNaWpr5Xacd02BX63hdtNRBZ19ISEjw6qWS0QUAAECOadGihanJLVWqlCldWL16tbz22mvSoUMHdxmEljIMGzZMKlasaAJfnXdXSxtatWrl1bEIdAEAAGzEyuXpxXS+XA1cn3jiCTlw4IAJYB999FFzgwiXXr16yfHjx6Vz585y+PBhadCggcydO1ciIyO965cz/W0oAC/ppQSd8iPi1tFihUfldncApHPok8653QUAmXxuxsZEy5EjR85by+qvz+y4Dh+KI28+n7adlnpC9r17f668rnMhowsAAGAjVi5ndHMSgS4AAICNWDYKdJl1AQAAACGJjC4AAICdWN5NB5btNgMQGV0AAACEJDK6AAAANmJRowsAAAAENzK6AAAANmKR0QUAAACCG4FuOu+9954ULlzY5+3qt5w5c+ZcdDvXX3+9ufezP/mqrwhMDoclA+6rKxsmtJWD0zvI+vFtpc/dtT32efvpRnJyTmeP5bMBzXKtz4AdvD1+nFxVu6YUL1rILI0aJMi8ud+4tz/1+KNSrXJ5KVIwSuJLFJO7WreUjX/8kat9RvCy9D/Lx0uATrtgu9KFhx56SN5//33ze3h4uJQqVUoefPBBee655/x2zL1790qRIkUuup1PP/3U9Bm4UN1bXyGP3FJNHnn9B/k98ZDUKV9MJjzdSJKOp8pbX6137zdv5U559I2F7scpp87kUo8Be7j0sstk6IgXpEKFiuJ0OuXDD943wezSFaulWvXqUvvKOtL2vnYSH19KDh48KMOHDpLbbr1Z/ti0TfLkyZPb3UeQsWxUumC7QFfdcsstMnnyZElJSZGvv/5annzySRNAlihRwi/Hi4uLu6jnp6amSt68eaVo0aI+6xPsqX7lWPly+XaZuzLRPN554Jjc3bCC1K1YXET+F+imnk6T/YdP5mJPAXtpflsLj8eDhw6XiRPGyfJlS02g2/GRzu5tpcuUkYGDh8nVda6QHdu3S7ny5XOhx0BwsGXpQkREhAk+S5cuLY8//rg0adJEPv/887P227Jli7Rs2VJiY2OlQIECctVVV8l33313Vra2efPmEhUVJWXLlpVp06ZJmTJlZPTo0VmWA/Tu3VsqVaok+fLlk3Llykn//v3l1KlT7u2DBg2SWrVqyTvvvGPajIyMPKt04ccff8z00oFmrF0+++wzufLKK83z9TiDBw+W06dPu7dv2rRJGjZsaLZXq1ZN5s+f77P3GIFp6cb9ckPNS6VCyWjzuEaZopJQNVa+XfVv4Oty3eUlZMd7D8jasXfL6482kKIFI3Kpx4D9nDlzRmZM/1iOHz8u9eonnLVd1095f7KUKVtWLouPz5U+IkRuGGH5eAlAtszoZqRB6j///HPW+mPHjsmtt94qw4cPN8HxlClTpEWLFrJx40ZT8qC07OHvv/82gadmhbt16yYHDhw45/EKFixo6oFLliwp69atk0ceecSs69Wrl3ufzZs3y6xZs0y5QmaXpa655hoTZLts2LDB9FUDV/Xzzz+bvo0ZM0auu+46E7R37vxvRmDgwIGSlpYmrVu3NkH8smXL5MiRI9mq/9UsuC4uSUlJ530OAscrs9ZIoai8svbNu+VMmlPyOCwZOHWFfPzTZvc+81ftks+WbJftB5KkXFwhGXz/1fJZ/2bSqM9nkpbmzNX+A6Hst3Xr5PrrEiQ5OdkkV6Z/MluqVqvm3j5h3FvyfN9eJtCtVLmyfPXNfHO1D0DWbB3oah3UggULZN68edKlS5eztl9xxRVmcRk6dKjMnj3bZH+feuop+eOPP0yGd8WKFVK3bl2zj2ZhK1aseM7j9uvXz/27Zn979OghH3/8sUegq+UKGlgXK1Ys0zb0j5urJEKD9E6dOkmHDh3MojR726dPH2nfvr15rBld7b8eQwNd7bf2X1+7BtxqxIgR0qzZuQcdjRw50rSN4HTnteWlbaMK8tBr38vviQelZtlL5OUOCbL34HGZ+sMms8/MRVvc+6/fcUjWbT8oGybcKw0vLyE//t+eXOw9ENo0eF326xqTeJj96SfySIf28u2Che5gV2t0Gze5Sfbt2yujX3tF7r/3bvn+p1/cV/2A7LKo0Q1tX375pfm2rOUCmtm87777TLnAzJkzz8ro6vqvvvrKZE/1sv/Jkydl586dZrtmdsPCwkx5gEuFChXOO/Bs+vTpJtOqWVY9hrZbqFAhj320rCKrIDc9fQ1t2rQx+7/++uvu9WvXrpVffvnFZKPTXw7TTMGJEydMBjg+Pt4d5KqEhLMvkWXUt29fk7VOn9HVdhAcRjxUz2R1XcGsBrKlihWQnm1quwPdjLbvPyp/HTkp5eOiCXQBP9IERvkKFczvV9apIyt/XSFj33hd3hw3wayLjo42S4WKFeXqevWlRLEi8tmc2XJP23tzuedA4LJloHvDDTfIuHHjzB8VDfQ0WM2MZlq1bvWVV14xAayWONx5550m23qhlixZIu3atTNZ0aZNm5o/WprNffXVVz32y58/f7ba0xrjxMREWb58ucfr0ABaj6HlCRldzLd/LeHQBcEpKm+YpDk9yw+0hMFxji/il8bkl5iCkbLv0An/dxCAmyZi0peKZbwiqUtqFtuBc7HI6IY2DSI1cD0fzYjq4K477rjDHTxu377dvb1y5comG7t69WqpU6eOu7b20KFDWba5ePFik319/vnn3et27NhxQa/jtddekxkzZpg2Y2JiPLZpllkzzlm9zqpVq5oAWTPVrtkmli5dekH9QPD4+tcd0vvO2pL41zEzvVitspfI07fXkCkLNprt+SPD5Pl76sicJdtk3+ETpkZ3ePt6smXvEZm/2nPAGgDf6f98X2l6SzMzfdjRo0dl+sfT5KeFP8oXX8+TbVu3yiczp0vjJjfLJcWKye5du+TVl18wyZemzW7N7a4DAc2WgW52aa2tDgbTAWj6TUVnR9Bv2C5VqlQxMzboIC/NEOtgtO7du5s/Pll9s9E2tfRBs7g6i4OWRWjdr7e0xlbrbceOHSuXXHKJ7Nu3z6zXY2uWeMCAAXLbbbeZQXOahXY4HKac4bfffpNhw4aZfuvMD1rD+/LLL5sShPTBN0JTt7cXy8B2dc1MCsWio2TvoRMyad4GGTFjlTu7e3mZotLuhkpSOH9es/27NbtkyNRfzZRjAPzjrwMHpOPDD8q+vXvN3/DLa9Q0Qa7W5O7Zs0d+WfSzvDlmtEmkFI+NlQYNGsoPPy2W4sV1akDAO5b17+LrNgMRge55MqY6uEtnONBgUqcFyzjLgA4Y69ixo5ntQAeH6WCt9evXZ1kecPvtt0vXrl3NYDa9JKVTk2kArbXA3li0aJGpuX3sscfM4qKBq87ooGURWos8ZMgQefHFF00QroG5DlpTGvhqgK19v/rqq82gOK0b1jmGEbqOJZ+SnpOWmCUzyaln5PbB/7sbE4CcMX7ipCy3aYndnC++ztH+wA6BruXzNgOR5dQiH/jMrl27zOAszbg2btxYQp0G/pp9iLh1tFjhUbndHQDpHPrkfzcZABA4n5uxMdFmdo2MA9Fz6jO7XJdPxBGRvbFA2ZWWcly2vnFnrryucyGje5G+//57U7tbo0YNU++q5QSaHXXNZwsAABBQLD9kYAM0o0uge5F0eq/nnntOtm7dam76oGUOU6dONaUCAAAAyD0EuhdJa2F1AQAACAaWjaYXc+R2BwAAAAB/IKMLAABgI5aNphcjowsAAICQREYXAADARhwOyyy+5PRxe75CRhcAAAAhiYwuAACAjVg2qtEl0AUAALARi+nFAAAAgOBGRhcAAMBGLBuVLpDRBQAAQEgiowsAAGAjFjW6AAAAQHAjowsAAGAjFhldAAAAILiR0QUAALARy0azLhDoAgAA2IglfihdkMCMdCldAAAAQEgiowsAAGAjlo1KF8joAgAAICSR0QUAALARi+nFAAAAgOBGRhcAAMBGLGp0AQAAgOBGRhcAAMBGLBvV6BLoAgAA2IhF6QIAAAAQ3MjoAgAA2Ihlo9IFMroAAAAISWR0AQAA7MTyQ01tYCZ0yegCAAAgNJHRBQAAsBGLGl0AAAAguJHRBQAAsBHLRvPoEugCAADYiEXpAgAAABDcyOgCAADYiGWj0gUyugAAAAhJZHQBAABsxKJGFwAAAAhuZHQBAABsxCKjCwAAAAQ3MroAAAA2Ytlo1gUCXQAAABuxKF0AAAAA/KNMmTLugDv98uSTT5rtycnJ5veYmBgpUKCAtGnTRvbv3+/1cQh0AQAAbFi6YPl48caKFStk79697mX+/Plm/V133WV+du3aVb744guZOXOmLFy4UPbs2SOtW7f2+rVSugAAAIAcVaxYMY/HL7zwgpQvX14aNWokR44ckUmTJsm0adPkxhtvNNsnT54sVatWlaVLl0r9+vWzfRwyugAAADZiZVIy4ItFJSUleSwpKSnn7U9qaqp8+OGH0qFDB9POypUr5dSpU9KkSRP3PlWqVJFSpUrJkiVLvHqtBLoAAADwifj4eImOjnYvI0eOPO9z5syZI4cPH5aHHnrIPN63b5/kzZtXChcu7LFfbGys2eYNShcAAABsxPLDdGCu5hITE6VQoULu9REREed9rpYpNGvWTEqWLOnbThHoAgAAwFc0yE0f6J7Pjh075LvvvpNPP/3UvS4uLs6UM2iWN31WV2dd0G3eoHQBAADARhyW5ZflQuggs+LFi0vz5s3d6+rUqSPh4eGyYMEC97qNGzfKzp07JSEhwav2yegCAADYiBUgd0ZLS0szgW779u0lLOx/IanW9nbs2FG6desmRYsWNRniLl26mCDXmxkXFIEuAAAAcpyWLGiWVmdbyGjUqFHicDjMjSJ05oamTZvKW2+95fUxCHQBAABsxAqQWwDffPPN4nQ6M90WGRkpY8eONcvFoEYXAAAAIYmMLgAAgI04rH8XX7cZiMjoAgAAICSR0QUAALAT68Jqas/XZiAiowsAAICQREYXAADARqwAmUc3JxDoAgAA2Ij133++bjMQUboAAACAkERGFwAAwEYcTC8GAAAABDcyugAAADZiBcgtgHMCGV0AAACEJDK6AAAANmLZaHoxMroAAAAISWR0AQAAbMRhWWbxdZuBiEAXAADARixKFwAAAIDgRkYXAADARiymFwMAAACCGxldAAAAG7Go0QUAAACCGxldAAAAG3HYaHoxMroAAAAISWR0AQAAbMT6b/F1m4GIjC4AAABCEhldAAAAG7FsNI8ugS4AAICNOKx/F1+3GYgoXQAAAEBIIqMLAABgI5aNShfI6AIAACAkkdEFAACwGSswE7A+R0YXAAAAIYmMLgAAgI1Y1OgCAAAAwY2MLgAAgI04bDSPLoEuAACAjViULgAAAADBjYwuAACAjVj/Lb5uMxCR0QUAAEBIuqBA9+eff5b7779fEhISZPfu3WbdBx98IIsWLfJ1/wAAAOBDDsvyyxISge6sWbOkadOmEhUVJatXr5aUlBSz/siRIzJixAh/9BEAAADwf6A7bNgwGT9+vEycOFHCw8Pd66+99lpZtWqV9z0AAABAjrEs/ywhEehu3LhRGjZseNb66OhoOXz4sK/6BQAAAORsoBsXFyebN28+a73W55YrV+7iegMAAIAcmUfX8vESEoHuI488Is8884wsW7bMvKg9e/bI1KlTpUePHvL444/7p5cAAADwCctGpQtez6Pbp08fSUtLk8aNG8uJEydMGUNERIQJdLt06eKfXgIAAAD+DnQ1i/v8889Lz549TQnDsWPHpFq1alKgQAFvmwIAAEAOc/hhOrBAnV7sgu+MljdvXhPgAgAAACER6N5www3nLDj+/vvvL7ZPAAAA8BPLDzW1AZrQ9T7QrVWrlsfjU6dOyZo1a+S3336T9u3b+7JvAAAAQM4FuqNGjcp0/aBBg0y9LgAAAAKX5YfpwEJmerGs3H///fLuu+/6qjkAAAAgdwajZbRkyRKJjIz0VXMIMoO73CCR+QvmdjcApFPkqqdyuwsAMnCeSQ2ILKfDD22GRKDbunVrj8dOp1P27t0rv/76q/Tv39+XfQMAAICPWTYqXfA60I2OjvZ47HA4pHLlyjJkyBC5+eabfdk3AAAAIGcC3TNnzsjDDz8sNWrUkCJFilz4UQEAAJArLEtv8OD7NgORVyUVefLkMVnbw4cP+69HAAAAQG7UDl9++eWydetWXxwbAAAAOcxh+WcJiUB32LBh0qNHD/nyyy/NILSkpCSPBQAAAAiqGl0dbNa9e3e59dZbzePbb7/dY4Sdzr6gj7WOFwAAAIHJYtaFsw0ePFgee+wx+eGHH/zbIwAAACAnA13N2KpGjRr54rgAAADIBQ4/1NQGao1uWCikpQEAAJA9luX76cCsUAh0K1WqdN5g9+DBgxfbJwAAACBnA12t0814ZzQAAAAED4dlmcXXbQZ9oNu2bVspXry4/3oDAAAA5HSgS30uAABA8HNcyI0UstFmIHJ4O+sCAAAAcDF2794t999/v8TExEhUVJTUqFFDfv31V4+4c8CAAVKiRAmzvUmTJrJp0yb/BbppaWmULQAAAITIrAuWj5fsOnTokFx77bUSHh4u33zzjfz+++/y6quvSpEiRdz7vPTSSzJmzBgZP368LFu2TPLnzy9NmzaV5ORk/9XoAgAAABfjxRdflPj4eJk8ebJ7XdmyZT2yuaNHj5Z+/fpJy5YtzbopU6ZIbGyszJkzx4wZC/aSCgAAAPiBQ/6ddcGni/yb0k1KSvJYUlJSzjr+559/LnXr1pW77rrLVAvUrl1bJk6c6N6+bds22bdvnylXcNFZv+rVqydLlizx8rUCAADANiw/li5oplaDUtcycuTIs46/detWGTdunFSsWFHmzZsnjz/+uDz99NPy/vvvm+0a5CrN4Kanj13bsovSBQAAAPhEYmKiFCpUyP04IiIi03FfmtEdMWKEeawZ3d9++83U47Zv3158iYwuAACAjTgs/yxKg9z0S2aBrs6kUK1aNY91VatWlZ07d5rf4+LizM/9+/d77KOPXduy/Vq9fXMAAACAC6UzLmzcuNFj3Z9//imlS5d2D0zTgHbBggXu7Vrvq7MvJCQkeHUsShcAAABsxDIZWN/eCMyb5rp27SrXXHONKV24++67Zfny5fL222+b5d+2LHn22Wdl2LBhpo5XA9/+/ftLyZIlpVWrVl71i0AXAAAAOeaqq66S2bNnS9++fWXIkCEmkNXpxNq1a+fep1evXnL8+HHp3LmzHD58WBo0aCBz586VyMhIr45FoAsAAGAjlpc3eMhum9647bbbzJJ1e5YJgnW5GNToAgAAICSR0QUAALARR7pZEnzZZiAi0AUAALAR67//fN1mIKJ0AQAAACGJjC4AAICNOGxUukBGFwAAACGJjC4AAICNOMjoAgAAAMGNjC4AAICNWJZlFl+3GYjI6AIAACAkkdEFAACwEYeNanQJdAEAAGzEsv5dfN1mIKJ0AQAAACGJjC4AAICNOCzLLL5uMxCR0QUAAEBIIqMLAABgIw4bDUYjowsAAICQREYXAADATiw/zJJARhcAAADIOWR0AQAAbMQhlll83WYgIqMLAACAkERGFwAAwEYsG90ZjUAXAADARhxMLwYAAAAENzK6AAAANuLgFsAAAABAcCOjCwAAYCOWjQajkdEFAABASCKjCwAAYLcbRljcMAIAAAAIWmR0AQAAbMSyUY0ugS4AAICNOPxwST9QSwQCtV8AAADARSGjCwAAYCOWZZnF120GIjK6AAAACElkdAEAAGzE+m/xdZuBiIwuAAAAQhIZXQAAABtxWH64YQQ1ugAAAEDOIaMLAABgM5bYA4EuAACAjVg2ujMapQsAAAAISWR0AQAAbMTihhEAAABAcCOjCwAAYCMOP2Q6AzVzGqj9AgAAAC4KGV0AAAAbsajRBQAAAIIbGV0AAAAbsfxww4jAzOcS6AIAANiKRekCAAAAENzI6AIAANiIg+nFAAAAgOBGRhcAAMBGLGp0AQAAgOBGRhcAAMBGLBtNL0ZGFwAAACGJjC4AAICNWNa/i6/bDEQEugAAADbiEMssvm4zEFG6AAAAgJBERhcAAMBGLBuVLpDRBQAAQEgiowsAAGAj1n//+brNQERGFwAAACGJjC4AAICNWNToAgAAAMGNjC4AAICNWH6YR5caXQAAAARM6YLl48UbgwYNEsuyPJYqVaq4tycnJ8uTTz4pMTExUqBAAWnTpo3s37/f69dKoAsAAIAcV716ddm7d697WbRokXtb165d5YsvvpCZM2fKwoULZc+ePdK6dWuvj0HpAgAAgI1YATIYLSwsTOLi4s5af+TIEZk0aZJMmzZNbrzxRrNu8uTJUrVqVVm6dKnUr18/28cgowsAAACfSEpK8lhSUlKy3HfTpk1SsmRJKVeunLRr10527txp1q9cuVJOnTolTZo0ce+rZQ2lSpWSJUuWeNUfAl0AAAAb3jDC8vF/Kj4+XqKjo93LyJEjM+1DvXr15L333pO5c+fKuHHjZNu2bXLdddfJ0aNHZd++fZI3b14pXLiwx3NiY2PNNm9QugAAAACfSExMlEKFCrkfR0REZLpfs2bN3L/XrFnTBL6lS5eWGTNmSFRUlG86Q0YXAADAXhyWfxalQW76JatANyPN3laqVEk2b95s6nZTU1Pl8OHDHvvorAuZ1fSe87V6tTcAAADgY8eOHZMtW7ZIiRIlpE6dOhIeHi4LFixwb9+4caOp4U1ISPCqXUoXAAAAbMRKV1Pryza90aNHD2nRooUpV9CpwwYOHCh58uSRe++919T2duzYUbp16yZFixY1meEuXbqYINebGRcUgS4AAICNWAEwvdiuXbtMUPvPP/9IsWLFpEGDBmbqMP1djRo1ShwOh7lRhM7c0LRpU3nrrbe87heBLgAAAHLUxx9/fM7tkZGRMnbsWLNcDAJdAAAAG7EuoNQgO20GIgajAQAAICSR0QUAALARR7rpwHzZZiAiowsAAICQREYXAADARqwAmF4sp5DRBQAAQEgio5vBQw89ZG45N2fOHJ+1+eOPP8oNN9wghw4dMre4uxiWZcns2bOlVatW4g++7CsCT71ShaV+6SIe6w6eSJUPVu6WghFh0uHq+Eyf99WG/bL57xM51EvAngrki5CBT9wmt994hRQrUkDWbtwlPV76RFb+vlPCwhwy6IkW0rRBdSl7WYwkHUuW75f9If3HfC57/zqS211HkLECYB5dW2R0NajUwO2FF17wWK9Bpq73RpkyZWT06NHZ2k/b1iV//vxy5ZVXysyZM8WfrrnmGtm7d6+508fF0naaNWvmk37Bnv4+nioTl+50LzPX7jXrj6Wc9livy5IdhyT1dJrsOHgyt7sNhLxxA+6TG+tXkQ793pe6d4+Q75b8IV+N7yIli0VLvsi8UqtqvLww8RtJuPdFadt9olQqHSszRz+a291G0E4vJj5fAlGuly7ohMAvvviiySDmlCFDhpiAcfXq1XLVVVfJPffcI4sXL/bb8fLmzStxcXFeB+/ppaammp/aTkREhA97B7txOp1y4tQZ95J8Ou3f9SIe63UpH5NPNv19XE6l6VYA/hIZES6tGteS50fPkV9WbZGtiX/L8Alfy5bEv+SRu64zGdzbHn9TZs1fLZt2HJDl67ZL1xdmSJ1qpSQ+zvMqDYAACnSbNGligreRI0eec79Zs2ZJ9erVTZCnWdlXX33Vve3666+XHTt2SNeuXd3Z2nMpWLCgOWalSpXMHTeioqLkiy++yHTfuXPnmtvS6WX8mJgYue2222TLli0e+2iQXKtWLRO0161b152RXrNmjbscQB9rSYTS293pbe8uvfRSyZcvn9SoUUM++ugjjzb1NT311FPy7LPPyiWXXGJufae0HVdZxaBBg9yvN/3y3nvvme1paWnmfS1btqx5jVdccYV88sknHsf5+uuvzfug27VkYfv27ed87xD8CkeFS8er4+WhupdJ08rFpGBEnkz3K14grxQvECHr9x3N8T4CdhOWxyFhYXkkOfWUx/rklFNyTe3ymT6nUMEo83f+8FGuuMA7DrHEYfl4CdCcbq4Hunny5JERI0bIG2+8Ye57nJmVK1fK3XffLW3btpV169aZAK9///7ugO7TTz+Vyy67zJ2p1SW7wsLCJDw83J0xzej48ePSrVs3+fXXX2XBggXmvst33HGH+eOikpKSpEWLFiZYXbVqlQwdOlR69+59zmMmJydLnTp15KuvvpLffvtNOnfuLA888IAsX77cY7/333/fZIN/+eUXGT9+/Fnt9OjRw/16dXnllVdM4KzBttIgd8qUKea569evN18E7r//flm4cKHZnpiYKK1btzb916C8U6dO0qdPn3P2Xe83ra85/YLgse9oinz751/y2W/75PvN/0ihyDC5s2ZJCc9z9h+o6rEF5Z8TqbL3aEqu9BWwk2MnUmTp2q3S95FmUqJYtDgclrS99SqpV7OsxF1S6Kz9I/KGybCnW8qMuSvl6PHkXOkzEAwCYjCaBo6aER04cKBMmjTprO2vvfaaNG7c2AS3SjOQv//+u7z88sumzrdo0aImYHZlarNLg1vNDB85ckRuvPHGTPdp06aNx+N3331XihUrZo5/+eWXy7Rp00wWdeLEiSajW61aNdm9e7c88sgjWR5XM7kapLp06dJF5s2bJzNmzJCrr77avb5ixYry0ksvZdlOgQIFzKKWLl0q/fr1M8Gx9ksDUv0C8d1330lCQoLZp1y5crJo0SKZMGGCNGrUSMaNGyfly5d3Z8crV65svkhoKUlWNHgePHhwltsR2HYcSpf5OXHKBL46AK3SJfll/f5j7k15HJZULp5flu389yoEAP/r0G+KTBjUTrZ+O1xOnz4ja/5IlBlzf5XaVUt57KcD0z58qaP57Hl6xPRc6y+Cl+WHmtrAzOcGQEbXRYMrDdI2bNhw1jZdd+2113qs08ebNm2SM2fOeH0szbhqgKjZTz2uDoZr3rx5pvvqMbTMQIPEQoUKmbIJtXPnTvNz48aNUrNmTRPkuqQPVjOjfdbMr2aBNUjXvmig62rTRbO+2aHP01kYNHjWzLfavHmznDhxQm666SZ3QKyLZnhdpRf6vtarV8+jLVdQnJW+ffuaLwauRbPCCF6pZ9Lk8MlTEh0V7rG+4iX5JczhkD8O/C/4BeBf23b9LTd3el1iErpJxWb95boHXpHwsDyybfffHkHu1Bc7SqkSRUzNLtlcIAgyuqphw4amDlUDKc3S+lPPnj3NMTTwi42NPWdNr17WL126tMnYlixZ0pQsaMY0q1KH7NBM9Ouvv25midBgV2d/0FrcjG3q+vPR0orbb7/dBKhauuFy7Ni/AYqWR2gGOb2LGcymz2UwXOgId1gSHRkmx1M9vzBWjy0gWw+ekJOn/i3RAZBzTiSnmqVwwShpck1VeX70Zx5BbvlSxeSWzmPk4JHjud1VBCvLPindgAl0lWZWtYRBL6GnV7VqVVOnmp4+1hIGLVlQWsua3eyuDu6qUKHCeffTQWOasdUg97rrrjPr9NJ/etrXDz/80JQKuALAFStWnLNd7XvLli1NvazS4PnPP/80ZQ/ejp7XNvT5H3zwgUfArm1pfzTbq2UKmdH39fPPP/dYpyUQCF0NyhaVbQdPSFLyaSmQN4+ZU1dD2T//+l/mVgPfS6Mj5bP1+3O1r4DdNEmoauYi/XP7ASkfX0xGdG0lf27bL1M+X2KC3Gkvd5LaVeKl9TPjTXlRbExB87yDR07IqdPeX90E7CCgAl3NbrZr107GjBnjsb579+5mGjC93K9TgS1ZskTefPNNeeutt9z7aEnBTz/9ZAasaYCnwezFKlKkiJlp4e2335YSJUqYoDHjYK377rtPnn/+eTOgTLfpPjooTGWVKdbaW539QGdr0GNoDfL+/fu9DnR1UJ7W4H777bcmg+vK4up8vVqvrKUMOgBNA2GdOUJLDTTI1hKM9u3by2OPPWbqczXDrQPRdNCfa4AfQlOBiDxyS+ViEhmeR06eOiN7kpJlxpo9HplbHYR2LOWMZz0vAL+LLhApQ7rcLpfGFjbB62cL1sjAsV/I6dNpUqpEUWlxfU2z3/LpfT2ep+UOP6/clEu9RjCybHQL4IAKdJVefp8+3bO4Xm/qoAO1BgwYYIJdDTp1v/QlDvr40UcfNYOrNLuq2c6LpTMsfPzxx/L000+bcgXN3moQrlN/uWjQqFOTPf744yYbrcG69lMD4PR1u+npoLGtW7eaUg2tE9YgWWtsNRD1hs6eoMGt3pAivcmTJ5v3Rt8rHTinA8j0eDpFmr6Xzz33nNmvVKlSZto2DYZ11gutLdYBbB06dLig9wuBb+4ff513n8U7DpkFQM7SOXJ1yczOvQclqvZTOd4nhCjLD3cyC8w4VyynLyJCeJg6dao8/PDDJnDV+WlDmU4vphnkF75ZK5H5/72MBiAw9Hn6f/ONAwgMzjOpkrJuookRNFmWG5/ZC9bslAIFfXvsY0eTpHGtUrnyuoIqoxuMdCYDnZVBB32tXbvWzOqgsx+EepALAACCj2WfsWgEur6wb98+U66gP7Ws4q677pLhw4fndrcAAABsjUDXB3r16mUWAACAgGfZJ6UbMDeMAAAAAHyJjC4AAICNWDaaXoyMLgAAAEISGV0AAAAbsfwwj67P5+X1ETK6AAAACElkdAEAAGzEss+kCwS6AAAAtmLZJ9KldAEAAAAhiYwuAACAjVhMLwYAAAAENzK6AAAANmIxvRgAAAAQ3MjoAgAA2Ihln0kXyOgCAAAgNJHRBQAAsBPLPildAl0AAAAbsZheDAAAAAhuZHQBAABsxGJ6MQAAACC4kdEFAACwEcs+Y9HI6AIAACA0kdEFAACwE8s+KV0yugAAAAhJZHQBAABsxLLRPLoEugAAADZiMb0YAAAAENzI6AIAANiIZZ+xaGR0AQAAEJrI6AIAANiJZZ+ULhldAAAAhCQyugAAADZi2Wh6MTK6AAAACElkdAEAAGzEstE8ugS6AAAANmLZZywapQsAAAAITWR0AQAA7MSyT0qXjC4AAABCEhldAAAAG7GYXgwAAAAIbmR0AQAA7MTyw3RggZnQJaMLAACA0ERGFwAAwEYs+0y6QKALAABgK5Z9Il1KFwAAABCSyOgCAADYiMX0YgAAAEBwI6MLAABgI5Yfphfz+XRlPkJGFwAAALnmhRdeEMuy5Nlnn3WvS05OlieffFJiYmKkQIEC0qZNG9m/f7/XbRPoAgAA2HDSBcvHy4VYsWKFTJgwQWrWrOmxvmvXrvLFF1/IzJkzZeHChbJnzx5p3bq11+0T6AIAACDHHTt2TNq1aycTJ06UIkWKuNcfOXJEJk2aJK+99prceOONUqdOHZk8ebIsXrxYli5d6tUxCHQBAADsxPJfSjcpKcljSUlJybIbWprQvHlzadKkicf6lStXyqlTpzzWV6lSRUqVKiVLlizx6qUS6AIAANhwejHLx/+p+Ph4iY6Odi8jR47MtA8ff/yxrFq1KtPt+/btk7x580rhwoU91sfGxppt3mDWBQAAAPhEYmKiFCpUyP04IiIi032eeeYZmT9/vkRGRoo/kdEFAACwESvdFGM+W/5rW4Pc9Etmga6WJhw4cECuvPJKCQsLM4sOOBszZoz5XTO3qampcvjwYY/n6awLcXFxXr1WMroAAADIMY0bN5Z169Z5rHv44YdNHW7v3r1N+UN4eLgsWLDATCumNm7cKDt37pSEhASvjkWgCwAAYCPWRUwHdq42s6tgwYJy+eWXe6zLnz+/mTPXtb5jx47SrVs3KVq0qMkMd+nSxQS59evX96pfBLoAAAAIKKNGjRKHw2EyujpzQ9OmTeWtt97yuh0CXQAAABuxAvAWwD/++KPHYx2kNnbsWLNcDAajAQAAICSR0QUAALAVK5erdHMOgS4AAICNWAFYuuAvlC4AAAAgJJHRBQAAsBHLNoULZHQBAAAQosjoAgAA2IhFjS4AAAAQ3MjoAgAA2Ij133++bjMQkdEFAABASCKjCwAAYCeWfaZdINAFAACwEcs+cS6lCwAAAAhNZHQBAABsxGJ6MQAAACC4kdEFAACwEYvpxQAAAIDgRkYXAADATiz7TLtARhcAAAAhiYwuAACAjVj2SeiS0QUAAEBoIqMLAABgI5aN5tEl0AUAALAVyw/TgQVmpEvpAgAAAEISGV0AAAAbsWxUukBGFwAAACGJQBcAAAAhiUAXAAAAIYkaXQAAABuxqNEFAAAAghsZXQAAANvNomv5vM1ARKALAABgIxalCwAAAEBwI6MLAABgI5YfbtgboAldMroAAAAITWR0AQAA7MSyT0qXjC4AAABCEhldAAAAG7FsNL0YGV0AAACEJDK6AAAANmLZaB5dAl0AAAAbsewzFo3SBQAAAIQmMroAAAB2YtknpUtGFwAAACGJjC4AAICNWEwvBgAAAAQ3MroAAAA2YjG9GJA9TqfT/Ew+fiy3uwIgA+eZ1NzuAoAszkvX52duSEpKCoo2fYFAFxfl6NGj5uegO6/N7a4AABBUn5/R0dE5esy8efNKXFycVCwb75f2tW09RiCxnLn5lQJBLy0tTfbs2SMFCxYUK1CvWyDb38bj4+MlMTFRChUqlNvdAfAfzs3QomGXBrklS5YUhyPnh0olJydLaqp/rvZokBsZGSmBhIwuLoqepJdddlludwM+pB+kfJgCgYdzM3TkdCY3PQ1EAy0Y9SdmXQAAAEBIItAFAABASCLQBWBERETIwIEDzU8AgYNzE7hwDEYDAABASCKjCwAAgJBEoAsAAICQRKALAACAkESgCwBB7qGHHpJWrVq5H19//fXy7LPP5ng/fvzxR3PjmMOHD+f4sQEgMwS6AODHAFQDP130jkEVKlSQIUOGyOnTp/163E8//VSGDh2arX0JTgGEMu6MBgB+dMstt8jkyZMlJSVFvv76a3nyySclPDxc+vbt67Gf3pLTV/eIL1q0qE/aAYBgR0YXAPxI5z6Ni4uT0qVLy+OPPy5NmjSRzz//3F1uMHz4cHPP+8qVK5v9ExMT5e6775bChQubgLVly5ayfft2d3tnzpyRbt26me0xMTHSq1cvyThLZMbSBQ2ye/fuLfHx8aY/mlmeNGmSafeGG24w+xQpUsRkdrVfKi0tTUaOHClly5aVqKgoueKKK+STTz7xOI4G7pUqVTLbtZ30/QSAQECgCwA5SINCzd6qBQsWyMaNG2X+/Pny5ZdfyqlTp6Rp06ZSsGBB+fnnn+WXX36RAgUKmKyw6zmvvvqqvPfee/Luu+/KokWL5ODBgzJ79uxzHvPBBx+Ujz76SMaMGSMbNmyQCRMmmHY18J01a5bZR/uxd+9eef31181jDXKnTJki48ePl/Xr10vXrl3l/vvvl4ULF7oD8tatW0uLFi1kzZo10qlTJ+nTp4+f3z0A8A6lCwCQAzTrqoHtvHnzpEuXLvLXX39J/vz55Z133nGXLHz44Ycmk6rrNLuqtOxBs7daS3vzzTfL6NGjTdmDBplKA1FtMyt//vmnzJgxwwTTmk1W5cqVO6vMoXjx4uY4rgzwiBEj5LvvvpOEhAT3czSw1iC5UaNGMm7cOClfvrwJvJVmpNetWycvvviin95BAPAegS4A+JFmajV7qtlaDWLvu+8+GTRokKnVrVGjhkdd7tq1a2Xz5s0mo5tecnKybNmyRY4cOWKyrvXq1XNvCwsLk7p1655VvuCi2dY8efKY4DS7tA8nTpyQm266yWO9ZpVr165tftfMcPp+KFdQDACBgkAXAPxIa1c1+6kBrdbiamDqohnd9I4dOyZ16tSRqVOnntVOsWLFLrhUwlvaD/XVV1/JpZde6rFNa3wBIFgQ6AKAH2kwq4O/suPKK6+U6dOnmzKCQoUKZbpPiRIlZNmyZdKwYUPzWKcqW7lypXluZjRrrJlkra11lS6k58oo6yA3l2rVqpmAdufOnVlmgqtWrWoG1aW3dOnSbL1OAMgpDEYDgADRrl07ueSSS8xMCzoYbdu2baY29+mnn5Zdu3aZfZ555hl54YUXZM6cOfLHH3/IE088cc45cMuUKSPt27eXDh06mOe42tS6XaWzQWg9sJZYaN2wZnO1dKJHjx5mANr7779vyiZWrVolb7zxhnmsHnvsMdm0aZP07NnTDGSbNm2aGSQHAIGEQBcAAkS+fPnkp59+klKlSpnBZpo17dixo6nRdWV4u3fvLg888IAJXrUmVoPSO+6445ztaunEnXfeaYLiKlWqyCOPPCLHjx8327Q0YfDgwWbGhNjYWHnqqafMer3hRP/+/c3sC9oPnflBSxl0ujGlfdQZGzR41qnHdFCcDmADgEBiObMawQAAAAAEMTK6AAAACEkEugAAAAhJBLoAAAAISQS6AAAACEkEugAAAAhJBLoAAAAISQS6AAAACEkEugAAAAhJBLoAAAAISQS6AAAACEkEugAAAAhJBLoAAACQUPT/QOJK4+YEw0MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_plagio = sum(1 for _, _, label in test_pairs if label == 1)\n",
    "num_no_plagio = sum(1 for _, _, label in test_pairs if label == 0)\n",
    "\n",
    "print(f\"Ejemplos con plagio (1): {num_plagio}\")\n",
    "print(f\"Ejemplos sin plagio (0): {num_no_plagio}\")\n",
    "\n",
    "cm = compute_confusion_matrix(loaded_model, test_pairs, device, batch_size=batch_size, threshold=threshold)\n",
    "plot_confusion_matrix(cm, labels=['Plagiarized', 'Not Plagiarized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3ce512b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.6061, Best F1 score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "best_threshold, best_f1 = find_best_threshold(loaded_model, val_pairs, device, batch_size=batch_size, thresholds=np.linspace(0, 2, 100))\n",
    "print(f\"Best threshold: {best_threshold:.4f}, Best F1 score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f54be710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2524, Test Accuracy: 65.17%\n"
     ]
    }
   ],
   "source": [
    "avg_loss, accuracy = evaluate(loaded_model, test_pairs, device, best_threshold, batch_size=batch_size)\n",
    "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dd09f37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4801636040210724, np.False_)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plagiarism(path1, path2, model, embedding_layer, node_type_to_idx, parser, threshold, device='cpu'):\n",
    "    def procesar_archivo(filepath):\n",
    "        with open(filepath, 'r', encoding='utf8') as file:\n",
    "            code = file.read()\n",
    "        tree = parser.parse(bytes(code, \"utf8\"))\n",
    "        root_node = tree.root_node\n",
    "\n",
    "        nodes = []\n",
    "        edges = []\n",
    "        def traverse(node, parent_idx=None):\n",
    "            idx = len(nodes)\n",
    "            nodes.append(node.type)\n",
    "            if parent_idx is not None:\n",
    "                edges.append((parent_idx, idx))\n",
    "            for child in node.children:\n",
    "                traverse(child, idx)\n",
    "        traverse(root_node)\n",
    "\n",
    "        node_features = [node_type_to_idx.get(typ, 0) for typ in nodes]\n",
    "        x = embedding_layer(torch.tensor(node_features)).to(device)\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(device)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        return data\n",
    "\n",
    "    g1 = procesar_archivo(path1)\n",
    "    g2 = procesar_archivo(path2)\n",
    "\n",
    "    g1 = g1.to(device)\n",
    "    g2 = g2.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        h1, h2 = model(g1, g2)\n",
    "        distance = F.pairwise_distance(h1, h2)\n",
    "        pred = torch.sigmoid(-distance).cpu().item()\n",
    "\n",
    "    return pred, pred > threshold\n",
    "\n",
    "plagiarism('./datasets/ir_plag_preprocessed/400.java', './datasets/ir_plag_preprocessed/466.java', loaded_model, embedding_layer, node_type_to_idx, parser, best_threshold, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
